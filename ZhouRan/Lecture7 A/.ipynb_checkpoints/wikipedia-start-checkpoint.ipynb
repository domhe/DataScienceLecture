{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Big Data Lab</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 11 March 2019, with material from Olivier Grisel</div>\n",
    "\n",
    "We're going to review eigenvectors and eigenvalues for the midterm, and then use related tools to explore the Wikipedia graph. Now that you know how to explore small graphs, you are ready for bigger ones!\n",
    "\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/bigger.jpg\" width=300 />\n",
    "</center>\n",
    "\n",
    "Here's a good [backgrounder](https://www.geeksforgeeks.org/graph-and-its-representations/), with code, on graphs, in case you want to explore graph structures some more. Graphs are very important. It's the essence of Google, facebook, LinkedIn, Twitter, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review: Eigenvalues and Eigenvectors\n",
    "\n",
    "Linear algebra is the study of *linear transformations* on **vectors**, which represent points in a finite dimensional space. The matrix-vector product $y = A x$ is a linear combination of the columns of $A$.  The familiar definition,\n",
    "\n",
    "$$ y_i = \\sum_j A_{i,j} x_j $$\n",
    "\n",
    "which says:\n",
    "\n",
    "$$ y_1 = A_{1,1} x_1 + \\cdots + A_{1,n} x_n \\\\\n",
    "   y_2 = A_{2,1} x_1 + \\cdots + A_{2,n} x_n \\\\\n",
    "   \\vdots \\\\\n",
    "   y_n = A_{n,1} x_1 + \\cdots + A_{n,n} x_n $$\n",
    "\n",
    "can also be viewed as\n",
    "\n",
    "$$ y = \\Bigg[ A_{:,0} \\Bigg| A_{:,1} \\Bigg| \\dotsm \\Bigg] \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\end{bmatrix}\n",
    "= \\Bigg[ A_{:,0} \\Bigg] x_0 + \\Bigg[ A_{:,1} \\Bigg] x_1 + \\dotsb . $$\n",
    "\n",
    "The notation $A_{i,j}$ corresponds to the Python syntax `A[i,j]` and the colon `:` means the entire range (row or column).  So $A_{:,j}$ is the $j$th column, and $A_{i,:}$ is the $i$th row.  The corresponding Python syntax is `A[:,j]` and `A[i,:]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climbing Mountains: A little exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define vector: position of mountain climber\n",
    "x = np.array([0,1,2])\n",
    "print(\"initial: \", x)\n",
    "print('---')\n",
    "\n",
    "# Define matrix: jump-twist-and-half-with-three-spins-and-one-amazing-handhold  \n",
    "Twist = np.array([[2,1,2],[3,2,0],[1,0,1]])\n",
    "print(\"Twist matrix: \")\n",
    "print(Twist)\n",
    "print('---')\n",
    "\n",
    "# Define matrix: rappel with rope over a dangerous cliff\n",
    "Rappel = np.array([[5,3,1],[4,0,1],[3,2,-1]])\n",
    "print(\"Rappel matrix: \")\n",
    "print(Rappel)\n",
    "print('---')\n",
    "\n",
    "# Position of mountain climber after jump-twist\n",
    "y = Twist @ x\n",
    "print(\"After jump-twist: \", y)\n",
    "print('---')\n",
    "\n",
    "# position of mountain climber after rappel following jump-twist\n",
    "z = Rappel @ y\n",
    "print(\"After rappel: \", z)\n",
    "print('---')\n",
    "\n",
    "# Combination jump-twist and rappel\n",
    "Combo = Rappel @ Twist\n",
    "print(\"Combo matrix: \")\n",
    "print(Combo)\n",
    "print('---')\n",
    "\n",
    "# position of climber from initial position after combination jump-twist and rappel\n",
    "z2 = Combo @ x\n",
    "print(\"After combo: \", z2)\n",
    "print(z == z2)\n",
    "print('---')\n",
    "\n",
    "# Inverse of Combo\n",
    "Cinv = np.linalg.inv(Combo)\n",
    "print(\"Inverse of Combo matrix: \")\n",
    "print(Cinv)\n",
    "print('---')\n",
    "\n",
    "# Initial position of climber:\n",
    "x2 = Cinv @ z2\n",
    "print(\"Initial position of climber: \", x2)\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication is associative! And we can follow the actions of state machines on vector states with matrix multiplication, which is just a math operation that defines how we mutliply spreadsheets together!\n",
    "\n",
    "Tell me that's not neat stuff!\n",
    "\n",
    "We can use [numpy.linalg.solve()](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.solve.html) to solve a linear system of equations.\n",
    "\n",
    "For example, what is a mountain climber's position $x$ such that $C$ $x$ = $z$ where $y$ is the column vector (33, 22, 17)?\n",
    "\n",
    "$$ C x = z$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[20, 11, 11],[9, 4, 9], [11, 7, 5]])\n",
    "z = np.array([33, 22, 17])\n",
    "solution = np.linalg.solve(C, z)\n",
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's the initial position of our mountain climber! So *that's* what math is.. just mountain-climbing! And think that I was scared of math once upon a time... \n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/scared.jpg\" width=400 />\n",
    "(must've had *bad* professors..)\n",
    "</center>\n",
    "\n",
    "\n",
    "We can use linear algebra to analyze the game of Monopoly, and we can use it to study graphs. Google used it to rank the Web! But first, we need to understand eigenvectors and eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and eigenvectors\n",
    "\n",
    "### What are eigenvectors?\n",
    "\n",
    "* A Matrix is a mathematical object that acts on a (column) vector, resulting in a new vector, i.e. A**x**=**b**. It represents a **move**, in any direction, for a mountain-climber.\n",
    "* An **eigenvector** is the resulting vector from that same transformation, that is also parallel to **x** (some multiple of **x**). It's as if the **new position** of the mountain climber is **in the same direction** as defined by his previous position and the origin (where he started his climb).\n",
    "$$ {A}\\underline{x}=\\lambda \\underline{x} $$\n",
    "$\\lambda$ is its **eigenvalue**. It's **how much** the mountain climber moved from the previous position.\n",
    "\n",
    "\n",
    "### What are eigenvalues?\n",
    "\n",
    "Given $A \\in \\mathbb{R}^{n\\times n}$, $\\lambda$ is the **eigenvalue** of $A$ if there is a non-zero vector $x$, the corresponding **eigenvector**, such that the following is true:\n",
    "\n",
    "$$Ax = \\lambda x, \\;\\; \\text{for} \\; x \\neq 0$$\n",
    "\n",
    "Is there a move that can take a mountain-climber to *some factor* times his original position? If so, the old position is an eigenvector, and the moving factor is its eigenvalue.\n",
    "\n",
    "Formally, given a square matrix $A \\in \\mathbb{R}^{n\\times n}$, we say that $\\lambda \\in \\mathbb{C}$ is an **eigenvalue** of $A$ and $x \\in \\mathbb{C}^n$ is the corresponding **eigenvector**.\n",
    "\n",
    "Intuitively, this definition means that multiplying $A$ by the vector $x$ results in a new vector that points in the same direction as $x$, but is scaled by a factor $\\lambda$.\n",
    "\n",
    "Also note that for any **eigenvector** $x \\in \\mathbb{C}^n$, and scalar $c \\in \\mathbb{C}, A(cx) = cAx = c\\lambda x = \\lambda(cx)$, so $cx$ is also an **eigenvector**. For this reason when we talk about **“the” eigenvector** associated with $\\lambda$, we usually assume that the **eigenvector** is normalized to have length $1$ (this still creates some ambiguity, since $x$ and $−x$ will both be **eigenvectors**, but we will have to live with this).\n",
    "\n",
    "For any $\\lambda$ an eigenvalue of $A$, there is a vector space, the **eigenspace**, that corresponds to $\\lambda$:\n",
    "\n",
    "$$\\{ x : A x = \\lambda x \\}$$\n",
    "\n",
    "Any non-zero vector in this space is an eigenvector. One convenient requirement is that the eigenvector has norm $1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "What are the eigenvalues and eigenvectors of our Combo mountain jump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.linalg as la\n",
    "\n",
    "C = np.array([[20, 11, 11],[9, 4, 9], [11, 7, 5]]) \n",
    "\n",
    "(v, r) = la.eig(C, left = False) # You can read the help, buy the left eigenvectors don't get created without this. \n",
    "d = sp.diag(v)  # by default, eig puts the eigenvalues in a 1-D array. We will need a diagonal matrix in a moment.\n",
    "\n",
    "print(v)\n",
    "print(d)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means that if the mountain climber starts at any of the three positions described by the eigenvectors, she will end up facing in the same direction (from the perspective of an observer at the origin of the mountain), just slightly closer or further away from the observer, by a factor of the eigenvalue associated with the eigenvector.\n",
    "\n",
    "Tell me that's not simple stuff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of Eigenvectors: Markov Chains\n",
    "\n",
    "A **Markov chain** is any *memorlyess* process that can be modeled with a *stochastic transition* matrix. It's like a mountain climber that always forgets her previous position, and only cares about the next (which is the right state of mind if you're climbing *El Capitan* in Yellowstone National Park!)\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/moody.png width = 600 />\n",
    "    Moody's credit rating transitions\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "### Repeated transitions\n",
    "Conjecture: If the transition matrix $P$ is the same after each step (time homogeneous), then the k-step transition probability is.. the k-th power of the transition matrix, $P^k$.\n",
    "\n",
    "I don't think I need to convnce you of this anymore, we illustrated this with the mountain climber and the Combo matrix!\n",
    "\n",
    "And so, the k-step transition is characterized by the matrix $A^k$.\n",
    "\n",
    "### Steady state limit\n",
    "Now what if, in the long term, after many powers of $k$, we suddenly transition to a condition where the states don't change very much? Well, we acheive what is called a **steady-state** condition: \n",
    "\n",
    "$$P^{\\;k+1} = P^{\\;k}$$\n",
    "\n",
    "Stationary distributions are super-interesting to Wall Street, because they **describe the *future***! Suppose we call that future the state vector $\\pi$. Then:\n",
    "\n",
    "$$\\pi = P . \\pi$$\n",
    "\n",
    "Hey, wait a second, does that equation remind you of anything?\n",
    "\n",
    "\n",
    "### Eigenvectors and the future\n",
    "The eigenvectors $X_1, \\cdots X_n$ of $n$ x $n$ matrix $A$, and its associated eigenvalues $λ_i$, are such that:\n",
    "\n",
    "$$A X_i = λ_i X_i$$\n",
    "\n",
    "And so the future, described as state vector $\\pi$, is the eigenvector associated with the eigenvalue 1. Google realized that this corresponds to the steady state condition of a sliver surfer surfing the Web through its hyperlinks. It descibes the probabilistic future state of the sliver surfer. If you rank all of its components from highest probability to lowest probability, it's the Google search page for your search query (corresponding to a silver surfer surfing for Web pages relevant to your query terms)!\n",
    "\n",
    "And *this* is the scary math to prove this:\n",
    "\n",
    "### The Math\n",
    "\n",
    "Define the matrices $P$ and $D$ composed of eigenvectors and eigenvalues:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/pd.png width = 500 />\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "Then:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/appd.png width = 400 />\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "And if:\n",
    "\n",
    "$$ A P = P D$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$ A = P D P^{-1}$$\n",
    "\n",
    "So, **any matrix** can be written as the product of a matrix $P$ whose columns are composed of its eigenvectors, times the matrix composed of its eigenvalues in the main diagonal, times the inverse of $P$.\n",
    "\n",
    "Take the power of that equation:\n",
    "\n",
    "$$ A^2 = P D P^{-1} .  P D P^{-1} = P D . P^{-1} P . D P^{-1} = P D . D P^{-1} = P D^2 P^{-1} $$\n",
    "\n",
    "And if you keep doing this many times, you will see that:\n",
    "\n",
    "$$ A^n = P D^n P^{-1} $$\n",
    "\n",
    "So, just compute $D^n$ and you can find the long-term (steady-state) transition matrix of any Markov chain with the formula above! And do you know how easy it is to compute $D^n$? It is just the matrix with the n-powers of the eigenvalues in the main diagonal! $P$ and $P^{-1}$ have no powers, so they're *easy* to compute if you have the eigenvectors of $A$:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/dn.png width = 200 />\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "So now let's think, if $n$ is a large number, what happens to $\\lambda^n$? \n",
    "\n",
    "What happens if $\\lambda < 1$, and what happens if $\\lambda > 1$?\n",
    "\n",
    "Is it possible that $\\lambda$ is bigger than 1? Does $A^n$ remain *stochastic*?\n",
    "\n",
    "And what if all $\\lambda_i$ are less than one?\n",
    "\n",
    "So what does this tell you?\n",
    "\n",
    "Assume the mountain climber climbs the entire mountain using just one type of jump, let's say the **Twist** matrix jump.\n",
    "\n",
    "Imagine that we are able to represent any position on the mountain as a **linear combination of eigenvectors** of the **Twist** matrix. That, in fact, is a *theorem* that is true if the **Twist** matrix is *non-singular* (has a non-zero determinant).\n",
    "\n",
    "To find the mountain climber's position after $n$ **Twist** jumps, we multiply her position vector by the **Twist** matrix, $n$ times, right? Which corresponds to a single multiplication by the matrix **Twist$^n$**, right? Each application of **Twist** adds a factor of the eigenvalue $λ$ to the coefficient of each eigenvector in the expansion of the state. Recall: The eigenvectors don’t transform much.. They just squish or expand (by a factor of $λ$). So after $n$ transitions, the coefficient of each eigenvector has gained a factor $λ^n$.\n",
    "\n",
    "If λ > 1, this factor would grow without bound as $n$ increases, leading to matrix elements larger than 1, which would catapult the mountain climber to the moon! So, there must not be any eigenvalue strictly larger than one!\n",
    "\n",
    "If λ < 1, this factor would suppress the contribution of the corresponding eigenvector as $n$ increases. It’s fine if this happens to some eigenvectors, but if it happens to *all of them*, the probability interpretation of our state vector would also be ruined, because all entries in our vector would be driven to zero, and the mountain climber would **never move**.\n",
    "\n",
    "So, **there must be at least one eigenvalue greater or equal to one!**\n",
    "\n",
    "The **dominant eigenvector** (eigenvector corresponding to the eigenvalue = 1) of any Markov chain *is* the **steady-state limit (the *long-term future*) of the Markov chain**!\n",
    "\n",
    "The dominant eigenvector will become the *only unsuppressed contribution* to the state vector as $n$ gets large.\n",
    "This eigenvector therefore represents the steady-state towards which every **linear system** tends to.\n",
    "\n",
    "So if we start with a random vector (random mountain climber position), and apply the **Twist** matrix a million times, we will ultimately converge to the dominant eigenvector, since all other eigenvalue-to-the-power-a-million contributions will become suppressed. \n",
    "\n",
    "That is the essence of the **Power method** that we used last lesson to find the dominant eigenvector of our foodchain, in order to rank species by their ecological importance.\n",
    "\n",
    "It's the same method used by Google to rank the WWW!\n",
    "\n",
    "We'll investigate this today with Wikipedia!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver Surfer Equation\n",
    "\n",
    "If we call the WWW transition matrix $M$ and the vector of PageRank values (dominant eigenvector) $r$ for a particular Google query, we have:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{r} = M\\boldsymbol{r}\n",
    "$$\n",
    "\n",
    "But our browsers **have a browser bar**, where the user enters a URL to surf directly to a page (maybe we get that URL from word-of-mouth). So that way, we *can* get to any Web page direclty. In fact, Google did some research that uncovered that about 85% of the time, we just follow links, and 15% of the time, we enter URLs. So we modify the PageRank algorithm with a so-called \"damping factor\", usually taken to be 0.85. \n",
    "\n",
    "85% of the time, the silver surfer follows a link at random but with well-defined probabilities (in the same way a random sampling from a Normal distribution will give us a random-looking process but with well-defined probabilities leading to a normal distribution for its histogram), but for the other 15%, he randomly jumps to\n",
    "any *arbitrary* page. It's as if every page had a low probability link to every\n",
    "other page even if the two pages don't link to each other through hyperlinks.\n",
    "\n",
    "If we call the damping factor $d$, and $\\boldsymbol{1}$ is the Identity matrix, then the modified PageRank equation is:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{r} = dM\\boldsymbol{r} + \\frac{1-d}{n} \\boldsymbol{1}\n",
    "$$\n",
    "\n",
    "or:\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{I} - dM)\\boldsymbol{r} = \\frac{1-d}{n} \\boldsymbol{1}\n",
    "$$\n",
    "\n",
    "I call this equation the **silver surfer formula**, and it shows you the contribution of the URL bar to search results. That is the reason why Google gives out Chrome ***for free***. It needs to **spy** on your URL bar so that it can keep refining the dominant eigenvector for the World Wide Web (have you noticed how aggressive Google has been lately every time we give it a query with another browser?)!\n",
    "\n",
    "And this is the `Power` method (below) we used last lecture to rank species by ecological importance: So if we start with a random vector (random probabilities of being eaten by someone), and apply the **Yummy-yummy** matrix a hundred thousand times, we will ultimately converge to the dominant eigenvector: the most yummiest food, since all other eigenvalue-to-the-power-a-hundred-thousand contributions will become *suppressed*. \n",
    "\n",
    "### Why do we use a damping factor for yummy-yummy? \n",
    "Why do we say that shrimp, on rare occasions, eat sharks :-)?\n",
    "Because some species only eat, and are eaten by no other species! So we cannot have a real Markov chain since all probabilities do not sum to 1. So by adding a damping factor, we get a Markov chain so that the **fixed point theorem** applies. It's as if every species has a low probability to eat every other species, even if they don't usually do (after all, sharks don't eat humans but sometimes they take a bite out of a surfer or two..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(M, damping=0.85, max_iter=10**5):\n",
    "    n = M.shape[0]\n",
    "    r0 = np.full(n, 1/n)\n",
    "    r = r0\n",
    "    for _iter_num in range(max_iter):\n",
    "        rnext = damping * M @ r + (1 - damping) / n\n",
    "        if np.allclose(rnext, r):\n",
    "            break\n",
    "        r = rnext\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big graphs\n",
    "\n",
    "This is a picture I took when I interviewed for Facebook. It's a monitor that shows their network, and by corollary, our planet too, with more technologically advanced locations in highlight:\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/fb.png\" width=500 />\n",
    "</center>\n",
    "\n",
    "I haven't experimented with the facebook graph, yet, although I understand there is an API to download it. Let's download a less politically controversial **big graph** for our lab: A dataset of Wikipedia links from [DBpedia](http://wiki.dbpedia.org/). DBpedia provides structured Wikipedia data available in 125 languages. \n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/wikipedia.jpg\" width=300 />\n",
    "</center>\n",
    "\n",
    "The full DBpedia data set features 38 million labels and abstracts in 125 different languages, 25.2 million links to images and 29.8 million links to external web pages; 80.9 million links to Wikipedia categories, and 41.2 million links to [YAGO](http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/) categories*\". Read about DBPedia [here](http://wiki.dbpedia.org/about).\n",
    "\n",
    "[YAGO](https://en.wikipedia.org/wiki/YAGO_(database)) (Yet Another Great Ontology) is an open source knowledge base developed at the [Max Planck Institute for Computer Science](https://www.cis.mpg.de/) in Saarbrücken, Germany. YAGO has knowledge of more than 10 million entities and contains more than 120 million facts about these entities.The information in YAGO is extracted from Wikipedia (e.g., categories, redirects, infoboxes), [WordNet](https://en.wikipedia.org/wiki/WordNet) (e.g., synsets, hyponymy), and [GeoNames](https://en.wikipedia.org/wiki/GeoNames). \n",
    "\n",
    "The accuracy of YAGO was manually evaluated to be above [95%](https://en.wikipedia.org/wiki/YAGO_(database) on a sample of facts. YAGO is written in [Turtle](https://en.wikipedia.org/wiki/Turtle_(syntax) (Terse RDF-Triple Language), which is similar to [SPARQL](https://en.wikipedia.org/wiki/SPARQL), another RDF query language. And [RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework) is the Resource Description Framework (RDF), a family of W3C specifications originally designed as a metadata data model. It is a general method for conceptual description or modeling of information. It is used in **knowledge management** applications. For example, it's how [Watson](https://en.wikipedia.org/wiki/Watson_(computer) stores facts (I think). \n",
    "\n",
    "`Scikit-Learn` includes Scikit-specific examples, but it's based on SciPy. [SciKit Learn Example](http://scikit-learn.org/stable/auto_examples/applications/wikipedia_principal_eigenvector.html#sphx-glr-auto-examples-applications-wikipedia-principal-eigenvector-py)\n",
    "\n",
    "In upcoming lectures, we'll introduce `Scikit-learn` with **Machine Learning**. Scikit-learn also includes useful data mining functionality such as [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) and [LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). LDA is a statistical model that allows sets of observations to be explained by unobserved (latent) groups that explain why some parts of the data are similar. \n",
    "\n",
    "Let's import the tools we need to explore Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pickle\n",
    "from bz2 import BZ2File\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.decomposition import randomized_svd\n",
    "from sklearn.externals.joblib import Memory\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download straight from the Web to our notebook. You could also download manually from source, but if you have to download *many* files, this technique is much more practical. We'll use `tqdm`, which you learned about, to track download progress. It only tracks per file downloaded, unfortunately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# create this folder first on your hard drive!\n",
    "PATH = 'd:/bigdata/dbpedia3/'\n",
    "\n",
    "URL_BASE = 'http://downloads.dbpedia.org/3.5.1/en/'\n",
    "filenames = [\"redirects_en.nt.bz2\", \"page_links_en.nt.bz2\"]\n",
    "\n",
    "with tqdm(total=len(filenames)) as pbar:\n",
    "    for filename in filenames:\n",
    "        if not os.path.exists(PATH+filename):\n",
    "            print(\"Downloading '%s', please wait...\" % filename)\n",
    "            open(PATH+filename, 'wb').write(urlopen(URL_BASE+filename).read())\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects_filename = PATH+filenames[0]\n",
    "page_links_filename = PATH+filenames[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct a graph **adjacency matrix**, of which Wikipedia pages point to which. We will store this in a square matrix, with a $1$ in position $(r, c)$ indicating that the topic in row $r$ points to the topic in column $c$. As you know, this matrix is super useful, and expressed in sparse matrix format the most efficient representation of the graph.\n",
    "\n",
    "We can then use this matrix for graph mining. For example power $A^2$ will give you all the ways there are to get from one Wikipedia page to another in exactly 2 hyperlinks.\n",
    "\n",
    "Find a good window-based editor, and examine these huge files. On Windows, I use [LTFViewer](https://www.portablefreeware.com/?id=693); it allows you to window-into gigabit-size files. I uploaded it to Blackboard. Only works on Windows. On the Mac, use [HexFiend](http://ridiculousfish.com/hexfiend/), for example.\n",
    "\n",
    "By using a reader like LTFViewer and unzipping content, we see that lines in `redirects_en.nt.bz2` look like this:\n",
    "* <http://dbpedia.org/resource/Mok_Siu_Chung> <http://dbpedia.org/property/redirect> <http://dbpedia.org/resource/Max_Mok> .\n",
    "\n",
    "Who's [Max Mok](https://en.wikipedia.org/wiki/Max_Mok)? 谁是 Max Mok? The most famous actor you've never heard of :-)\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/maxmok.jpg\" width=150 />\n",
    "</center>\n",
    "\n",
    "Let's loop through redirections and create a dictionary of source-to-final-destination. And let's keep an eye on memory all the while.\n",
    "\n",
    "We use [tqdm_notebook](https://pypi.org/project/tqdm/#ipython-jupyter-integration) to keep track of our file processing, and we track memory usage to make sure we don't blow up. *Always do* this when working with big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "def mem_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / psutil.virtual_memory().total\n",
    "\n",
    "mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, .5%. Cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPEDIA_RESOURCE_PREFIX_LEN = len(\"http://dbpedia.org/resource/\")\n",
    "SLICE = slice(DBPEDIA_RESOURCE_PREFIX_LEN + 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `BZ2File` allows you to work directly with zipped formats. It also leverages tqdm-like functionality to inform you of progress. How cool for working with big data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(filename): return (line.split() for line in BZ2File(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redirect(targ, redirects):\n",
    "    seen = set()\n",
    "    while True:\n",
    "        transitive_targ = targ\n",
    "        targ = redirects.get(targ)\n",
    "        if targ is None or targ in seen: break\n",
    "        seen.add(targ)\n",
    "    return transitive_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redirects(redirects_filename):\n",
    "    redirects={}\n",
    "    lines = get_lines(redirects_filename)\n",
    "    return {src[SLICE]:get_redirect(targ[SLICE], redirects) \n",
    "                for src,_,targ,_ in tqdm_notebook(lines, leave=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5441c8eadfc0472aa5a239d5613434d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "redirects = get_redirects(redirects_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample. Run the cell below a few times, Different samples each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "keys = random.sample(list(redirects), 5)\n",
    "print(keys)\n",
    "\n",
    "values = [redirects[k] for k in keys]\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that `Bytes` literals are always prefixed with 'b' or 'B'; they produce an instance of the `bytes` type instead of the `str` type. They may only contain ASCII characters; bytes with a numeric value of 128 or greater must be expressed with escapes. Much better memory-wise to use byte types rather than str types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oke-dokee, memory usage has increased by quite a bit, from .5% to about 5% of my computers's RAM (for reading in dozens of millions of pages). I'm still in very manageable territory, but let's keep an eye out.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/scroodge.jpg\" width=150 />\n",
    "</center>\n",
    "\n",
    "Now let's create a list of articles and limit ourselves to this many Wikipedia articles: 100 million\n",
    "```python\n",
    "limit=100000000\n",
    "```\n",
    "\n",
    "Naaaaah, let's start with a million instead, to have more time to work on the notebook in class.\n",
    "\n",
    "Baby steps, baby!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit= 1000000 #100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary's `setdefault()` method is similar to `get()`, but will set `dict[key]=default` if key is not already in the dictionary.\n",
    "\n",
    "So we're reading an article name in (minus the WWW domain prefix), and building an index map that maps article names to an integer, and then a source list of articles in integer format, and a hyperlinked (destination) article in integer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_item(lst, redirects, index_map, item):\n",
    "    k = item[SLICE]\n",
    "    lst.append(index_map.setdefault(redirects.get(k, k), len(index_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a51bd662e64a24961a06e0ce2c11e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute integer index map\n",
    "index_map = dict() # links->IDs\n",
    "lines = get_lines(page_links_filename)\n",
    "source, destination, data = [],[],[]\n",
    "for l, split in tqdm_notebook(enumerate(lines), total=limit):\n",
    "    if l >= limit: break\n",
    "    add_item(source, redirects, index_map, split[0])\n",
    "    add_item(destination, redirects, index_map, split[2])\n",
    "    data.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), np.sum(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(random.sample(source, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(random.sample(destination, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), np.sum(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = random.sample(list(index_map), 5)\n",
    "print(keys)\n",
    "\n",
    "values = [index_map[k] for k in keys]\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okee-dokee, still at 5% of our computer's RAM (for reading in 1 Million pages). \n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/memory.jpg\" width=200 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(data); n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.sample(list(index_map), 1)\n",
    "print(key)\n",
    "print('has index')\n",
    "value = [index_map[k] for k in key]\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times does this show up in the destination list?\n",
    "dest_index = [i for i,x in enumerate(source) if x == 572]  #483 #1157907\n",
    "if (0 == len(dest_index)):\n",
    "    print('oops, rerun cell above plz!')\n",
    "else:\n",
    "    print(dest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source[600:700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source[dest_index[0]], destination[dest_index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check which page in the source (has index `destination[dest_index[0]]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_name, index in index_map.items():\n",
    "    if index == destination[dest_index[0]]:\n",
    "        print(page_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on Wikipedia, `key` has redirected to `page_name`. Let's verify..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse matrix formats\n",
    "\n",
    "Promised you I would give you more information on sparse matrix formats. Here it is..\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/sparse.png\" width=300 />\n",
    "</center>\n",
    "\n",
    "The `sparse` module of SciPy contains objects to efficiently represent sparse matrices, and sparse matrices are your friends for bigdata processing\n",
    "\n",
    "For sparse matrices, thee is however are a wide array of possible formats, and\n",
    "the *right* format depends on the problem you're on.\n",
    "\n",
    "Let's cover the three most commonly-used formats (COO, CSR, and CSC). For a more complete list, see the\n",
    "comparison table later in this notebook, as well as online documentation for\n",
    "`scipy.sparse`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COO (COOrdinate) format\n",
    "\n",
    "Perhaps the most intuitive is the coordinate, or COO, format.\n",
    "This uses three 1D arrays to represent a 2D matrix $A$.\n",
    "Each of these arrays has length equal to the number of nonzero values in $A$,\n",
    "and together they list (i, j, value) coordinates of every entry that is not\n",
    "equal to 0.\n",
    "\n",
    "- The `row` and `col` arrays, which together specify the location of each\n",
    "  non-zero entry (row and column indices, respectively).\n",
    "- The `data` array, which specifies the *value* at each of those locations.\n",
    "\n",
    "Every part of the matrix that is not represented by the `(row, col)` pairs is\n",
    "considered to be 0.\n",
    "Much more efficient!\n",
    "\n",
    "So, to represent the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array([[ 4,  0, 3],\n",
    "              [ 0, 32, 0]], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "data2 = np.array([4, 3, 32], dtype=float)\n",
    "row = np.array([0, 0, 1])\n",
    "col = np.array([0, 2, 1])\n",
    "\n",
    "s_coo = sparse.coo_matrix((data2, (row, col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.toarray()` method of every sparse format in `scipy.sparse` returns a\n",
    "NumPy array representation of the sparse data.\n",
    "We can use this to check that we created `s_coo` correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identically, we can use the `.A` *property*, which is just like an attribute,\n",
    "but actually executes a function. `.A` is a particularly dangerous property,\n",
    "because it can hide a potentially very large operation: the dense version\n",
    "of a sparse matrix can be orders of magnitude bigger than the sparse matrix\n",
    "itself, bringing a computer to its knees, in just three keystrokes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_coo.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better to use the `toarray()` method\n",
    "wherever it does not impair readability, as it more clearly signals a\n",
    "potentially expensive operation. Use `.A` where it makes\n",
    "the code more readable by virtue of its brevity (for example, when trying to\n",
    "implement a sequence of mathematical equations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** write out the COO representation of the following matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = np.array([[0, 0, 6, 0, 0],\n",
    "               [1, 2, 0, 4, 5],\n",
    "               [0, 1, 0, 0, 0],\n",
    "               [9, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 6, 7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We first list the non-zero elements of the array, left-to-right and\n",
    "top-to-bottom, as if reading a book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_data = np.array([6, 1, 2, 4, 5, 1, 9, 6, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then list the row indices of those values in the same order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_row = np.array([0, 1, 1, 1, 1, 2, 3, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the column indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_col = np.array([2, 0, 1, 3, 4, 1, 0, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily check that these produce the right matrix, by checking equality\n",
    "in both directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_coo0 = sparse.coo_matrix(s2)\n",
    "print(s2_coo0.data)\n",
    "print(s2_coo0.row)\n",
    "print(s2_coo0.col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_coo1 = sparse.coo_matrix((s2_data, (s2_row, s2_col)))\n",
    "print(s2_coo1.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, although the COO format is intuitive, it's not very optimized to\n",
    "use the minimum amount of memory, or to traverse the array as quickly as\n",
    "possible during computations (*data locality* is very important to efficient computation).\n",
    "Look at the COO representation above to help you identify\n",
    "redundant information:\n",
    "Notice all those repeated `1`s in thr row index vector?\n",
    "\n",
    "### CSR (Compressed Sparse Row) format\n",
    "\n",
    "If we use COO to enumerate the nonzero entries row-by-row, rather than in\n",
    "arbitrary order (which the format allows), we end up with many consecutive,\n",
    "repeated values in the `row` array.\n",
    "These can be compressed by indicating the *indices* in `col` where the *next* row\n",
    "starts, rather than repeatedly writing the row index.\n",
    "This is the basis for the *compressed sparse row* or *CSR* format.\n",
    "\n",
    "Advantages of the CSR format:\n",
    "* efficient arithmetic operations CSR + CSR, CSR * CSR, etc.\n",
    "* efficient row slicing\n",
    "* fast matrix vector products\n",
    "\n",
    "Disadvantages of the CSR format:\n",
    "* slow column slicing operations (consider CSC)\n",
    "* changes to the sparsity structure are expensive (consider LIL or DOK)\n",
    "\n",
    "Let's work through the example above. In CSR format, the `col` and `data`\n",
    "arrays are unchanged (but `col` is renamed to `indices`). However, the `row`\n",
    "array, instead of indicating the rows, indicates *where* in `col` each row\n",
    "begins, and is renamed to `indptr`, for \"index pointer\".\n",
    "\n",
    "So, let's look at `row` and `col` in COO format, ignoring `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = [0, 1, 1, 1, 1, 2, 3, 4, 4]\n",
    "col = [2, 0, 1, 3, 4, 1, 0, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each new row begins at the index where `row` changes.\n",
    "The 0th row starts at index 0, and the 1st row starts at index 1, but the 2nd\n",
    "row starts where \"2\" first appears in `row`, at index 5.\n",
    "Then, the indices increase by 1 for rows 3 and 4, to 6 and 7.\n",
    "The final index, indicating the end of the matrix, is the total number of\n",
    "nonzero values (9).\n",
    "So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "indptr = [0, 1, 5, 6, 7, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use these hand-computed arrays to build a CSR matrix in SciPy.\n",
    "We can check our work by comparing the `.A` output from our COO and\n",
    "CSR representations to the numpy array `s2` that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = np.array([6, 1, 2, 4, 5, 1, 9, 6, 7])\n",
    "\n",
    "coo = sparse.coo_matrix((data3, (row, col)))\n",
    "csr = sparse.csr_matrix((data3, col, indptr))\n",
    "\n",
    "print('The COO and CSR arrays are equal: ',\n",
    "      np.all(coo.A == csr.A))\n",
    "print('The CSR and NumPy arrays are equal: ',\n",
    "      np.all(s2 == csr.A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to store large, sparse matrices, and perform computations on them,\n",
    "is incredibly powerful, and can be applied in many domains.\n",
    "\n",
    "For example,\n",
    "one can think of the entire web as a large, sparse, $N \\times N$ matrix.\n",
    "Each entry $X_{ij}$ indicates whether web page $i$ links to page $j$.\n",
    "By normalizing this matrix and solving for its dominant eigenvector,\n",
    "one obtains the so-called PageRank—one of the numbers Google uses to\n",
    "order your search results. (You can read more about this in the next chapter.)\n",
    "\n",
    "As another example, we can represent the human brain as a large $m \\times m$\n",
    "graph, where there are $m$ nodes (positions) in which you\n",
    "measure activity using an MRI scanner.  After a while of measuring,\n",
    "correlations can be calculated and entered into a matrix $C_{ij}$.\n",
    "Thresholding this matrix produces a sparse matrix of ones and zeros. \n",
    "The eigenvector corresponding to the second-smallest eigenvalue of this matrix\n",
    "partitions the $m$ brain areas into subgroups, which, it turns out,\n",
    "are often related to functional regions of the brain [^Newman]!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"landscape\">\n",
    "<table style=\"font-size: 50%;\">\n",
    "<colgroup>\n",
    "<col width=\"2%\" />\n",
    "<col width=\"11%\" />\n",
    "<col width=\"20%\" />\n",
    "<col width=\"20%\" />\n",
    "<col width=\"12%\" />\n",
    "<col width=\"10%\" />\n",
    "<col width=\"11%\" />\n",
    "<col width=\"10%\" />\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th align=\"left\"></th>\n",
    "<th align=\"left\">bsr_matrix</th>\n",
    "<th align=\"left\">coo_matrix</th>\n",
    "<th align=\"left\">csc_matrix</th>\n",
    "<th align=\"left\">csr_matrix</th>\n",
    "<th align=\"left\">dia_matrix</th>\n",
    "<th align=\"left\">dok_matrix</th>\n",
    "<th align=\"left\">lil_matrix</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td align=\"left\"><p>Full name</p></td>\n",
    "<td align=\"left\"><p>Block Sparse Row</p></td>\n",
    "<td align=\"left\"><p>Coordinate</p></td>\n",
    "<td align=\"left\"><p>Compressed Sparse Column</p></td>\n",
    "<td align=\"left\"><p>Compressed Sparse Row</p></td>\n",
    "<td align=\"left\"><p>Diagonal</p></td>\n",
    "<td align=\"left\"><p>Dictionary of Keys</p></td>\n",
    "<td align=\"left\"><p>Row-based linked-list</p></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td align=\"left\"><p>Note</p></td>\n",
    "<td align=\"left\"><p>Similar to CSR</p></td>\n",
    "<td align=\"left\"><p>Only used to construct sparse matrices, which are then converted to CSC or CSR for further operations.</p></td>\n",
    "<td align=\"left\"></td>\n",
    "<td align=\"left\"></td>\n",
    "<td align=\"left\"></td>\n",
    "<td align=\"left\"><p>Used to construct sparse matrices incrementally.</p></td>\n",
    "<td align=\"left\"><p>Used to construct sparse matrices incrementally.</p></td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td align=\"left\"><p>Use cases</p></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Storage of dense sub-matrices</li>\n",
    "<li>Often used in numerical analyses of discretized problems,</li>\n",
    "<li>such as finite elements, differential equations</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Fast and straightforward way of constructing sparse matrices</li>\n",
    "<li>During construction, duplicate coordinates are summed—useful for, e.g., finite element analysis</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Arithmetic operations (supports addition, subtraction, multiplication, division, and matrix power</li>\n",
    "<li>Efficient column slicing</li>\n",
    "<li>Fast matrix-vector products (CSR, BSR can be faster, depending on the problem)</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Arithmetic operations</li>\n",
    "<li>Efficient row slicing</li>\n",
    "<li>Fast matrix-vector products</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Arithmetic operations</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Changes in sparsity structure are inexpensive</li>\n",
    "<li>Arithmetic operations</li>\n",
    "<li>Fast access to individual elements</li>\n",
    "<li>Efficient conversion to COO (but no duplicates allowed)</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Changes in sparsity structure are inexpensive</li>\n",
    "<li>Flexible slicing</li>\n",
    "</ul></td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td align=\"left\"><p>Cons</p></td>\n",
    "<td align=\"left\"></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>No arithmetic operations</li>\n",
    "<li>No slicing</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Slow row slicing (see CSR)</li>\n",
    "<li>Changes to sparsity structure are expensive (see LIL, DOK)</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Slow column slicing (see CSC)</li>\n",
    "<li>Changes to sparsity structure are expensive (see LIL, DOK)</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Sparsity structure limited to values on diagonals</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Expensive for arithmetic operations</li>\n",
    "<li>Slow matrix-vector products</li>\n",
    "</ul></td>\n",
    "<td align=\"left\"><ul>\n",
    "<li>Expensive for arithmetic operations</li>\n",
    "<li>Slow column slicing</li>\n",
    "<li>Slow matrix-vector products</li>\n",
    "</ul></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency matrix for Wikipedia using sparse matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a sparse matrix using Scipy's COO format, and that convert it to CSR.\n",
    "\n",
    "**Questions**: What are COO and CSR?  Why would we create it with COO and then convert it right away?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data), len(source), len(destination), n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sparse.coo_matrix((data, (destination,source)), shape=(n,n), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = sparse.csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(data, destination, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {i: name for name, i in index_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.sample(list(names), 5)\n",
    "print(key)\n",
    "print('has index')\n",
    "value = [names[k] for k in key]\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now in case you want to save intermediate work, you need to find out about [pickling](https://www.datacamp.com/community/tutorials/pickle-python-tutorial#what) in python.\n",
    "\n",
    "### Object serialization in Python: Pickling\n",
    "\n",
    "Read about the standard [here](https://docs.python.org/3/library/pickle.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X, open(PATH+'X.pkl', 'wb'))\n",
    "pickle.dump(index_map, open(PATH+'index_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pickle.load(open(PATH+'X.pkl', 'rb'))\n",
    "index_map = pickle.load(open(PATH+'index_map.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {i: name for name, i in index_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank for Wikipedia using the Power method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first learn how to normalize a sparse matrix (make it column-stochastic), and this method is borrowed from [SciPy](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.matrix.A1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row1: 1,2, row2: 3,4\n",
    "S = sparse.csr_matrix(np.array([[1,2],[3,4]])) \n",
    "Sr = S.sum(axis=0).A1; Sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.data / np.take(Sr, S.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now reuse a slight variant of `power_method()` from last lecture to approximate the dominant eigenvector for Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from last lecture:\n",
    "def power(M, damping=0.85, max_iter=10**5):\n",
    "    n = M.shape[0]\n",
    "    r0 = np.full(n, 1/n)\n",
    "    r = r0\n",
    "    for _iter_num in range(max_iter):\n",
    "        rnext = M @ r\n",
    "        nrm = np.linalg.norm(rnext)\n",
    "        rnext /= nrm\n",
    "        if np.allclose(rnext, r):\n",
    "            break\n",
    "        r = rnext\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slight variant\n",
    "def power_method(A, max_iter=100):\n",
    "    n = A.shape[0]\n",
    "    #A = np.copy(A) #nonono\n",
    "    A.data /= np.take(A.sum(axis=0).A1, A.indices)\n",
    "    #A.data /= np.take(A.sum(axis=0), indices) \n",
    "\n",
    "    scores = np.ones(n, dtype=np.float32) * np.sqrt(A.sum()/(n*n)) # initial guess\n",
    "    with tqdm(total=max_iter) as pbar:\n",
    "        for i in range(max_iter):\n",
    "            scores = A @ scores\n",
    "            nrm = np.linalg.norm(scores)\n",
    "            scores /= nrm\n",
    "            print(nrm)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = power_method(Y, max_iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ex(v):\n",
    "    # np.squeeze() Selects a subset of the single-dimensional entries in the shape\n",
    "    print(', '.join(names[i].decode() for i in np.abs(v.squeeze()).argsort()[-1:-10:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ex(scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And those are the most popular pages on Wikipedia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SVD instead\n",
    "\n",
    "We compare our dominant eigenvector approach with another approach, called the Singular Value Decomposition (SVD) approach, which is a [generalization](https://en.wikipedia.org/wiki/Singular-value_decomposition) of the eigendecomposition of a positive semidefinite normal matrix.\n",
    "\n",
    "Singular value decomposition is essentially trying to reduce a rank R matrix to a rank K < R matrix. It means that we can take a list of R unique vectors, and approximate them as a linear combination of K unique vectors. It's dimensionality reduction for matrices, while PCA is dimensionality reduction for vectors. Here's an illustrative example.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/feynman.png\" width=1000 />\n",
    "</center>\n",
    "\n",
    "We use scikit-learn's exemplar: `randomized_svd()`. Using python's `%time`\n",
    "gives us an estimate of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time U, s, V = randomized_svd(Y, 3, n_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top wikipedia pages according to principal singular vectors\n",
    "show_ex(U.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ex(U.T[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ex(V[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ex(V[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "France, philosopy, and greek mythology appear to be very popular pages. And also Sabermetrics, which is major league baseball data analysis!\n",
    "\n",
    "In this notebook, we looked at different sparse matrix formats and we processed a *big graph* (Wikipedia) using sparse matrix representation. That gave us the horse power to do data science on the dataset. We also briefly mentionned about standard data science alorithms PCA and SVD. We also learned how to `pickle` to cache intermediate results on disk.\n",
    "\n",
    "We used the power method to find the eigenvector corresponding to the largest eigenvalue of our matrix of Wikipedia links, also called the **dominant eigenvector**.  This eigenvector gave us the relative importance of each Wikipedia page, because it's the stochastic vector that the silver surfer tends to after many many traversals of the graph at the speed of light (relative occupations of Web pages). Google originally implemented the PageRank algorithm using mapreduce() methods.\n",
    "\n",
    "* Note: [The Second Eigenvalue of the Google Matrix](https://nlp.stanford.edu/pubs/secondeigenvalue.pdf): has implications for * the convergence rate of the standard PageRank algorithm as the web scales, for the stability of PageRank to perturbations to * the link structure of the web, for the detection of Google spammers, and for the design of algorithms to speed up PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming with `Toolz`\n",
    "\n",
    "I ran this notebook up to 100 Million articles, and it took about 15 minutes to read them in memory. If you want to read over a Billion, you can still do it with streaming. There's a great library in Python that lets you do that, called `Toolz`, and I hope we can play with in in subsequent lectures with another application of `SciKit-learn`. \n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/ready.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "* Next lecture, we do some more advanced graph analysis and we learn about the Fiedler vector. Plotting the graph of Wikipedia using the Fiedler vector would be really interesting. And we also take another look at Wikipedia where we also take *earnings* into account."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
