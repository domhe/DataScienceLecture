{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Spring 19, Lecture 9, Introduction to Machine Learning</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos with material by [Jeff Heaton](https://www.heatonresearch.com) and Douglas Blank, 25 March 2019, [Greek independence day](https://www.britannica.com/topic/Greek-Independence-Day)</div>\n",
    "\n",
    "In this introduction to Machine Learning notebook, we learn about regression, classification, and Neural Netrworks. Additionally, we learn the difference between trained and untrained learning, and we also learm something important about human beings.\n",
    "\n",
    "# Introduction to Machine Learning\n",
    "\n",
    "In the 1960s, scientists thought that one day computers could learn like human beings, and even become artifical human beings. Thus the terminology *Artificial Intelligence* was coined, and we dreamt that one day, we were going to live with robots amidst us, which we were going to program.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/thinkrobot.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "In the mid 1980s, scientists at Canadian univerisites were able to leverage enough computing power with good statistical mathematics to demonstrate the predictive power of artificial neural networks. Then we understood that machines don't have to be programmed to be human, because we, humans (specifically, human brains) are essentially statistical machines.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/anns.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "So, **intelligence** *is statistics*. And emotion is the evolutionary trait that makes human beings want to do statistics when we wake up every morning. More specifically, math is the *genotype* (the evolutionary advantage) and emotion is the *phenotype* (how the evolutionary advantage gets enforced). Emotion drives us to predict, and that in turn challenges us to use our brain, which is a statistical machine that does math in order to predict. So emotion drives us to do math. That's right, when you wake up in the morning, the first thing you want to do is **math**. \n",
    "\n",
    "We have a *metric* for how much we want to predict. If we don't want to predict very much, that is called *stupid*. If we want to predict a lot, that is called *intelligent*. When we're busy predicting we can't actually enjoy the present, so there is alwways a middle ground between intelligent and stupid that we strive for. If we have a lot of advantages in life (like a rich dad or a pretty face or washboard abs), then we don't have to predict very much (because our daddy's money is going to do that talking) so we can afford to be more stupid. If we don't have a lot of advantages on our side, we have to predict more to even things out with the other humans, so we are forced into intelligence. And that is why our species rules the plant, and giraffes do not.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/giraffe.jpg\" width=500 />\n",
    "*Wait, we don't?*\n",
    "</center>\n",
    "\n",
    "But professor, I don't want to do math when I wake up! I want to go to the gym, or make love to my boyfriend. \n",
    "\n",
    "No, I say, you want to do math.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/wakeup.jpg\" width=500 />\n",
    "</center>\n",
    "\n",
    "You want to go to the gym and select the machine that will give you washboard abs because you know washboard abs are sexy and you want to be the *sexiest* student at Northeastern. So you want to do math to predict what it will take to get this six-pack right here:\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/washboardf.jpg\" width=300 />\n",
    "</center>\n",
    "\n",
    "You want to look at the face of your boyfriend and based on the patterns of his smile or his frown, you want to compute the *dominant eigenvector* in order to predict his happiness, because you love him, so making him happier will make *you* happier and keep you together, longer.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/boyfriend.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "So you see, you **want** to do math all the time, and it is this *evolutionary trait* that makes human the #1 species on the planet, and not giraffes or sloths. Your brain is a giant **predictor** that uses past knowledge in order to help you take the right step to maximize your future happiness.\n",
    "\n",
    "That is why I am such a fan of bayesian statistics, and why we learned Bayesian modelling togehter. The \n",
    "Bayesian approach asserts that their is prior knowledge about the distribution of the data and model parameters. This prior knowledge alone is used to make predictions about future observations. Once data is observed, it is used to *modulate* the prior assumptions. That, in a nutshell, is *learning*.\n",
    "\n",
    "And that is why you took this class, so you can learn the math (statistics and linear algebra) in an entertaining way with a good professor, because math taught badly is *soooooo* boring. And me, I want to be reborn as a sloth, my next life.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/sloth-reborn.jpg\" width=600 />\n",
    "</center>\n",
    "\n",
    "So is that it? Intelligence is nothing more that statistics? Does that mean that we're nothing but pleasure machines? The answer to the first question is *Yes*. But the answer to the second questions is *No*. The fact that our brains are a prediciton machine and that emotions is the acquired evolutionary trait that drives us to *predict*, does not say anything about ethics and metaphysics: *Character*, *personality*, *soul*, *spirituality*, and *Ishvara Pranidhana* (dedication, devotion, and surrender of the fruits of one’s practice to a higher power, the 5th Niyama in Yoga) are still central humanistic, not mechanistic, concepts.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/8limbs.png\" width=500 />\n",
    "</center>\n",
    "\n",
    "Also, the next revolution in computing will be *affective computing*, where we endow computers with emotion, not for the purpose to please us, but for the purpose of giving them to the goal to improve their algorithms through reinforcement learning. \n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/sadrobot.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959), more specifically, the ability to *predict* missing columns of rows of data **based on given columns** and also sometimes on **many many rows of *all* columns**.\n",
    "\n",
    "Rows and columns should not scare you this deep into my class by now. It's just a matrix, a move from one position on a mountain to another, right?\n",
    "\n",
    "Machine Learning algorithms are divided into the following categories:\n",
    "\n",
    "**Supervised Learning** : training data includes the “correct” answer (rows of *all* columns)\n",
    "    <ul><li>Regression - algorithms that predict continuous outputs</li>\n",
    "        <li>Categorization - algorithms that predict discrete outputs</li>\n",
    "    </ul>\n",
    "\n",
    "**Unsupervised Learning** : problems where the algorithm is given a data set without any “right answers”. The objective is to find some underlying structure in the data, e.g. clustering (missing columns everywhere)\n",
    "<br/><br/>\n",
    "**Reinforcement Learning** : problems where a sequence of decisions are made as opposed to a single decision (or prediction) \n",
    "<br/><br/>\n",
    "**Learning Theory** : study of how and why (mathematically) a learning algorithm works\n",
    "\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/mlalgos.png\" width=500 />\n",
    "</center>\n",
    "\n",
    "The more important techniques are *linear regression* (supervised), *linear classification* (unsupervised), *regression trees and forests* (unsupervised), and *neural networks* (supervised).\n",
    "\n",
    "Well then, since math is what you wand to do in the morning, let's do the math your brain does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The math for the Bayesian approach and the Frequentist approach\n",
    "\n",
    "### Bayesian approach\n",
    "\n",
    "This is *Bayes' theorem*: <br/><br/>\n",
    "$p(Y|X) = \\frac{p\\left(X|Y\\right)p\\left(Y\\right)}{p\\left(X\\right)}$\n",
    "<br/><br/>\n",
    "\n",
    "Bayes' theorem appears repeatadly in the discussion of machine learning, so we start with a discussion about it.\n",
    "\n",
    "Assume, that we model some process where the model has \n",
    "free parameters contained in the vector $\\mathbf{w}$. **Free parameters** means we haven't fully constrained $\\mathbf{w}$. Now assume that we have some notion of the probability distribution of these parameters, $p(\\mathbf{w})$,\n",
    "called the *prior*. That is, we assume that **some** set of values from some space (e.g. the real numbers) is a possible best choice \n",
    "for $\\mathbf{w}$ with some probability $p(\\mathbf{w})$. Finally, assume that we observe a set of data, $\\mathbf{D}$, for the output we are attempting to predict with our model. Our objective is to solve a *reverse problem*: find\n",
    "the *best* set of parameters $\\mathbf{w}$ yielding the observed data. How we choose the *best* set is the challenge of machine learning. But once you find that model, you have *learned* the data by modeling the process that generates it. We did this in class with the probabilistic library `PyMc3`. So if I give you datapoints and you look at its histogram and you guess a good analytic model of that histogram (e.g. a *beta* distribution), then `PyMC3` will find the model for you!\n",
    "\n",
    "In the Bayesian approach we attempt to maximize the *the probability of the parameters given the data*, \n",
    "$p(\\mathbf{w}|D)$, known as the *posterior*. Using Bayes' theorem we can express the *posterior* as <br/><br/>\n",
    "\n",
    "$p(\\mathbf{w}|D) = \\frac{p(D|\\mathbf{w})p(\\mathbf{w})}{p(D)}$ <br/><br/>\n",
    "\n",
    "In order to apply a Bayesian approach, we must formulate models for both the *prior*, $p(\\mathbf{w})$, and the *likelihood function*, $p(D|\\mathbf{w})$. Given\n",
    "these models and some data, we can compute appropriate values for our free parameter vector $\\mathbf{w}$ by maximizing \n",
    "$p(\\mathbf{w}|D)$, which is proportional ($\\propto$) to $p(D|\\mathbf{w})p(\\mathbf{w})$. \n",
    "\n",
    "### Frequentist approach\n",
    "\n",
    "The frequentist approach, or **maximum likelihood estimation** (MLE), ignores the formulation of a *prior*, and goes directly to maximizing the likelihood function to find the model parameters. Thus, the frequentist approach can be described as maximizing *the probability of the data given the parameters*.\n",
    "\n",
    "A frequentist approach views the data $D$ as fixed and attempts to determine the model parameters $w$ by maximizing the likelihood function $p(D|\\mathbf{w})$.\n",
    "\n",
    "We assume we have specified a probability density model $p_{\\mathbf{w}}(d)$ for the observed data elements ${d_i \\in D}$ that is parameterized by $\\mathbf{w}$, i.e. $p$ is a **parametric model** for the distribution of $D$. \n",
    "\n",
    "For example, if $D$ has a normal distribution with mean $\\mu$ and variance $\\sigma^2$ (in which case the model $w$ is fully determined when $\\mu$ and $\\sigma$ are determined), then \n",
    "\n",
    "$$p_{\\mathbf{w}}(d) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-(d-\\mu)^2/2\\sigma^2}$$\n",
    "\n",
    "and the likelihood function is\n",
    "\n",
    "$$L(\\mathbf{w}; D) = \\prod_{i=1}^N p_{\\mathbf{w}}(d_i)$$\n",
    "\n",
    "where $N$ is the number of elements in $D$. That's because the probabiliy of $a$ and $b$ happening is $p(a) * p(b)$ if $a$ and $b$ are independent events. Thus the likelihood function is simply the product of the probability of each individual data point $d_i \\in D$ under the probability model $p_{\\mathbf{w}}$, implicitly assuming these data points are independent events. \n",
    "\n",
    "Out of mathematical convenience, we will most often work with the *log-likelihood* function (which turns the product into a sum by properties of the log function), i.e. the logarithm of $L(\\mathbf{w}; D)$\n",
    "\n",
    "$$l(\\mathbf{w};D) = \\sum_{i=1}^N \\log p_{\\mathbf{w}}(d_i)$$\n",
    "\n",
    "because $\\log(ab) = \\log(a) + \\log(b)$. Why are we allowed to do this? Because the logarithm of a function does not change the monotonicity (increasing or decreasing) of the function, it just crunches down its shape.\n",
    "\n",
    "The method of maximum likelihood chooses the value $\\mathbf{w} = \\widehat{\\mathbf{w}}$ that maximizes the *log-likelihood* function above. That's it!. Students typically find this very complicated. So if you kind of understand this, you are golden!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression and the *General Linear Model*\n",
    "\n",
    "*Assume* that the target variable (what we have to guess, based on the independent variable $x$ - assume your boyfriend's mood based on his facial expression $x$) is described by <br/><br/>\n",
    "    $t = y(\\mathbf{x},\\mathbf{w}) + \\epsilon$\n",
    "    <br/><br/>\n",
    "where $y(\\mathbf{x},\\mathbf{w})$ is an as of yet undefined function of $\\mathbf{x}$ and $\\mathbf{w}$, and $\\epsilon$ is a <font color=\"red\"><em>Gaussian</em></font> distributed noise component. \n",
    "\n",
    "Let's define a **model** for $y(\\mathbf{x},\\mathbf{w})$. \n",
    "\n",
    "The *general linear regression* model is defined as follows\n",
    "\n",
    "$$y(\\mathbf{x},\\mathbf{w}) = \\sum_{j=0}^{M-1} w_j\\phi_j(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{\\phi}(\\mathbf{x})$$\n",
    "\n",
    "where $\\mathbf{x}$ is a $D$ dimensional input vector, $M$ is the number of free parameters in the model, $\\mathbf{w}$ is a column vector of the free parameters, and \n",
    "\n",
    "$$\\phi(\\mathbf{x}) = {\\phi_0(\\mathbf{x}),\\; \\phi_1(\\mathbf{x}), \\ldots,\\;\\phi_{M-1}(\\mathbf{x})}$$\n",
    "\n",
    "with \n",
    "\n",
    "$$\\phi_0(\\mathbf{x})=1$$\n",
    "\n",
    "$\\phi$ is a set of **basis functions** where \n",
    "    each $\\phi_i$ is in the real valued function space \n",
    "    $\\\\{f \\in \\mathbf{R}^D\\Rightarrow\\mathbf{R}^1\\\\}$. \n",
    "    \n",
    "**Basis functions** means you can express *any* function as a linear combination of these basis functions, much like you can express any 3D vector as a linear combination of the basis functions $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$. \n",
    "    \n",
    "It is important to note that the set of basis functions, $\\phi$, <font color=\"red\">*need\n",
    "not be linear*</font> with respect to $\\mathbf{x}$. That is why this model is called the ***general* linear regression** model.\n",
    "\n",
    "Further, note that this model defines an entire class of models. In order to \n",
    "contruct an actual predictive model for some observable quantity, we will have to make a further assumption on the choice of the set of basis functions, $\\phi$. \n",
    "\n",
    "Note that that $\\mathbf{w}^T$ is an $1 \\times M$ vector and that $\\mathbf{\\phi}(\\mathbf{x})$ is a $M \\times 1$ vector so that the target, $y$ is a scalar. This can be extended to $K$ dimensional target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "We first obtain observation training data, $\\mathbf{t}$, and the *best* value of $\\mathbf{w}$, is that which maximizes the likelihood function, $p(\\mathbf{t}\\;|\\;\\mathbf{w})$.\n",
    "\n",
    "Under the Gaussian noise condition it can be shown \\[`math snip snip..`\\] that the maximum likelihood function for the training data is:\n",
    "    \n",
    "$$p(\\mathbf{t}\\;|\\;\\mathbf{X},\\mathbf{w},\\sigma^2) = \\prod_{n=1}^N \\mathcal{N}(t_n\\;|\\;\\mathbf{w}^T\\phi(\\mathbf{x}_n),\\;\\sigma^2) = \\prod_{n=1}^N \\mathcal{N}(\\mathbf{w}^T\\phi(\\mathbf{x}_n)@t_n,\\;\\sigma^2)$$\n",
    "    \n",
    "where $\\mathbf{X}=\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N\\}$ is the input value set for the corresponding $N$ oberved output values contained in the vector \n",
    "$\\mathbf{t}=\\{\\mathbf{t}_1,\\ldots,\\mathbf{t}_N\\}$, and $\\mathcal{N}(\\mu,\\sigma^2)$ is the Normal Distribution (Gaussian).\n",
    "    \n",
    "Thus, taking its base $e$ log:\n",
    "\n",
    "$$\\ln(p(\\mathbf{t}\\;|\\;\\mathbf{X},\\mathbf{w},\\sigma^2)) =\\frac{N}{2}\\ln\\frac{1}{\\sigma^2} -\\frac{N}{2}\\ln(2\\pi) - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N\n",
    "    \\{t_n -\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\}^2$$\n",
    "    \n",
    "Setting the derivative with respect to $\\mathbf{w}$ of the logarithm of the maximum likelihood above equal to zero, one can obtain \\[`math snip snip..`\\] the maximum likelikhood parameters given by the <em>normal equations</em>:\n",
    "    \n",
    "$$\\mathbf{w}_{ML} = \\left(\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^T\\mathbf{t}$$\n",
    "\n",
    "where $\\Phi$ is the $N \\times M$ <em>design matrix</em> with elements $\\Phi_{n,j}=\\phi_j(\\mathbf{x}_n)$, and $\\mathbf{t}$ is the $N \\times K$\n",
    "    matrix of training set target values (for $K=1$, it is simply a column vector). Note that $\\mathbf{\\Phi}^T$ is a $M \\times N$ matrix, so that $\\mathbf{w}_{ML}=\\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^T\\mathbf{t}$ is \n",
    "$(M \\times N)\\times(N \\times M)\\times(M\\times N)\\times(N \\times K) = M \\times K$, where $M$ is the number of free parameters and $K$ is the number of predicted \n",
    "target values for a given input. <br/>\n",
    "</p>\n",
    "\n",
    "Note that the only term in the likelihood function that depends on $\\mathbf{w}$ is the last term. Thus, *<font color=\"red\">maximizing the likelihood\n",
    "function with respect to $\\mathbf{w}$ __under the assumption of Gaussian noise__ is equivalent to minimizing a \n",
    "sum-of-squares error function</font>*  $\\sum_{n=1}^N\n",
    "    \\{t_n -\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\}^2$. That is why a probabilistic optimization problem reduces to a least-square problem.\n",
    "\n",
    "The quantity, $\\mathbf{\\Phi}^\\dagger=\\left(\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\right)^{-1}\\mathbf{\\Phi}^T$ is known as the \n",
    "[**Moore-Penrose pseudo-inverse**](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of $\\Phi$. When $\\Phi^T\\Phi$ is invertible, the pseudo-inverse is \n",
    "equivalent to the inverse. When this condition fails, the pseudo-inverse can be found with techniques such as **singular value decomposition** (SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Linear Data\n",
    "\n",
    "Let's generate data for $y = m*x + b + \\epsilon $ where $\\epsilon$ is a random Gaussian component with zero mean. \n",
    "    \n",
    "Given this data, let's apply the maximum likelihood solution to find values for parameters $m$ and $b$. \n",
    "\n",
    "Given that we know our data is linear, we chose basis functions $\\phi_0(x)=1$ and $\\phi_1(x)=x$. Thus, our \n",
    "our model will be $y=\\theta_0\\phi_0(x) + \\theta_1\\phi_1(x)$, where presumabely the solution should yield $\\theta_0 \\approx b$ and $\\theta_1 \\approx m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# in order to compare between examples, set a seed in random\n",
    "seed = 123456789\n",
    "np.random.seed(seed)\n",
    "\n",
    "def y(x,m,b,mu=0,sigma=1): \n",
    "    return m*x + b + np.random.normal(mu,sigma,1)[0]\n",
    "\n",
    "# training data, with N data points\n",
    "N = 101\n",
    "M = 2\n",
    "t = np.empty(N)\n",
    "domain_bound = 1.0 / N\n",
    "domain = np.empty(N)\n",
    "\n",
    "for i in range(N): \n",
    "    domain[i] = i * domain_bound\n",
    "    \n",
    "for i in range(N): \n",
    "    t[i] = y(x=domain[i],m=4.89,b=0.57)\n",
    "    \n",
    "# design matrix, phi, N X M: basis functions $\\phi_0(x)=1$ and $\\phi_1(x)=x$\n",
    "phi = np.array([np.ones(N), domain]).T\n",
    "print(phi[0:5])\n",
    "\n",
    "# find the solution. In this case case phi.T * phi is invertible, so do the folloing:\n",
    "temp1 = np.linalg.inv(np.dot(phi.T,phi)) #inverse of phi.T X phi\n",
    "temp2 = np.dot(temp1, phi.T)\n",
    "w1 = np.dot(temp2,t) #solution\n",
    "print('w1=', w1)\n",
    "\n",
    "# assuming that phi.T X phi was not invertible we could find the pseudo inverse using the pinv function.\n",
    "# We expect to obtain the same solution!\n",
    "phi_pi = np.linalg.pinv(phi)\n",
    "w2 = np.dot(phi_pi,t)\n",
    "print('w2=', w2)\n",
    "\n",
    "# compute the model predicted values for the training data domain\n",
    "predicted_t = [w2[0]+w2[1]*x for x in domain]\n",
    "plt.plot(domain,t)\n",
    "plt.plot(domain,predicted_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok neat, this is MLE and yet it looks just like linear least squares! Isn't it neat how your brain does [algebra](https://en.wikipedia.org/wiki/Algebra), and it looks like [geometry](https://en.wikipedia.org/wiki/Geometry)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Trigonometric Data\n",
    "\n",
    "One common misconception regarding regression **under the *General* Linear model** (GLM) is that the basis functions are required to be linear. \n",
    "\n",
    "This is not the case. Indeed, the basis functions need not even be polynomial! They must be <em>linearly independent</em>, i.e. orthogonal. It is only the dependence on the model parameters, $\\mathbf{w}_{ML}$, that *must* be linear. \n",
    "\n",
    "An example of a nonlinear parameter model would be $y=\\exp(a)\\sin(x)$ where $a$ is the free parameter.\n",
    "\n",
    "Let's generate trigonometric data of the form $y = a + b\\sin(x) + c\\cos(x) + \\epsilon $ where again $\\epsilon$ is a random Gaussian component with zero mean. \n",
    "\n",
    "Here we chose basis functions $\\phi_0=1$, $\\phi_1(x)=sin(x)$ and $\\phi_2(x)=cos(x)$. If you're wondering if we're cheating a bit here, the answer is yes. In reality, we may not know ahead of time what the appropriate basis functions for the observed data should be. The appropriate choice may be suggested by the data, knowledge of the problem, and other machine learning techniques. It's like when we did probabilistic programming with `PyMC3`: You take the histogram of the data and you *guess* a probability distribution shape. This is similar: we *guess* basis functions. This is a *hyperparameter* of the model.\n",
    "\n",
    "In this example, our model will be $y=\\theta_0\\phi_0(x) + \\theta_1\\phi_1(x) + + \\theta_2\\phi_2(x)$, where presumably the solution should yield $\\theta_0 \\approx a$, $\\theta_1 \\approx b$ and $\\theta_2 \\approx c$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# in order to compare between examples, set a seed in random\n",
    "seed = 123456789\n",
    "np.random.seed(seed)\n",
    "\n",
    "def y(x,a,b,c,mu=0,sigma=1): \n",
    "    return a + b*math.sin(x) + c*math.cos(x) + np.random.normal(mu,sigma,1)[0]\n",
    "\n",
    "# training data, with N data points\n",
    "N = 101\n",
    "M = 3\n",
    "t = np.empty(N)\n",
    "domain = np.empty(N)\n",
    "domain_bound = 4.0*math.pi/N\n",
    "\n",
    "for i in range(N): \n",
    "    domain[i] = i * domain_bound\n",
    "for i in range (N): \n",
    "    t[i] = y(x=domain[i],a=1.85,b=0.57,c=4.37)\n",
    "    \n",
    "# design matrix, phi, N X M\n",
    "c1 = [math.sin(x) for x in domain]\n",
    "c2 = [math.cos(x) for x in domain]\n",
    "phi = np.array([np.ones(N),c1,c2]).T\n",
    "\n",
    "# find the solution. In this case case phi.T X phi is invertible so do the folloing:\n",
    "temp1 = np.linalg.inv(np.dot(phi.T,phi)) #inverse of phi.T X phi\n",
    "temp2 = np.dot(temp1, phi.T)\n",
    "w1 = np.dot(temp2,t) #solution\n",
    "\n",
    "print ('w1=', w1)\n",
    "\n",
    "# assuming that phi.T X phi was not invertible we could find the pseudo inverse using the pinv function\n",
    "# we expect to obtain the same solution\n",
    "phi_pi = np.linalg.pinv(phi)\n",
    "w2 = np.dot(phi_pi,t)\n",
    "print ('w2=', w2)\n",
    "\n",
    "# compute the model predicted values for the training data domain\n",
    "predicted_t = [w2[0]+w2[1]*math.sin(x)+w2[2]*math.cos(x) for x in domain]\n",
    "plt.plot(domain,t)\n",
    "plt.plot(domain, predicted_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Stochastic Gradient Descent (SGD)\n",
    "\n",
    "In cases where the training data set is *very large* or data is received *in a stream*, a direct solution using the normal equations may not be possible, just like solving google's silver surfer equation *exactly*, *analytically*, is not possible and so we use the Power method involving knowledge of the dominant eigenvector (that its associated eigenvalue is 1).\n",
    "\n",
    "And so here the **tractable approach** is the so-called **stochastic gradient descent** algorithm, which is the basis of the loss function of most artificial neural network models. \n",
    "\n",
    "If the total error function, $TE$, is the sum of a given error function, $E$, evaluated at each of the $N$ training inputs, $TE = \\sum_{i=1}^N E(\\mathbf{x}_i)$ then the stochastic gradient descent algorithm is\n",
    "\n",
    "$$\\mathbf{w}^{\\tau + 1} = \\mathbf{w}^\\tau - \\eta \\bigtriangledown E_\\tau$$\n",
    "\n",
    "where ${\\tau}$ is the iteration number and $\\eta$ is a learning rate parameter. \n",
    "\n",
    "For this type of total error function, the order of evaluation does not change the result. If the error function is the sum-of-squares function, then the algorithm is\n",
    "\n",
    "$$\\mathbf{w}^{\\tau + 1} = \\mathbf{w}^\\tau + \\eta \\left(t_n - \\mathbf{w}^{(\\tau)T}\\phi_n\\right)\\phi_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Repeat example 2 with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#in order to compare between examples, set a seed in random\n",
    "seed = 123456789\n",
    "np.random.seed(seed)\n",
    "\n",
    "def y(x,a,b,c,mu=0,sigma=1): \n",
    "    return a + b*math.sin(x) + c*math.cos(x) + np.random.normal(mu,sigma,1)[0]\n",
    "\n",
    "N = 101\n",
    "M = 3\n",
    "w = np.zeros((M,1))\n",
    "phi = np.empty((M,1))\n",
    "eta = 0.25\n",
    "\n",
    "# create arrays to store the values as they are generated so they can be plotted at the end\n",
    "x = np.empty(N)\n",
    "t = np.empty(N)\n",
    "domain_bound = 4.0*math.pi/N\n",
    "for i in range(N):\n",
    "    x[i] = i * domain_bound\n",
    "    t[i] = y(x[i], a=1.85, b=0.57, c=4.37)\n",
    "    phi = np.array([[1], [math.sin(x[i])], [math.cos(x[i])]]) \n",
    "    w = w + eta*(t[i] - np.dot(w.T,phi))*phi #the learning model\n",
    "\n",
    "print(w.T)\n",
    "\n",
    "# compute the model predicted values for the training data domain\n",
    "predicted_t = [w[0] + w[1]*math.sin(item) + w[2]*math.cos(item) for item in x]\n",
    "plt.plot(x,t)\n",
    "plt.plot(x,predicted_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yay :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Over and Under Fitting\n",
    "\n",
    "The biggest issue in machine learning is over-fitting to the data, so that the model fits the data *exactly* and predicts new data *badly*. The technique used to counter this effect is called **regularization**.\n",
    "\n",
    "The general total error function <em>with a regularization term</em> is given by\n",
    "\n",
    "$$E_D(\\mathbf{w}) + \\lambda E_W(\\mathbf{w})$$\n",
    "\n",
    "where $\\lambda$ is the regularization coefficient and $E_W$ is the regularization term. \n",
    "\n",
    "A commonly used regularization term is the sum-of-squares of the model parameter elements\n",
    "\n",
    "$$E_W(\\mathbf{w}) = \\frac1{2}\\mathbf{w}^T\\mathbf{w}$$\n",
    "    \n",
    "known as the <em>weight decay</em> regularizer. The goal here is to reduce the variance of our model. This regularization terms leads to the optimal solution, assuming a linear regression model with Gaussian noise on the training data, of\n",
    "\n",
    "$$\\mathbf{w} = \\left(\\lambda \\mathbf{I} + \\mathbf{\\Phi}^T \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^T\\mathbf{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Overfitting\n",
    "\n",
    "In this example, we use the same training data as Example 1, except that here, the model is erroneously chosen to be a **7th order polynomial**!\n",
    "\n",
    "This example is somewhat contrived, but it illustrates the point that an overfit model can be corrected to some extent using regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#in order to compare between examples, set a seed in random\n",
    "seed = 123456789\n",
    "np.random.seed(seed)\n",
    "\n",
    "def y(x,m,b,mu=0,sigma=1): \n",
    "    return m*x + b + np.random.normal(mu,sigma,1)[0]\n",
    "\n",
    "def el_pow(x,pow):\n",
    "    temp = x\n",
    "    for i in range(pow-1):\n",
    "        temp = temp * x\n",
    "    return temp\n",
    "\n",
    "def prediction(params, x):\n",
    "    pred = 0\n",
    "    for i in range(len(params)):\n",
    "        pred += params[i] * math.pow(x,i)\n",
    "    return pred\n",
    "\n",
    "# training data, with N data points\n",
    "N = 101\n",
    "M = 8\n",
    "t = np.empty(N)\n",
    "domain = np.empty(N)\n",
    "domain_bound = 1.0/N\n",
    "\n",
    "for i in range(N): \n",
    "    domain[i] = i*domain_bound\n",
    "for i in range(N): \n",
    "    t[i] = y(x=domain[i], m=4.89,b=0.57)\n",
    "\n",
    "# find the solution without using regularization\n",
    "# design matrix, phi, N X M\n",
    "phi = np.array([np.ones(N),domain, el_pow(domain,2), el_pow(domain,3), el_pow(domain,4), el_pow(domain,5), el_pow(domain,6), el_pow(domain,7)]).T\n",
    "temp1 = np.linalg.inv(np.dot(phi.T,phi)) #inverse of phi.T X phi\n",
    "temp2 = np.dot(temp1, phi.T)\n",
    "w1 = np.dot(temp2, t) #solution\n",
    "\n",
    "print ('w1=',w1)\n",
    "predicted_t = [prediction(w1,x) for x in domain]\n",
    "\n",
    "#find the regularized solution\n",
    "lam = 0.1\n",
    "temp1 = np.linalg.inv(lam * np.eye(M) + np.dot(phi.T,phi))\n",
    "temp2 = np.dot(temp1, phi.T)\n",
    "w2 = np.dot(temp2, t)\n",
    "print ('w2=',w2)\n",
    "predicted_t_reg = [prediction(w2,x) for x in domain]\n",
    "\n",
    "#add some plots\n",
    "plt.plot(domain,t)\n",
    "plt.plot(domain,predicted_t)\n",
    "plt.plot(domain,predicted_t_reg)\n",
    "plt.legend((\"training\",\"un-regularized\",\"regularized\"), loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><font color=\"grey\">*The math here is a bit complex, so skip this section of you feel like it* :-)</font></div>\n",
    "# 6. Bayesian Estimation\n",
    "\n",
    "Here we consider a fully **Bayesian approach** to the regression problem. First note, that our fundamental **model** assumptions still hold. Namely, we assume that the observation we are trying to predict is modeled by\n",
    "\n",
    "$$t = y(\\mathbf{x},\\mathbf{w}) + \\epsilon$$\n",
    "\n",
    "where $\\epsilon$ is a Gaussian distributed noise component. \n",
    "\n",
    "Additionally, the assumption of a linear dependence on the model parameters, $\\mathbf{w}$ also still holds:\n",
    "\n",
    "$$y(\\mathbf{x},\\mathbf{w}) = \\sum_{j=0}^{M-1} w_j\\phi_j(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{\\phi}(\\mathbf{x})$$\n",
    "\n",
    "Recall that the Bayesian approach implies that the *best* model parameters, $\\mathbf{w}$, are those that maximize the **posterior** probability, which from Bayes' Theorem is given by:\n",
    "\n",
    "$$p(\\mathbf{w}|\\mathbf{t}) = \\frac{p(\\mathbf{t}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathbf{t})}$$\n",
    "\n",
    "### Prior Model\n",
    "The first step in formulating a Bayesian model is to construct a model for the *prior* probability model, $p(\\mathbf{w})$. \n",
    "\n",
    "In general, any probability distribution model could be chosen to \n",
    "model the prior. The appropriate choice is dependent on different factors including prior knowledge of the problem and mathematical convenience. Often, the prior is chosen to be the [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior) of the \n",
    "likelihood function. This is a choice of mathematical convenience because it implies that the *posterior* can be derived analytically. It is often a reasonable choice, as is this case here for the problem of linear regression. \n",
    "\n",
    "Thus, given the Gaussian distribution used for the likelihood function above, we assume a Gaussian distribution for our *prior* of the form:\n",
    "\n",
    "$$p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}\\;|\\;\\mathbf{m}_0, \\; \\mathbf{S}_0)$$\n",
    "\n",
    "where $\\mathbf{m}_0$ is the prior **mean** and $\\mathbf{S}_0$ is the prior **covariance**. \n",
    "\n",
    "We will *assume* $\\mathbf{m}_0 = 0$ and an infinitely broad prior so that $\\mathbf{S}_0=\\alpha^{-1}\\mathbf{I}$ where $\\alpha$ is a precision parameter that we will have to choose. Given these choices of *prior* and *likelihood* functions the *posterior* probability is given by\n",
    "\n",
    "$$p(\\mathbf{w}|\\mathbf{t}) = \\mathcal{N}\\left(\\mathbf{w}\\;|\\;\\mathbf{m}_N, \\;\\mathbf{S}_N\\right)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\mathbf{m}_N = \\beta \\; \\mathbf{S}_N \\mathbf{\\Phi}^T \\mathbf{t}$$\n",
    "\n",
    "$$\\mathbf{S}_N^{-1} = \\alpha \\;\\mathbf{I} + \\beta \\; \\mathbf{\\Phi}^T\\mathbf{\\Phi}$$\n",
    "\n",
    "where $\\beta = 1/\\sigma^2$ is the inverse variance of the random noise component associated with the target variable, \n",
    "$t = y(\\mathbf{x},\\mathbf{w}) + \\mathcal{N}(0, \\sigma^2)$. \n",
    "\n",
    "Finally, the log of the *posterior* is seen to be the sum of the log likelihood **and** the log of the prior:\n",
    "\n",
    "$$\\ln p(\\mathbf{w}\\;|\\;\\mathbf{t}) = -\\frac{\\beta}{2}\\sum_{n=1}^N\\left[{t_n - \\mathbf{w}^T \\phi(\\mathbf{x}_n)}\\right]^2 - \\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w} + constant$$\n",
    "\n",
    "**NOTE:** <font color=\"red\">Maximizing the posterior function with respect to $\\mathbf{w}$ __under the assumption of Gaussian noise and a Gaussian prior__ is equivalent to the least-squares error solution \n",
    "with the addition of a regulariztion term $\\lambda = \\alpha/\\beta$.</font>\n",
    "\n",
    "Rather than find a point estimate for $\\mathbf{w}$ by maximizing the posterior and thereby make point predictions for the target variable $t$, it is more instructive to use the posterior to formulate a *predictive distribution* for $t$. For **our model assumptions** this is given by\n",
    "\n",
    "$$p(t\\;|\\;\\mathbf{t},\\alpha,\\beta) = \\int p(t\\;|\\;\\mathbf{w},\\beta) \\; p(\\mathbf{w}\\;|\\;\\mathbf{t}, \\alpha, \\beta) \\;d\\mathbf{w} = \\mathcal{N}(t\\;|\\;\\mathbf{m}_N^T\\phi(\\mathbf{x}), \\frac{1}{\\beta} + \\phi(\\mathbf{x})^T \\mathbf{S}_N \\phi(\\mathbf{x}))$$\n",
    "\n",
    "where a point estimate of $t$ is given my the mean $\\mu = \\mathbf{m}_N^T\\phi(\\mathbf{x})$ and an estimate of the uncertainty is given by the standard deviation \n",
    "$\\sigma_N^2(\\mathbf{x}) = \\frac{1}{\\beta} + \\phi(\\mathbf{x})^T \\mathbf{S}_N \\phi(\\mathbf{x} )$\n",
    "\n",
    "There is one final issue with completing the Bayesian model, namely the determination of $\\alpha$ and $\\beta$. This can be done in a fully Bayesian manner by developing prior models but this tends to make the equations intractible. Instead the so called *evidence function* approach is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Bayesian estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# in order to compare between examples, set a seed in random\n",
    "seed = 123456789\n",
    "np.random.seed(seed)\n",
    "\n",
    "alpha = 0.4\n",
    "beta = 5.0\n",
    "\n",
    "def y(x,coefs, mu=0, sigma=1.0/beta): \n",
    "    ans = 0\n",
    "    for i in range(len(coefs)): \n",
    "        ans += coefs[i]*math.pow(x,i)\n",
    "    return ans + np.random.normal(mu,sigma,1)[0]\n",
    "    \n",
    "# training data, with N = 101 data points\n",
    "N = 101\n",
    "M = 4\n",
    "t = np.empty(N)\n",
    "domain = np.empty(N)\n",
    "domain_bound = 3.0/N\n",
    "\n",
    "for i in range(N): \n",
    "    domain[i] = i*domain_bound\n",
    "for i in range(N): \n",
    "    t[i] = y(x=domain[i],coefs=[1.75, 0.25, -1.0])\n",
    "\n",
    "# Let's assume that we want to fit a 3rd order polynomial to the data even though we know its a second order\n",
    "# polynomial. Given the Bayesain approach, we should see that unecessary terms are damped out. We have \n",
    "# y = phi_0 + phi_1 * x + phi_2 x^2 + phi_3 x^4\n",
    "\n",
    "# design matrix, phi, N X M where N = 101 and M = 4\n",
    "d2 = domain * domain\n",
    "phi = np.array([np.ones(N),domain, d2, d2 * domain]).T\n",
    "alphaI = alpha * np.eye(M)\n",
    "SN = np.linalg.inv(alphaI + beta * np.dot(phi.T,phi)) #posterior variance\n",
    "mN = beta * np.dot(np.dot(SN, phi.T), t)\n",
    "point_estimates = [np.dot(mN, phi[i]) for i in range(N)]\n",
    "uncertain_t = [1.0/beta + np.dot(np.dot(phi[i].T, SN), phi[i]) for i in range(N)]\n",
    "plt.plot(domain,t)\n",
    "plt.errorbar(domain,point_estimates, uncertain_t, ecolor = \"red\")\n",
    "plt.legend(('training','Bayes'),loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Conclusion\n",
    "\n",
    "Why did we spend so much time on linear regression? Because Machine Learning ***is*** regression: Building a model of the data so we can throw away the data and keep the model to predict future action. And **linear** is the simplest kind of regression. So now we can move on to more complicated models.\n",
    "\n",
    "And since now you know your brain does math, didn't you want to know what kind of math your brain does? \n",
    "\n",
    "Note that *you don't have to learn this math* because `SciPy` does this for you with a few API calls. You can also use `PyMC3` to build a probabilistic program to determine your model parameters, and *we did this in class*. \n",
    "\n",
    "In short, whether by math or procedurally with algorithms, *that* is what your brain does when you try to find a new girlfriend: You remember all your *failed attempts*, and you build a mental model of what it takes to woo a Northeastern girl successfully and you *don't* repeat the same mistakes!\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/nu-chick.png\" width=500 />\n",
    "</center>\n",
    "\n",
    "Too difficult? Build a `PyMC3` model for your final project and share with your buddies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Classification\n",
    "\n",
    "So that math of classification is tough because it usually involves an **activation function** to help classify, same kind of activation function involved in neural networks, and that activation function is generally non-linear. So we'll cut straight to the chase and showcase an example using the **logistic function**.\n",
    "\n",
    "Classification is where, given an input vector $\\mathbf{x}$, you need to determine $p(C_k\\;|\\;\\mathbf{x})$ where $k\\in {1\\ldots K}$ and $C_k$ is a discrete class label, in other words the elements $y_i = p(C_i\\;|\\;\\phi(\\mathbf{x}))$ i.e. the probability that the correct class label is $C_i$ given the input vector $\\mathbf{x}$. The function $\\phi$ is known as the **activation function** and its inverse is known as the **link function**. $\\phi$ is often nonlinear.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The case of Probabilistic Discriminative Models\n",
    "\n",
    "Probablistic discriminative models assume a generalized *linear* model:\n",
    "\n",
    "$$y(\\phi(\\mathbf{x})) = f(\\mathbf{w}^T\\phi(\\mathbf{x}))$$\n",
    "\n",
    "\n",
    "#### The case of Logistic Regression\n",
    "\n",
    "We will consider Logistic Regression for the case of **two classes**, assuming a model for the class posterior probabilities, $p(C_k\\;|\\;\\phi(\\mathbf{x}))$, in the form of the logistic sigmoid:\n",
    "\n",
    "$$p(C_1\\;|\\;\\phi) = f(a) = \\sigma(\\mathbf{w}^T\\phi)$$\n",
    "\n",
    "with \n",
    "\n",
    "$$p(C_2\\;|\\;\\phi) = 1 - p(C_1\\;|\\;\\phi)$$\n",
    "\n",
    "We apply maximum likelihood to obtain the model parameters. Assume that our training set, $\\mathbf{t}$, is of the form $\\\\{\\phi_n,t_n\\\\}$ where $\\phi_n=\\phi(\\mathbf{x}_n)$, $t_n \\in \\{0,1\\}$,\n",
    "and $n=1\\ldots N$. The likelihood function of the training data is then\n",
    "\n",
    "$$p(\\mathbf{t}|\\mathbf{w}) = \\prod_{n=1}^N \\sigma(\\mathbf{w}^T \\phi(\\mathbf{x}_n))^{t_n} (1 - \\sigma(\\mathbf{w}^T \\phi(\\mathbf{x}_n)))^{1-t_n}$$\n",
    "\n",
    "Defining the error function, $E(\\mathbf{w})$, as the negative of the log-likelihood function, and taking the gradient with respect to $\\mathbf{w}$, we obtain\n",
    "\n",
    "$$\\bigtriangledown E(\\mathbf{w}) = \\sum_{n=1}^N \\left[\\sigma(\\mathbf{w}^T \\phi(\\mathbf{x}_n)) - t_n\\right] \\phi(\\mathbf{x}_n)$$\n",
    "\n",
    "Superficially, this error function looks the same as that obtained for **linear** regression under the assumption of a Gaussian noise model which had a closed form solution. However, the nonlinearity of the *sigmoid* function, $\\sigma(\\mathbf{w}^T \\phi(\\mathbf{x}_n))$ prevents a closed form solution in the **logistic** regression problem. We therefore must apply an iterative method to obtain a numerical solution for the parameters, $\\mathbf{w}$. Here we will\n",
    "consider the *Newton-Raphson* method for which minimizing the error function takes the form\n",
    "\n",
    "$$\\mathbf{w}^{\\tau+1} = \\mathbf{w}^{\\tau} - \\mathbf{H}^{-1}\\bigtriangledown E(\\mathbf{w})$$\n",
    "\n",
    "where $\\mathbf{H}$ is the *Hessian* matrix composed of the second derivatives of the error function\n",
    "\n",
    "$$\\mathbf{H} = \\bigtriangledown \\bigtriangledown E(\\mathbf{w}) = \\Phi^T \\mathbf{R} \\Phi$$\n",
    "\n",
    "where $\\Phi$ is the $N \\times M$ design matrix whos $n^{th}$ row is given by $\\phi(\\mathbf{x_n})^T$ and $\\mathbf{R}$ is an $N \\times N$ diagonal matrix with elements $R_{n,n} = \\sigma(\\mathbf{w}^T \\phi(\\mathbf{x}_n)) \\left[1-\\sigma(\\mathbf{w}^T \\phi(\\mathbf{x}_n))\\right]$. This can be reduced to a form equivalent to that of localy weighted linear *regression* as follows\n",
    "\n",
    "$$\\mathbf{w}^{\\tau+1} = \\left( \\Phi^T \\mathbf{R} \\Phi \\right)^{-1} \\Phi^T \\mathbf{R} \\mathbf{z}$$\n",
    "\n",
    "where $\\mathbf{z}$ is an $N$ dimensional vector defined by\n",
    "\n",
    "$$\\mathbf{z} = \\Phi \\mathbf{w}^{\\tau} - \\mathbf{R}^{-1}(\\mathbf{y} - \\mathbf{t})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6\n",
    "\n",
    "Consider an example with two classes and 2D input, $\\mathbf{x_n} = (x_n^{(1)},x_n^{(2)})$. As an experiment, try to increase the number of training points, $N$. Eventually, the training points will overlap so that it will not be possible to completely seperate them with the transformation provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 123456789\n",
    "np.random.seed(seed)\n",
    "\n",
    "N = 100 #number of data points, SEE WHAT HAPPENS AS YOU INCREASE THIS TO SAY 200\n",
    "D = 2   #dimension of input vector\n",
    "t = np.zeros(N) #training set classifications\n",
    "X = np.zeros((N,D)) #training data in input space\n",
    "sigma = .25\n",
    "mu0 = 0.0\n",
    "mu1 = 1.0\n",
    "\n",
    "# function for generating 2D class scatter plot\n",
    "def createScatter(X, t, ax):\n",
    "    C1x = []  \n",
    "    C1y = []\n",
    "    C2x = []\n",
    "    C2y = []\n",
    "    for i in range(len(t)):\n",
    "        if t[i] > 0: \n",
    "            C1x.append(X[i,0])\n",
    "            C1y.append(X[i,1])\n",
    "        else: \n",
    "            C2x.append(X[i,0])\n",
    "            C2y.append(X[i,1])\n",
    "    ax.scatter(C1x,C1y)\n",
    "    ax.scatter(C2x,C2y, color='r')\n",
    "    \n",
    "# Generate test data. (NOTE THIS IS NOT BASED ON A GENERATIVE APPROACH. This is likely to work. \n",
    "# IN PRACTICE THIS DATA WOULD BE OBTAINED VIA OBSERVATION)\n",
    "# Pick a value from a uniform distribution [0,1). If it is less than 0.5, assign class 1 and pick x1,x2 from a normal\n",
    "# distribution(mu0,sigma) otherwise assign class 2 and pick x1,x2 from a normal distribution(mu1,sigma)\n",
    "for i in range(N):\n",
    "    #choose class to sample for\n",
    "    fac = 1\n",
    "    if np.random.rand() <= 0.5:\n",
    "        thismu = mu0\n",
    "        t[i] = 1\n",
    "    else: \n",
    "        thismu = mu1\n",
    "        t[i] = 0\n",
    "        if np.random.rand() < 0.5: fac = -1\n",
    "        \n",
    "    X[i,0] = fac * np.random.normal(thismu, sigma)\n",
    "    X[i,1] = fac * np.random.normal(thismu, sigma)\n",
    "    \n",
    "\n",
    "f, axarr = plt.subplots(1,3)\n",
    "f.subplots_adjust(right=2.5)\n",
    "f.text(0.75,0.975,'Fixed Input Transformation Yields Linear Class Boundary',horizontalalignment='center',verticalalignment='top')\n",
    "\n",
    "ax1 = axarr[0]\n",
    "ax1.set_xlabel('x1')\n",
    "ax1.set_ylabel('x2')\n",
    "createScatter(X,t,ax1)\n",
    "\n",
    "# The training data does not have a linear boundary in the original input space. So lets apply a tranformation, phi to try \n",
    "# to make it linearly seperable\n",
    "# NOTE: This transformation is not the only one that works. For example try switching the values of MU1 and MU2. The result \n",
    "# will be a different mapping that is still linearly seperable. This technique is known as a kernel method\n",
    "def phi(x,mu,sigma):\n",
    "    detSigma = np.linalg.det(sigma)\n",
    "    fac = math.pow(2*math.pi, len(mu)/2.0) * math.sqrt(detSigma)\n",
    "    arg = -0.5 * np.dot((x-mu).T, np.dot(np.linalg.inv(sigma), x-mu) )\n",
    "    return math.exp(arg) / fac\n",
    "    \n",
    "phiX = np.zeros((N,D))\n",
    "MU1 = np.ones(D)*mu0\n",
    "MU2 = np.ones(D)*mu1\n",
    "SIGMA = np.diag(np.ones(D))*sigma\n",
    "for i in range(N):\n",
    "    phiX[i,0] = phi(x=X[i,:], mu=MU2, sigma=SIGMA)\n",
    "    phiX[i,1] = phi(x=X[i,:], mu=MU1, sigma=SIGMA)\n",
    "    \n",
    "ax2 = axarr[1]\n",
    "ax2.set_xlabel('phi1(x)')\n",
    "ax2.set_ylabel('phi2(x)')\n",
    "createScatter(phiX, t, ax2)\n",
    "\n",
    "# Now lets apply machine learning to determine the boundary. We will assume M = 3, i.e. that there are 3 free parameters \n",
    "# that is w = [w0, w1, w2]^T and phi_n = [1, phiX[0], phiX[1]]\n",
    "M = 3\n",
    "Phi = np.ones((N,M))\n",
    "Phi[:,1] = phiX[:,0]\n",
    "Phi[:,2] = phiX[:,1]\n",
    "w = np.zeros(M)\n",
    "R = np.zeros((N,N))\n",
    "y = np.zeros(N)\n",
    "\n",
    "def sigmoid(a):\n",
    "   return 1.0 / (1.0 + math.exp(-a))\n",
    "\n",
    "def totalErr(y,t):\n",
    "    e = 0.0\n",
    "    for i in range(len(y)):\n",
    "        if t[i] > 0:\n",
    "            e += math.log(y[i])\n",
    "        else:\n",
    "            e += math.log(1.0 - y[i])\n",
    "    return -e\n",
    "\n",
    "# start Newton-Raphson. As a stopping criteria we will use a tolerance on the change in the error function \n",
    "# and a max number of iterations\n",
    "max_its = 100\n",
    "tol = 1e-2\n",
    "w0 = [w[0]] \n",
    "w1 = [w[1]]\n",
    "w2 = [w[2]]\n",
    "err = []\n",
    "error_delta = 1 + tol\n",
    "current_error = 0\n",
    "idx = 0\n",
    "\n",
    "while math.fabs(error_delta)>tol and idx < max_its:\n",
    "    #update y & R\n",
    "    for i in range(N): \n",
    "        zipped = zip(w, Phi[i,:])\n",
    "        y[i] = sigmoid(reduce(lambda accum, Z: accum + Z[0]*Z[1], zipped, 0))\n",
    "        R[i,i] = y[i] - y[i]*y[i]\n",
    "        \n",
    "    #update w\n",
    "    z = np.dot(Phi,w) - np.dot(np.linalg.pinv(R),y-t)\n",
    "    temp = np.linalg.pinv(np.dot(np.dot(Phi.T,R),Phi))\n",
    "    temp2 = np.dot(np.dot(temp, Phi.T),R)\n",
    "    w = np.dot(temp2, z)\n",
    "    w0.append(w[0])\n",
    "    w1.append(w[1])\n",
    "    w2.append(w[2])\n",
    "    idx += 1\n",
    "    temp = totalErr(y,t)\n",
    "    error_delta = current_error - temp\n",
    "    current_error = temp\n",
    "    err.append(error_delta)\n",
    "    \n",
    "print ('The total number of iterations was {0}'.format(idx))\n",
    "print ('The total error was {0}'.format(current_error))\n",
    "print ('The final change in error was {0}'.format(error_delta))\n",
    "print ('The final parameters were {0}'.format(w))\n",
    "    \n",
    "# our decision boundary is now formed by the line where sigma(a) = 0.5, i.e. where a = 0, which for this example is \n",
    "# where phi2 = -(w1/w2)phi1, i.e. where w * phi = .5\n",
    "bdryx = (0,1)\n",
    "bdryy = (-(w[0]+w[1]*bdryx[0])/w[2], -(w[0]+w[1]*bdryx[1])/w[2])\n",
    "ax2.plot(bdryx, bdryy)\n",
    "\n",
    "ax3 = axarr[2]\n",
    "ax3.plot(w0, color = 'blue')\n",
    "ax3.plot(w1, color = 'red')\n",
    "ax3.plot(w2, color = 'green')\n",
    "ax3.plot(err, color = 'black')\n",
    "ax3.legend((\"w0\",\"w1\",\"w2\", \"error delta\"), loc='upper left')\n",
    "ax3.set_xlabel('Newton-Raphson Iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Artificial Neural Networks\n",
    "\n",
    "Neural networks are an approach to machine learning that attempts to deal with the problem of large data dimensionality. The neural network approach uses a fixed number of basis functions - in contrast\n",
    "to methods such as support vector machines that attempt to adapt the number of basis functions themselves parameterized by the model parameters. This is a significant departure from linear regression and logistic regression methods where the models consisted of linear combinations of fixed basis functions, $\\phi(\\mathbf{x})$, that dependend only on the input vector, $\\mathbf{x}$. \n",
    "\n",
    "Why? Because in neural networks, the basis functions can now depend on both the model parameters *and* the input vector and thus take the form $\\phi(\\mathbf{x} | \\mathbf{w})$. \n",
    "\n",
    "It is simpler to start with **feed-forward** neural networks (as opposed to **recursive neural networks**). One can envision a neural network as a series of layers where each layer has some number of nodes and each node is a function. \n",
    "\n",
    "**Each layer represents a single linear regression model**!\n",
    "\n",
    "The nodes are connected to each other through inputs accepted from, and outputs passed to other nodes. A *feed-forward* network is one in which these connections do not form any directed cycles. See [here](http://en.wikipedia.org/wiki/Feedforward_neural_network) for more detail.\n",
    "\n",
    "As a matter of convention, we always refer the model as a $N$ layer model where $N$ is the number of layers for which adaptive parameters, $\\mathbf{w}$, must be determined. Thus for a model consisting of an input layer, one hidden layer, and an output layer, we consider $N$ to be 2 since parameters are determined only for the hidden and output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward Network Functions\n",
    "\n",
    "We will consider a basic two layer neural network model, i.e a model that maps inputs to a hidden layer and then to an output layer. We make **the following assumptions**:\n",
    "\n",
    "1. The final output will be a vector $Y$ with $K$ elements, $y_k$, where $y_k(\\mathbf{x},\\mathbf{w}) = p(C_1|\\mathbf{x})$ is the probability that node $k$ is in class $C_1$ and $p(C_2|\\mathbf{x}) = 1-p(C_1|\\mathbf{x})$\n",
    "2. The activation function at a given layer is an arbitrary nonlinear function of a linear combination of the inputs and parameters for that layer\n",
    "3. The network is fully connected, i.e. every node at the input layer is connected to every node in the hidden layer and every node in the hidden layer is connected to every node in the output layer\n",
    "4. A bias parameter is included at the hidden and output layers\n",
    "\n",
    "Working from the input layer toward the output layer, we can build this model as follows:\n",
    "\n",
    "### Input Layer\n",
    "Assume we have an input vector $\\mathbf{x} \\in \\Re^D$. Then the input layer consists of $D+1$ nodes where the value of the $i^{th}$ node for $i=0\\ldots D$, is 0 if $i=0$ and $x_i$, i.e. the $i^{th}$ value of $\\mathbf{x}$, otherwise.\n",
    "\n",
    "### Hidden Layer\n",
    "At the hidden layer we construct $M$ nodes where the value of $M$ depends on the specifics of the particular modelling problem. For each node, we define a *unit activation*, $a_m$, for $m=1\\ldots M$ as\n",
    "\n",
    "$$a_m = \\sum_{i=0}^D w_{ji}^{(1)}x_i$$\n",
    "\n",
    "where the $(1)$ superscript indicates this weight is for the hidden layer. The output from each node, $z_m$, is then given by the value of a *fixed nonlinear function* $h$, known as the *activation function*, acting on the unit activation:\n",
    "\n",
    "$$z_m = h(a_m) = h \\left( \\sum_{i=0}^D w_{mi}^{(1)}x_i \\right)$$\n",
    "\n",
    "Notice that $h$ is the same function for all nodes.\n",
    "\n",
    "### Output Layer\n",
    "The process at the output layer is essentially the same as at the hidden layer. We construct $K$ nodes, where again the value of $K$ depends on the specific modeling problem. For each node, we again define a *unit activation*, $a_k$, for $k=1 \\ldots K$ by\n",
    "\n",
    "$$a_k = \\sum_{m=0}^M w_{km}^{(2)} z_m$$\n",
    "\n",
    "We again apply a nonlinear activation function, say $y$, to produce the output\n",
    "\n",
    "$$y_k = y(a_k)$$\n",
    "\n",
    "Thus, the entire model can be summarized as a $K$ dimensional output vector $Y \\in \\Re^K$ where each element $y_k$ by\n",
    "\n",
    "$$y_k(\\mathbf{x},\\mathbf{w}) = y \\left( \\sum_{m=0}^M w_{km}^{(2)} h \\left( \\sum_{i=0}^D w_{mi}^{(1)}x_i \\right) \\right)$$\n",
    "\n",
    "### Generalizations\n",
    "There are a wide variety of generalizations possible for this model. Some of the more important ones for practical applications include\n",
    "\n",
    "* Addition of **hidden layers**\n",
    "* Inclusion of **skip-layer** connections, e.g. a connection from an input node directly to an output node\n",
    "* **Sparse network**, i.e. not a fully connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training\n",
    "\n",
    "Here we will consider how to determine the network model parameters. We will use a *maximum likelihood estimation* approach. \n",
    "\n",
    "The likelihood function for the network is dependent on the type of problem the network models. Of particular importance is the number and type of values generated at the output layer. We consider two cases below.\n",
    "\n",
    "### Regression Single Gaussian Target\n",
    "\n",
    "*Assume* that our target variable *t* is Gaussian distributed with an $\\mathbf{x}$ dependent mean, $\\mu(\\mathbf{x})$ and constant variance $\\sigma^2 = 1/\\beta$. Our network model is designed to estimate the unknown function for the mean, i.e. $y(\\mathbf{x},\\mathbf{w}) \\approx \\mu(\\mathbf{x})$ so that the distribution for the target value, *t*, is modeled by\n",
    "\n",
    "$$p(t\\;|\\;\\mathbf{x},\\mathbf{w}) = \\mathcal{N}(t\\;|\\;y(\\mathbf{x},\\mathbf{w}), \\;\\beta^{-1}) = = \\mathcal{N}(t@y(\\mathbf{x},\\mathbf{w}), \\;\\beta^{-1})$$\n",
    "\n",
    "where $\\mathcal{N}(\\mu,\\sigma^2)$ is the normal distribution with mean $\\mu$ and variance $\\sigma^2$. *Assume* the output layer activation function is the identity, i.e. the output is simply the the unit activations, and that we have $N$ independent observations $\\mathbf{X}={\\mathbf{x}_1, \\ldots, \\mathbf{x}_2 }$ and target values $\\mathbf{t} = {t_1, \\ldots, t_2}$, the likelihood function is\n",
    "\n",
    "$$p(\\mathbf{t}\\;|\\;\\mathbf{X},\\mathbf{w}, \\beta) = \\prod_{n=1}^N p(t_n\\;|\\;\\mathbf{x}_n, \\mathbf{w}, \\beta)$$\n",
    "\n",
    "The **total error function** is defined as the negative logarithm of the likelihood function given by\n",
    "\n",
    "$$\\frac {\\beta}{2} \\sum_{n=1}^N \\{y(\\mathbf{x}_n,\\mathbf{w}) - t_n \\}^2 - \\frac{N}{2} \\ln(\\beta) + \\frac{N}{2} \\ln (2\\pi)$$\n",
    "\n",
    "The parameter estimates, $\\mathbf{w}^{(ML)}$ (ML indicates maximum likelihood) are found by maximizing the equivalent sum-of-squares error function\n",
    "\n",
    "$$E(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\{y(\\mathbf{x}_n,\\mathbf{w}) - t_n \\}^2$$\n",
    "\n",
    "Note, due to the nonlinearity of the network model, $E(\\mathbf{w})$ is not necessisarily convex, and thus may have *local minima* and hence it is not possible to know for sure that the global minimum has been obtained.  The model parameter, $\\beta$ is found by first finding $\\mathbf{w}_{ML}$ and then solving\n",
    "\n",
    "$$\\frac{1}{\\beta_{ML}} = \\frac{1}{N}\\sum_{n=1}^N \\{ y(\\mathbf{x}_n,\\mathbf{w}^{(ML)}) - t_n \\}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Multiple Gaussian Targets\n",
    "\n",
    "Now *assume* that we have $K$ Gaussian distributed target variables, $t_1, \\ldots, t_K$, each with a mean that is independently conditional on $\\mathbf{x}$, i.e. the mean of $t_k$ is defined by some function $\\mu_k(\\mathbf{x})$. \n",
    "\n",
    "Also assume that all $K$ variables share the same variance, $\\sigma^2=1/\\beta$. Assuming the network output layer has $K$ nodes where $y_k(\\mathbf{x},\\mathbf{w}) \\approx \\mu_k(\\mathbf{x})$ and letting $\\mathbf{y}(\\mathbf{x},\\mathbf{w}) = y_1(\\mathbf{x},\\mathbf{w}), \\ldots, y_K(\\mathbf{x},\\mathbf{w})$, and that we again have $N$ training target values $\\mathbf{t}$ ($\\mathbf{t}$ is a $K \\times N$ matrix of the training values), the conditional distribution of the target training values is given by\n",
    "\n",
    "$$p(\\mathbf{t}|\\mathbf{x},\\mathbf{w}) = \\mathcal{N}(\\mathbf{t} | \\mathbf{y}(\\mathbf{x},\\mathbf{w}), \\beta^{-1} \\mathbf{I})$$\n",
    "\n",
    "The parameter estimates, $\\mathbf{w}^{(ML)}$ are again found by minimizing the sum-of-squares error function, $E(\\mathbf{w})$, and the estimate for $\\beta$ is found from\n",
    "\n",
    "$$\\frac{1}{\\beta_{ML}} = \\frac{1}{NK}\\sum_{n=1}^N ||y(\\mathbf{x}_n,\\mathbf{w}^{(ML)}) - t_n ||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Backpropagation\n",
    "\n",
    "Most training algorithms involve a two step procedure for minimizing the model parameter dependent error function\n",
    "\n",
    "1. Evaluate the derivatives of the error function with respect to the parameters\n",
    "2. Use the derivatives to iterate the values of the parameters\n",
    "\n",
    "**Error backpropagation** provides an efficient way to evaluate a feed-forward neural network's error function. *Note* that this only satisfies step 1 above. It is still necessary to choose an optimization technique in order to use the derivative information provided by error backpropagation to update the parameter estimates as indicated by step 2. \n",
    "\n",
    "The results presented here are applicable to \n",
    "\n",
    "1. An **arbitary** feed-forward network toplogy\n",
    "2. **Arbitrary** differentiable nonlinear activation functions\n",
    "\n",
    "The one *assumption* that is made is that the error function can be expressed as a *sum of terms*, one for each training data point, $t_n$ for $n \\in \\\\{1\\ldots N\\\\}$, so that\n",
    "\n",
    "$E(\\mathbf{w}) = \\sum_{n=1}^N E_n(\\mathbf{w},t_n)$\n",
    "\n",
    "For such error functions, the derivative of the error function with respect to $\\mathbf{w}) takes the form\n",
    "\n",
    "$\\bigtriangledown E(\\mathbf{w}) = \\sum_{n=1}^N \\bigtriangledown E_n(\\mathbf{w},t_n)$\n",
    "\n",
    "Thus we need only consider the evaluation of $\\bigtriangledown E_n(\\mathbf{w},t_n)$ which may be used directly for sequential optimization techniques or summed over the training set for batch techniques (see next section). The error backpropagation method can be summarized as follows\n",
    "\n",
    "1. Given an input vector, $\\mathbf{x}_n$, forward propagate through the network using \n",
    "    1. $a_j = \\sum_i w_{ji}z_i$\n",
    "    2. $z_j = h(a_j)$\n",
    "    \n",
    "2. Evaluate $\\delta_k \\equiv \\frac{\\partial E_n} {\\partial a_k}$ for the ouptput layer as\n",
    "    1. $\\delta_k = y_k - t_k$\n",
    "    \n",
    "3. Backpropagate the $\\delta$'s to obtain $\\delta_j$ for each hidden unit in the network using\n",
    "    1. $\\delta_j = h'(a_j) \\sum_k w_{kj}\\delta_k$\n",
    "    \n",
    "4. Evaluate the derivatives using \n",
    "    1. $\\frac{\\partial E_n} {\\partial w_{ji}} = \\delta_j z_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Optimization\n",
    "\n",
    "Given the neural network error function derivative estimate provided by *error backpropagation*, one can use a variety of numerical optimization techniques to find appropriate parameter estimates. The simplest technique is the method of *steepest descent* where the new parameter estimate $\\mathbf{w}^{(\\tau+1)}$ is given by\n",
    "\n",
    "$$\\mathbf{w}^{(\\tau+1)} = \\mathbf{w}^{(\\tau)} - \\eta \\bigtriangledown E(\\mathbf{w}^{(\\tau)})$$\n",
    "\n",
    "where $\\eta>0$ is the *learning rate*. Other **more advanced** optimization algorithms inlcude *conjugate gradients* and *quasi-Newton* methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy with real neural networks\n",
    "\n",
    "Ok, well that was an awful amount of math. How do biologoical neurons do that math? We will use python list comprehensions instead of math formulas. For some of us, it is easier that way!\n",
    "\n",
    "The human nervous system is composed of more than 100 billion cells known as neurons, maybe closer to 200 billion as we are born (and then neurons die when we are babies). Do you know how many stars are in a galaxy? How many galaxies in the universe? A neuron is a cell in the nervous system whose function it is to receive and transmit information. Neurons are made up of three major parts:\n",
    "\n",
    "* the cell body, or **soma**, which contains the nucleus of the cell and keeps the cell alive\n",
    "* a branching treelike fiber known as the **dendrite**, which collects information from other cells and sends the information to the soma\n",
    "* a long, segmented fiber known as the **axon**, which transmits information away from the cell body toward other neurons or to the muscles and glands\n",
    "\n",
    "<img src=\"https://c4.staticflickr.com/3/2656/4253587827_9723c3ffd3_z.jpg\" />\n",
    "\n",
    "*Photo courtesy of GE Healthcare, http://www.flickr.com/photos/gehealthcare/4253587827/ *\n",
    "\n",
    "<img src=\"https://askabiologist.asu.edu/sites/default/files/resources/articles/neuron_anatomy.jpg\"/>\n",
    "\n",
    "Some neurons have thousands of connections (dendrites), and these dendrites may themselves be branched to allow the cell to receive information from thousands of other cells. So a human brain has an order of a hundred trillion ($10^{14}$) connections.\n",
    "\n",
    "The axons are also specialized; some, such as those that send messages from the spinal cord to the muscles in the hands or feet, may be very long---even up to several feet in length. To improve the speed of their communication, and to keep their electrical charges from shorting out with other neurons, axons are often surrounded by a **myelin sheath**. The myelin sheath is a layer of fatty tissue surrounding the axon of a neuron that both acts as an insulator and allows faster transmission of the electrical signal. Axons branch out toward their ends, and at the tip of each branch is a *terminal button*.\n",
    "\n",
    "The actual working of neurons involves many aspects (including chemical, electrical, physical, timings). We abstract all of this away into three numbers:\n",
    "\n",
    "* **activation** - a value representing the excitement of a neuron\n",
    "* **default bias** - a value representing a default or bias (sometimes called a threshold)\n",
    "* **weight** - a value representing a connection to another neuron\n",
    "\n",
    "In addition, there is a **transfer function** that takes all of the incoming activations times their associated weights plus the bias, and squashes the resulting sum. This limits the activations from growing too big or too small.\n",
    "\n",
    "Time is handled in discrete steps. Real cells, of course, fire in non-discrete intervals. Consider the trigeminal ganglion cell: this is about 2 seconds of activity that was recorded from a rat ganglion cell after a single whisker (vibrissa) was moved and held in position. Listen for the rapid steady burst of action potentials. This neuron was firing about 100 action potentials every second. The picture below is the actual recording of a portion of what you are hearing...each action potential in this record is separated by about 10 milliseconds. There are 21 action potentials displayed in this picture of the recording.\n",
    "\n",
    "<img src=\"https://faculty.washington.edu/chudler/gif/spikes2.gif\" />\n",
    "\n",
    "This is what your brain sounds like on the inside. Imagine 100 billion cells doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(\"data/sndhair.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Networks of Neurons\n",
    "\n",
    "To build a network of neurons, we first start by grouping neurons together in **layers**.\n",
    "\n",
    "A typical Artificial Neural Network (ANN) is composed of three layers: **input**, **hidden**, and **output**. Each layer contains a collection of neurons, or simply **nodes** for short. Typically, the nodes in a layer are **fully connected** to the nodes in the next layer. For instance, every input node will have a weighted connection to every hidden node. Similarly, every hidden node will have a *weighted connection* to every output node.\n",
    "\n",
    "Processing in a network works as follows. Input is propagated forward from the input layer through the hidden layer and finally through the output layer to produce a response. Each node, regardless of the layer it is in, uses the same transfer function in order to propagate its information forward to the next layer. This is described next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Transfer Function\n",
    "\n",
    "Each node maintains an activation value that depends on the activation values of its incoming neighbors, the weights from its incoming neighbors, and an additional value, called the **default bias value**. To compute this activation value, we first calculate the node's net input.\n",
    "\n",
    "The net input is a weighted sum of all the incoming activations plus the node's bias value:\n",
    "\n",
    "$$ net_i = \\sum\\limits_{j=1}^n w_{ij} x_j + b_i $$\n",
    "\n",
    "where $w_{ij}$ is the weight, or connection strength, from the $j^{th}$ node to the $i^{th}$ node, $x_j$ is the activation signal of the $j^{th}$ input node, and $b_i$ is the bias value of the $i^{th}$ node. \n",
    "\n",
    "Here is some corresponding Python code to compute this function for each node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a neuron that takes in signals from 3 neurons upstream and outputs a signal to 3 neurons downstream.\n",
    "\n",
    "First, we define the indexes for the incoming nodes (`fromNodes`), and the result nodes (`toNodes`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "toNodes = range(3, 5)\n",
    "fromNodes = range(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That allows us to store the weights between nodes in a matrix, and other related values in lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias       = [0.2, -0.1, 0.5, 0.1, 0.4, 0.9]\n",
    "activation = [0.8, -0.3, -0.8, 0.1, 0.5]\n",
    "netInput   = [0, 0, 0, 0, 0]\n",
    "weights = [[ 0.1, -0.8], \n",
    "          [-0.3,  0.1], \n",
    "          [ 0.2, -0.1], \n",
    "          [ 0.0,  0.1], \n",
    "          [ 0.8, -0.8], \n",
    "          [ 0.4, 0.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute the `netInput[i]` as per the above equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in toNodes:\n",
    "    netInput[i] = bias[i]\n",
    "    for j in fromNodes:\n",
    "        netInput[i] += (weights[i][j] * activation[j]) \n",
    "netInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the equivalent list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netinput_nonzero = [bias[i] + sum([weights[i][j] * activation[j] for j in fromNodes]) for i in toNodes]\n",
    "netinput_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `weights[i][j]` is the weight $w_{ij}$, or connection strength, from the $j^{th}$ node to the $i^{th}$ node, `activation[j]` is the activation signal $x_j$ of the $j^{th}$ input node, and `bias[i]` is the bias value $b_i$ of the $i^{th}$ node. \n",
    "\n",
    "After computing the net input, each node has to compute its output activation. The value that results from applying the activation function to the net input is the signal that will be sent as output to all the nodes in the next layer. The **activation function** used in backprop networks is generally:\n",
    "\n",
    "$$ a_i = \\sigma(net_i) $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ \\sigma(x) = \\dfrac{1}{1 + e^{-x}} $$\n",
    "\n",
    "The method math.exp() returns returns exponential of x: $e^{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activationFunction(netInput):\n",
    "    return 1.0 / (1.0 + math.exp(-netInput))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the complete activation of a unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in toNodes:\n",
    "    activation[i] = activationFunction(netInput[i])\n",
    "activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\sigma$ is the activation function, as shown in the plot below. Notice that the function is monotonically increasing and bounded by 0.0 and 1.0 as the net input approaches negative infinity and positive infinity, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "xs = range(-10, 10)\n",
    "pts = [activationFunction(x) for x in xs]\n",
    "\n",
    "plt.plot(xs, pts)\n",
    "plt.xlabel(\"input\")\n",
    "plt.ylabel(\"output\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to set the weights?\n",
    "\n",
    "For many years, it was unknown how to learn the weights in a multi-layered neural network. In addition, Marvin Minsky and Seymour Papert from MIT proved in their 1969 book [Perceptrons](https://en.wikipedia.org/wiki/Perceptrons_(book)) that you could not do simple functions without having multi-layers (actually, the idea of using simulated evolution to search for the weights could have been used, but no one thought to do that). \n",
    "\n",
    "Specifically, they looked at the function XOR:\n",
    "\n",
    "**Input 1** | **Input 2** | **Target**\n",
    "------------|-------------|-------\n",
    " 0 | 0 | 0\n",
    " 0 | 1 | 1 \n",
    " 1 | 0 | 1 \n",
    " 1 | 1 | 0 \n",
    "\n",
    "This killed research into neural networks for more than a decade. So, the idea of neural networks generally was ignored until the mid 1980s when the **Back-Propagation of Error** (backprop) was created. That was when [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton), [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) (and [here](https://yann.lecun.com)), and [Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio) created the first successful artificial neural networks that *worked*, from universities in Canada, because the US National Science Foundation (NSF) did not fund artificial neural network research anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rule\n",
    "\n",
    "Backprop networks fall under the category of *supervised learning* schemes. That is, during training, the network is presented a training input, the inputs are propagated using the transfer function, until output appears in the output layer. The output is then compared with the expected or target output and an error is computed. The error is then backpropagated by applying the learning rule. \n",
    "\n",
    "A learning rule modifies the weights between nodes. The backpropagation algorithm, also called the *generalized delta rule*, systematically changes the weights by using a weight change equation. We use an optional momentum term in the weight change rule to help speed up convergence. The weight change rule is different for weights between the hidden-output layer nodes and the input-hidden layer nodes. For the hidden-output layer nodes it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "desiredOutput = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "actualOutput = [0.8, 0.6, 0.5, 0.8, 0.3]\n",
    "\n",
    "error = [0.0 for i in desiredOutput]\n",
    "delta = [0.0 for i in desiredOutput]\n",
    "\n",
    "EPSILON = 0.1   # learning rate\n",
    "MOMENTUM = 0.01 # a smoothing term\n",
    "\n",
    "weightUpdate = [[ 0.0, 0.0], \n",
    "                [ 0.0, 0.0], \n",
    "                [ 0.0, 0.0], \n",
    "                [ 0.0, 0.0], \n",
    "                [ 0.0, 0.0], \n",
    "                [ 0.0, 0.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the learning rule applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in toNodes:\n",
    "    error[i] = (desiredOutput[i] - actualOutput[i])\n",
    "    delta[i] = error[i] * actualOutput[i] * (1 - actualOutput[i])\n",
    "    for j in fromNodes:\n",
    "        weightUpdate[i][j] = (EPSILON * delta[i] * actualOutput[j]) + (MOMENTUM * weightUpdate[i][j])\n",
    "        \n",
    "weightUpdate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, at the $i^{th}$ output node, the error is the difference between desired and actual outputs. The weight change between a hidden layer node $j$ and output node $i$ --- `weightUpdate[i][j]` --- is a fraction of the computed delta value and additionally a fraction of the weight change from the previous training step. MOMENTUM is a constant that ranges between 0.0 and 1.0 and **EPSILON** is called the **learning rate** and is also a constant that varies between 0.0 and 1.0.\n",
    "\n",
    "In the above code `delta[i] * actualOutput[j]` is the partial derivative of the overall error with respect to each weight. This is the slope of error. Thus, backprop changes the weight a tiny portion of the slope of the error. We only know the slope of this curve, not the shape, and thus have to take very small steps.\n",
    "\n",
    "And that is all of the math, and Python, necessary to train a back-propagation of error neural network. Even though this is a very simple formulation, it has been proved that such three-layer network (input, hidden, output) is capable of computing any function that can be computed (Franklin and Garzon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a Neural Network\n",
    "\n",
    "Given a task, how does one train a neural network to do/solve the task? This involves the following steps:\n",
    "\n",
    " 1. Determine an appropriate network architecture.\n",
    " 1. Define a data set that will be used for training.\n",
    " 1. Define the neural network parameters to be used for training.\n",
    " 1. Train the network.\n",
    " 1. Test the trained network.\n",
    " 1. Do post-training analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Determining an appropriate architecture\n",
    "\n",
    "Recall that a neural network consists of an input layer, an output layer, and zero or more hidden layers. Once a network has been trained, when you present an input to the network, the network will propagate the inputs through its layers to produce an output (using the transfer function described above). If the input represents an instance of the task, the output should be the solution to that instance after the network has been trained. Thus, one can view a neural network as a general pattern associator. Thus, given a task, the first step is to identify the nature of inputs to the pattern associator. This is normally in the form of number of nodes required to represent the input. Similarly, you will need to determine how many output nodes will be required. For example, consider a simple logical connective, AND whose input-output characteristics are summarized in the table below:\n",
    "\n",
    "**Input A** | **Input B** | **Target**\n",
    "------------|-------------|-------\n",
    " 0 | 0 | 0\n",
    " 0 | 1 | 0 \n",
    " 1 | 0 | 0 \n",
    " 1 | 1 | 1 \n",
    "\n",
    "This is a very simple example, but it will help us illustrate all of the important concepts in defining and training neural networks.\n",
    "\n",
    "In this example, it is clear that we will need two nodes in the input layer, and one in the output layer. We can start by assuming that we will not need a hidden layer. In general, as far as the design of a neural network is concerned, you always begin by identifying the size of the input and output layers. Then, you decide how many hidden layers you would use. In most situations you will need one hidden layer, though there are no hard and fast rules about its size. Through much empirical practice, you will develop your own heuristics about this. We will return to this issue later. In the case of the AND network, it is simple enough that we have decided not to use any hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Define a dataset that will be used for training\n",
    "\n",
    "Once you have decided on the network architecture, you have to prepare the data set that will be used for training. Each item in the data set represents an input pattern and the correct output pattern that should be produced by the network (since this is supervised training). In most tasks, there can be an infinite number of such input-output associations. Obviously it would be impossible to enumerate all associations for all tasks (and it would make little sense to even try to do this!). You have to then decide what comprises a good representative data set that, when used in training a network, would generalize to all situations.\n",
    "\n",
    "In the case of the AND network, the data set is very small, finite (only 4 cases!), and exhaustive.\n",
    "\n",
    "The other issue you have to take into consideration here is that of the range of each input and output value. Remember the transfer function of a node is a sigmoid-function that serves to squash all input values between 0.0 and 1.0. Thus, regardless of the size of each input value into a node, the output produced by each node is between 0.0 and 1.0. This means that all output nodes have values in that range. If the task you are dealing with expects outputs between 0.0 and 1.0, then there is nothing to worry about. However, in most situations, you will need to *scale* the output values back to the values in the task domain. \n",
    "\n",
    "In reality, it is also a good idea to scale the input values from the domain into the 0.0 to 1.0 range (especially if most input values are outside the -5.0 and 5.0 range). Thus, defining a data set for training almost always requires a collection of input-output pairs, as well as scaling and unscaling operations. Luckily, for the AND task, we do not need to do any scaling, but we will see several examples of this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Define the neural network parameters\n",
    "\n",
    "The next step is to define the parameters required to train the neural network. These include the following:\n",
    "\n",
    " 1. The learning constant\n",
    " 1. The momentum constant\n",
    " 1. The tolerance\n",
    " 1. Other training-related parameters\n",
    "\n",
    "The learning rate, EPSILON, and the momentum constant, MOMENTUM, have to be between 0.0 and 1.0 and are critical to the overall training algorithm. The appropriate values of these constants are best determined by experimentation. Tolerance (which is also between 0.0 and 1.0) refers to the level of tolerance that is acceptable for determining correctness of the output. For example, if tolerance is set to 0.1, then an output value within 10% of the desired output is considered correct. Other training parameters generally exist to specify the reporting rate of the progress of the training, where to log such progress, etc. We will see specific examples of these as we start working with actual networks.\n",
    "\n",
    "For the AND network, we will set EPSILON to 0.5, MOMENTUM to 0.0, report the progress every 5 epochs (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Train the network\n",
    "\n",
    "Once all the parameters are specified, you start the training process. This involves presenting each input pattern to the network, propagating it all the way until an output is produced, comparing the output with the desired target, computing the error, backpropagating the error, and applying the learning rule.  This process is repeated  until all inputs are exhausted. A single pass through an entire data set is called an *epoch*. In general, you always train the network for several epochs (can be anywhere from a few hundred to millions!) until the network begins to show more improved and stable performance. Performance of the network is generally measured in terms of the *total sum-squared error* or *TSS* for short. This is the error in each pattern squared and summed over all the patterns. Initially, you will notice that the TSS is quite high, but it will slowly decrease as the number of epochs increase. \n",
    "\n",
    "You can either stop the training process after a certain number of epochs have elapsed, or after the TSS has decreased to a specific amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Test the trained network\n",
    "\n",
    "Once the network has been trained, it is time to test it. There are several ways of doing this. Perhaps the easiest is to turn learning off (another training parameter) and then see the outputs produced by the network for each input in the data set. When a trained network is going to be used in a *deployed* application, all you have to do is save the weights of all interconnections in the network into a file. The trained network can then be recreated at anytime by reloading the weights.\n",
    "\n",
    "Note: Instead of training-then-testing, there is another methodology: you can test-while-training. Conx's neural network system supports the idea of **cross validation**. With cross validation one defines a training corpus and testing corpus at the beginning. Occasionally, as training proceeds on the training corpus, the system will stop training momentarily (by turning learning off) and test its current weights on the test corpus. This methodology has the advantage of being able to stop when performance on the test corpus begins to drop, thereby preventing over-training. See [Conx Implementation Details]() for more details on cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Do post-training analysis\n",
    "\n",
    "Perhaps the most important step in using neural networks is the analysis one performs once a network has been trained. There are a whole host of analysis techniques, we will present some of them as we go along.\n",
    "\n",
    "Next, we will use Python package `calysto` to create and experiment with neural networks. We will use the AND network example to train the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: The AND function\n",
    "\n",
    "Consider the AND function:\n",
    "\n",
    "**Input A** | **Input B** | **Target**\n",
    "------------|-------------|-------\n",
    " 0 | 0 | 0\n",
    " 0 | 1 | 0 \n",
    " 1 | 0 | 0 \n",
    " 1 | 1 | 1\n",
    " \n",
    " First, install the `calysto` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install calysto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the AND experiment, we will create a 2-3-1 network:\n",
    "\n",
    "* input layer of two units\n",
    "* hidden layer of three (arbitrary, but not too big)\n",
    "* output layer of one unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calysto.ai.conx import *\n",
    "\n",
    "net = Network()\n",
    "net.addLayers(2, 3, 1) #input -2, hidden-3, output-1\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "net.addLayers(2, 3, 1)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can propagate activation through the network (from input layer, through the hidden layer, to output layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.propagate(input=[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how an untrained network works on the AND problem, we can just propagate the input activations for each input pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pattern in [[0, 0], [0, 1], [1, 0], [1, 1]]:\n",
    "    print(pattern, net.propagate(input=pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a network to determine the weights to compute the AND function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide training patterns (inputs and outputs)\n",
    "net.setInputs([[0.0, 0.0],[0.0, 1.0],[1.0, 0.0],[1.0, 1.0]])\n",
    "net.setOutputs([[0.0],[0.0],[0.0],[1.0]])\n",
    "\n",
    "# set learning parameters\n",
    "net.setEpsilon(0.5)\n",
    "net.setTolerance(0.2)\n",
    "net.setReportRate(1)\n",
    "\n",
    "# learn\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be pretty easy to learn this function, taking around 10 to 20 cycles through the 4 training patterns (called an **epoch**). We can test each input pattern to see if it really works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pattern in [[0, 0], [0, 1], [1, 0], [1, 1]]:\n",
    "    print(pattern, net.propagate(input=pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! It really works. But wait, it gets better.\n",
    "\n",
    "#### Generalization\n",
    "\n",
    "One of the great side benefits of using a neural network to solve a problem is that it can also do slightly different problems that it was not trained on.\n",
    "\n",
    "For example, we didn't explicitly train the network on [0.8, 0.8] for the AND problem, but we can see what the network thinks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.propagate(input=[0.8, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a reasonable answer. To see all of the reasonable answers, we can sample the entire input space for the two input units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros((100, 100))\n",
    "\n",
    "for x in range(100):\n",
    "    for y in range(100):\n",
    "        z[x][y] = net.propagate(input=[x/100, y/100])[0]\n",
    "\n",
    "plt.imshow(z, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.xlabel(\"input 1\")\n",
    "plt.ylabel(\"input 2\")\n",
    "plt.title(\"Output Activation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the bottom, right-hand corner is white, meaning close to 1.0. However, the network has learned the surrounding area is a gradation between black (0.0) and white. We can describe this as **generalization**.\n",
    "\n",
    "And that's it, that's how you learn, using just a few use cases about girlfriends, how to land the perfect Northeastern girl of your dreams.\n",
    "\n",
    "So *do* it :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusion\n",
    "\n",
    "The goal of this notebook was to proove to you that your **brain is just a predictive machine** that does math, and that we don't need to program machines to be human to get AI. We just need to program them just like any other program, using a bit of python, with the help of libraries like [Tensorflow](https://www.tensorflow.org/), [Torch](https://research.fb.com/downloads/torch/), etc. And we don't need to be afraid of Machine Learning. It's just statistics and linear algebra.\n",
    "\n",
    "Next lecture, we will look at packages a bit bigger than `calysto` and how to use them, and we will look at another Machine Learning algorithm (trees and forests) that works *great* for simpler problems and for which you do not need the power of big libraries like the ones mentionned above, and which should *always* be your first attempt at machine learning a dataset. It is often *sufficient* for the task at hand.\n",
    "\n",
    "## References\n",
    "Hinton, G. (1992) [How Neural Networks Learn From Experience](https://www.academia.edu/631731/How_neural_networks_learn_from_experience). Scientific American. September, 1992."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Homework: Exclusive OR (XOR), and more¶\n",
    "\n",
    "Consider the classical Exlusive Or (XOR) [problem](https://en.wikipedia.org/wiki/Exclusive_or) involving two inputs  **x1**  and  **x2**  where  $x_i ∈ (0,1)$.\n",
    "\n",
    "This problem states that the output should be 1 if exactly one of the inputs is 1 and 0 otherwise. Thus this problem has a very simple known input-output relationship\n",
    "\n",
    "**x1** | **x2** | **Output**\n",
    "------------|-------------|-------\n",
    " 0 | 0 | 0\n",
    " 1 | 0 | 1 \n",
    " 0 | 1 | 1 \n",
    " 1 | 1 | 0\n",
    "\n",
    "The XOR network is typically presented as having 2 input nodes, 2 hidden layer nodes, and one output nodes.\n",
    "\n",
    "Your output layer will have a single node. You will interpret the out values as being 0 (or false) for output values less than 0 and 1 (or true) for output values greater than 0.\n",
    "\n",
    "Use `calysto` as we did for the AND to solve the Exclusive OR problem. We say: program an **auto-encoder** for XOR.\n",
    "\n",
    "*Then*, download a dataset of your choice from the Web, and *learn it* with `calysto` (auto-encode it). Use that model to predict something useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
