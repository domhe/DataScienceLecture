{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 5 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 11 February 2019, with material from Peter Norvig</div>\n",
    "\n",
    "# Bayesian Lab: Modeling the effect of Drugs to improve scholarly performance\n",
    "\n",
    "Let's practice what we learned last lecture. Let's build a Bayesian model for a dataset, so we can extract precise statistics from the model itself, rather than the data.  But let's also learn about classical regresion models.\n",
    "\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/caryatids-at-erechtheumjpg.jpg\" width=400 />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression modeling\n",
    "A general goal of many statistical data analysis tasks is to relate the influence of one variable on another.\n",
    "\n",
    "For example:\n",
    "\n",
    "How test scores are correlated with tissue LSD concentration. See [here](https://ascpt.onlinelibrary.wiley.com/doi/abs/10.1002/cpt196895635).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "data_string = \"\"\"\n",
    "Drugs\tScore\n",
    "0\t1.17\t78.93\n",
    "1\t2.97\t58.20\n",
    "2\t3.26\t67.47\n",
    "3\t4.69\t37.47\n",
    "4\t5.83\t45.65\n",
    "5\t6.00\t32.92\n",
    "6\t6.41\t29.97\n",
    "\"\"\"\n",
    "\n",
    "lsd_and_math = pd.read...\n",
    "lsd_and_math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsd_and_math.plot(x='Drugs', y='Score', style='ro', legend=False, xlim=(0,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a model to characterize the relationship between $X$ and $Y$, recognizing that additional factors other than $X$ (the ones we have measured or are interested in) may influence the response variable $Y$.\n",
    "\n",
    "In general,\n",
    "\n",
    "$$M(Y|X) = f(X)$$\n",
    "\n",
    "for a paramteric regression model,\n",
    "\n",
    "$$M(Y|X) = f(X, \\beta, \\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $f$ is some function, for example a linear function of the parameters. And if you have more than one dimension of independent variables, you can associate a paramter to each:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = \\beta_0 + \\beta_1 x_{1i} + \\ldots + \\beta_k x_{ki} + \\epsilon_i$\n",
    "</div>\n",
    "\n",
    "Regression is a **weighted sum** of independent predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon_i$ accounts for the difference between the observed response $y_i$ and its prediction from the model $\\hat{y_i} = \\beta_0 + \\beta_1 x_i + \\ldots$. \n",
    "\n",
    "This is sometimes referred to as **process uncertainty**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear sum of squares\n",
    "\n",
    "We would like to select $\\beta_0, \\beta_1$ so that the difference between the predictions and the observations is zero, but this is not usually possible. Instead, we choose a reasonable criterion: ***the smallest sum of the squared differences between $\\hat{y}$ and $y$***.\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$R^2 = \\sum_i (y_i - [\\beta_0 + \\beta_1 x_i])^2 = \\sum_i \\epsilon_i^2 $$  \n",
    "</div>\n",
    "\n",
    "Squaring serves two purposes: \n",
    "\n",
    "1. To prevent positive and negative values from cancelling each other out\n",
    "2. To strongly penalize large deviations. \n",
    "\n",
    "Whether or not the latter is a desired depends on the goals of the analysis.\n",
    "\n",
    "In other words, we will select the parameters that minimize the squared error of the model. Let's assume a single indepndent variable $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_squares = lambda θ, x, y: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample calculation, using aribitrary parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_squares([0,0.7], ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have the stated objective of minimizing the sum of squares, so we can pass this function to one of several optimizers in SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin\n",
    "\n",
    "x, y = lsd_and_math.T.values\n",
    "b0, b1 = fmin(sum_of_squares, [0,1], args=(x,y))\n",
    "b0, b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lsd_and_math.plot(x='Drugs', y='Score', style='ro', legend=False, xlim=(0,8), ylim=(20, 90))\n",
    "ax.plot([0,10], [b0, b0+b1*10])\n",
    "for xi, yi in zip(x,y):\n",
    "    ax.plot([xi]*2, [yi, b0+b1*xi], 'k:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Alternative loss functions\n",
    "\n",
    "Minimizing the sum of squares is not the only criterion we can use; it is just a very popular, classic one. For example, we can try to minimize the sum of absolute differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_absval = lambda θ, x, y: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0, b1 = fmin(sum_of_absval, [0,0], args=(x,y))\n",
    "print('\\nintercept: {0:.2}, slope: {1:.2}'.format(b0,b1))\n",
    "ax = lsd_and_math.plot(x='Drugs', y='Score', style='ro', legend=False, xlim=(0,8))\n",
    "ax.plot([0,10], [b0, b0+b1*10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial regession\n",
    "\n",
    "We are not restricted to a straight-line regression model either. We can represent a curved relationship between our variables by introducing **polynomial** terms. For example, a **cubic** model:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_squares_quad = lambda θ, x, y: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0,b1,b2 = fmin(sum_squares_quad, [1,1,-1], args=(x,y))\n",
    "print('\\nintercept: {0:.2}, x: {1:.2}, x2: {2:.2}'.format(b0,b1,b2))\n",
    "ax = lsd_and_math.plot(x='Drugs', y='Score', style='ro', legend=False, xlim=(0,8))\n",
    "xvals = np.linspace(0, 8, 100)\n",
    "ax.plot(xvals, b0 + b1*xvals + b2*(xvals**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. High-order-Polynomial regession\n",
    "\n",
    "Are more parameters always better? Not necessarily, from the standpoint of model fit. For example, fitting a **6th order polynomial** to the LSD example certainly results in an **overfit**.\n",
    "\n",
    "Note `np.c_` translates slice objects to concatenation along the second axis. It's another way of doing array concatenate. See [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_poly(params, data):\n",
    "        x = np.c_[[data**i for i in range(len(params))]]\n",
    "        return np.dot(params, x)\n",
    "\n",
    "x, y = lsd_and_math.T.values\n",
    "    \n",
    "sum_squares_poly = lambda θ, x, y: np.sum((y - calc_poly(θ, x)) ** 2)\n",
    "betas = fmin(sum_squares_poly, np.zeros(7), args=(x,y), maxiter=1e6)\n",
    "plt.plot(x, y, 'ro')\n",
    "xvals = np.linspace(0, max(x), 100)\n",
    "plt.plot(xvals, calc_poly(betas, xvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Information-theoretic Criteria\n",
    "\n",
    "How to pick the right model? One approach is to use an information-theoretic criterion to select the most appropriate model. For example **Akaike's Information Criterion (AIC)** balances the fit of the model (in terms of the likelihood) with the number of parameters required to achieve that fit. We can calculate AIC as:\n",
    "\n",
    "$$AIC = n \\log(\\hat{\\sigma}^2) + 2p$$\n",
    "\n",
    "where $p$ is the number of parameters in the model and $\\hat{\\sigma}^2 = \\text{RSS}/(n-p-1)$, where RSS is [this](https://en.wikipedia.org/wiki/Residual_sum_of_squares).\n",
    "\n",
    "Notice that as the number of parameters increase, the residual sum of squares goes down, but the second term (a penalty) increases.\n",
    "\n",
    "AIC is a metric of **information distance** between a given model and a notional \"true\" model. Since we don't know the true model, the AIC value itself is not meaningful in an absolute sense, but is useful as a relative measure of model quality.\n",
    "\n",
    "To apply AIC to model selection, we choose the model that has the **lowest** AIC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(x)\n",
    "\n",
    "aic = lambda rss, p, n: n * np.log(rss/(n-p-1)) + 2*p\n",
    "\n",
    "RSS1 = sum_of_squares(fmin(sum_of_squares, [0,1], args=(x,y)), x, y)\n",
    "RSS2 = sum_squares_quad(fmin(sum_squares_quad, [1,1,-1], args=(x,y)), x, y)\n",
    "\n",
    "print('\\nModel 1: {0}\\nModel 2: {1}'.format(aic(RSS1, 2, n), aic(RSS2, 3, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, on the basis of \"information distance\", we would select the 2-parameter (linear) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a polynomial model characterizes a nonlinear relationship, it is a linear problem in terms of estimation. That is, the regression model $f(y \\; |\\; x)$ is **linear in the parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian Linear Regression with PyMC3\n",
    "\n",
    "In practice, we need not fit least squares models by hand because they are implemented generally in packages such as [`scikit-learn`](http://scikit-learn.org/) and [`statsmodels`](https://github.com/statsmodels/statsmodels/). \n",
    "\n",
    "But we are not always interested in just obtaining a line of best fit. We also want **estimates of uncertainty** in the line and the parameters used to calculate the line.\n",
    "\n",
    "Let's turn to a **Bayesian approach** and build a **Bayesian regression model** with PyMC3. In a Bayesian model, instead of **minimizing distance** between data and the model, we pick probabilistic parameters that **maximize the probability** of observing the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "We'll pick a normal distribution as our model, with a mean and a standard deviation as parameters:\n",
    "\n",
    "$$y_i \\sim \\text{Normal}(\\mu_i, \\sigma)$$\n",
    "\n",
    "Here, $\\mu_i$ is the expected value of the *i*th observation (test score), which is generated by the regression model at the corresponding value of $x$ (amount of drug ingested). We calculate this expected value as a function of the regression parameters and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors\n",
    "The first step in specifying our model is to specify **priors** for our model.\n",
    "\n",
    "We'll use a **linear model** for μ:\n",
    "\n",
    "$$μ = \\text{intercept} + \\text{slope}*x$$\n",
    "    \n",
    "and Normal pdfs for `intercept` and `slope`:\n",
    "\n",
    "$$\\text{intercept}, \\text{slope} \\sim \\text{Normal}(0, 100)$$\n",
    "\n",
    "and also\n",
    "\n",
    "$$\\sigma \\sim \\text{HalfCauchy}(1)$$\n",
    "\n",
    "The **half-Cauchy** distribution (half of a [Cauchy](https://en.wikipedia.org/wiki/Cauchy_distribution)) has support over positive continuous values, and relatively large tail probabilities, allowing for the possibility of extreme values. That is another very often used modeling pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Normal, Model\n",
    "\n",
    "with Model() as lsd_model:\n",
    "    \n",
    "    intercept = Normal('intercept', 0, sd=100)\n",
    "    slope = Normal('slope', 0, sd=100)\n",
    "    σ = HalfCauchy('σ', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the half Cauchy. [Kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) is a non-parametric way to estimate the probability density function of a random variable, we'll use Seaborn's KDE method to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import HalfCauchy\n",
    "\n",
    "ax = sns.kdeplot(HalfCauchy.dist(1).random(size=10000), gridsize=2000)\n",
    "ax.set_xlim(0, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model for `μ`, and `score` as a function of `μ` and `σ`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with lsd_model:\n",
    "    \n",
    "    μ = ..\n",
    "    score = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, all right, all right! Now your regression model is fully specified and you are ready to track your **posteriors**.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"https://media1.tenor.com/images/c4b036354e1a6e6fedd3809a0c945003/tenor.gif?itemid=5146096\" width=\"400\" />\n",
    "All right, all right, all right\n",
    "</center>\n",
    "We can now use the fitting method of our choice to estimate a posterior distribution. Let's use a **Markov chain Monte Carlo** algorithm, called **NUTS** (the No U-Turn Sampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import sample\n",
    "\n",
    "with lsd_model:\n",
    "\n",
    "    lsd_sample = sample(1000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot posteriors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import plot_posterior\n",
    "\n",
    "plot_posterior(lsd_sample[500:], varnames=['slope', 'intercept', 'σ']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking model fit\n",
    "\n",
    "Let's evaluate model fit by comparing model predictions with the observations used to fit a model. The fitted model can be used to **simulate data**, and the distribution of the simulated data should resemble the distribution of the actual data.\n",
    "\n",
    "Ssimulating data from the model is a natural component of the Bayesian modelling framework. Recall, from our introduction to Bayesian inference, the **posterior predictive distribution**:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Here, $\\tilde{y}$ represents some hypothetical new data that would be expected, taking into account the posterior uncertainty in the model parameters. \n",
    "\n",
    "Sampling from the posterior predictive distribution is straighforward in PyMC; the `sample_ppc` function draws posterior predictive checks from all of the data likelihoods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import sample_ppc\n",
    "\n",
    "with lsd_model:\n",
    "    \n",
    "    lsd_ppc = sample_ppc(lsd_sample, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields 1000 samples corresponding to each of the seven data points in our observation vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsd_ppc['score'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare these simulated data to the data we used to fit the model. \n",
    "\n",
    "The model should prove that it could have been used to generate the data that we observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for ax, real_data, sim_data in zip(axes_flat[:-1], y, lsd_ppc['score'].T):\n",
    "    ax.hist(sim_data, bins=20)\n",
    "    ax.vlines(real_data, *ax.get_ylim(), colors='red')\n",
    "    ax.set_yticklabels([])\n",
    "    sns.despine(left=True)\n",
    "\n",
    "axes_flat[-1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/scoobs.png\" width=200 />\n",
    "</center>\n",
    "\n",
    "What are we saying about the model? A good Bayesian model is one whereby the test score is a normal distribution, where the mean of that distribution is a linear function of the amount of drug ingested. If we use the model, we can now make guesses for different drug data points, and obtain predicted test scores as well as errors we make in the predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
