{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 9</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 27 March 2019</div>\n",
    "\n",
    "# Lab: Machine Learning (ML) with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about human brains?\n",
    "\n",
    "The average human brain has about 100 billion neurons (nerve cells)\n",
    "- Jellyfish: 800\n",
    "- Snail: 11,000\n",
    "- Fruit Fly: 100,000\n",
    "- Ant: 250,000\n",
    "- Honey Bee: 960,000\n",
    "- Rat: 200,000,000\n",
    "- Cat: 760,000,000\n",
    "- Human: 86,000,000,000 (10$^{11}$)\n",
    "\n",
    "The average human brain has about 10$^{14}$ synapses (interconnections between neurons)in the neocortex (sight and hearing), and about 10$^{15}$ synapses in the entire nervous system. For the neocortex:\n",
    "- Rat: 10$^{10}$\n",
    "- Cat: 10$^{12}$\n",
    "- Human: 10$^{14}$\n",
    "\n",
    "The chemical connection between neurons are effectively **analog**, or floating-point. Each neuron operates once every 10 ms or 100 times per second, so the correct term isn't \"bits per second\" but FLOPS. Collectively, an estimate based on common google searches is 700 exaFLOP per second. For reference, the new supercomputer being installed at Argonne Labs operates at 1 exaFLOPS, no training (software).\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/hu-brain.jpg width = 200 />\n",
    "</center>\n",
    "\n",
    "#### How about large objects?\n",
    "\n",
    "Stars in the Milky Way: 200 billion stars\n",
    "\n",
    "Galaxies in the Universe: 100 billion galaxies\n",
    "\n",
    "Atoms in the Universe: ~10$^{80}$. This only accounts only for the observable universe which reaches 46 billion light years in any direction, and is based on where the expansion of space has taken the most distant objects observed\n",
    "Within this observable universe, this matter is spread homogeneously throughout space, at least when averaged over distances longer than 300 million light-years.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/milkyway.jpg width = 400 />\n",
    "    *Our Milky Way*\n",
    "</center>\n",
    "\n",
    "On smaller scales, however, matter is observed to form into the clumps of hierarchically-organized luminous matter that we are all familiar with. Most atoms are condensed into stars, most stars are condensed into galaxies, most galaxies into clusters, most clusters into superclusters and, finally, into the largest-scale structures like the [**Great Wall of galaxies**](https://en.wikipedia.org/wiki/Sloan_Great_Wall).\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/greatwall.jpg width = 400 />\n",
    "    *Looks like a brain to me..*\n",
    "</center>\n",
    "\n",
    "#### How about scales?\n",
    "\n",
    "- Billion (Giga): 10$^{9}$\n",
    "    - Neurons in a human brain\n",
    "    - Stars in the Milky Way\n",
    "    - How many dollars americans owe\n",
    "- Trillion (Tera): 10$^{12}$\n",
    "- Quadrillion (Peta): 10$^{15}$\n",
    "- Quintillion (Exa): 10$^{18}$\n",
    "- Sextillion (Zeta): 10$^{21}$\n",
    "    - Stars in the Universe\n",
    "    - Data produced by the human race\n",
    "- Septillion (Yotta): 10$^{24}$\n",
    "    - Atoms in the Universe\n",
    "- Googol: 10$^{100}$\n",
    "    - 10,001st Fibonacci number\n",
    "- Googolplex: 10$^{googol}$\n",
    "\n",
    "\n",
    "#### How about today's machines?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/asus.png width = 800 />\n",
    "</center>\n",
    "\n",
    "**Nvidia** was first to produce a chip capable of programmable shading, the GeForce 3. \n",
    "Each pixel could now be processed by a short program that could include additional image textures as inputs, and each geometric vertex could likewise be processed by a short program before it was projected onto the screen.\n",
    "\n",
    "Used in the Xbox console, it competed with the PlayStation 2 (which used a custom vector DSP for hardware accelerated vertex processing). \n",
    "By October 2002, with the introduction of the ATI Radeon 9700 (also known as R300), the world's first Direct3D 9.0 accelerator, pixel and vertex shaders could implement looping and lengthy floating point math, and were quickly becoming as flexible as CPUs, yet orders of magnitude faster for image-array operations.\n",
    "Nvidia's **CUDA** platform, first introduced in 2007, was the earliest widely adopted programming model for GPU computing\n",
    "More recently **OpenCL** has become broadly supported, an open standard defined by the Khronos Group which allows for the development of code for both GPUs and CPUs with an emphasis on portability. \n",
    "OpenCL solutions are supported by Intel, AMD, Nvidia, and ARM, and according to a recent report by Evan's Data, OpenCL is the GPGPU development platform most widely used by developers in both the US and Asia Pacific. \n",
    "\n",
    "CUDA is specifically for NVIDIA GPUs while OpenCL is designed to work across a multitude of architectures including GPU, CPU and DSP.\n",
    "\n",
    "#### Common GPUs found today on advanced laptops: \n",
    "\n",
    "- Nvidia GTX 1080: $500\n",
    "8873 GFLOPS, 7.2 billion transistors\n",
    "2560 NVIDIA CUDA Cores and 8GB GDDR5X \n",
    "\n",
    "- Nvidia GTX 1070: $300\n",
    "1920 CUDA cores and 8GB GDDR5X\n",
    "\n",
    "It takes about 20 transistors to form an AND gate, which is equivalent to a neuron (fires or does not fire depending on input signal). So in terms of hardware, we are still below the density of a human brain by an order of magnitude. In terms of software, Google's deep mind brain used 16,000 processors with around 1 billion intetrconnections. So, still well below the density of the human brain. And your laptop consumes an order of magnitude more power than your brain. But when you shut off your computer, nothing happens to it. When oxygen fails to reach a neuron, it dies in a few minutes.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/gtx-nividia.png width = 800 />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Keras\n",
    "\n",
    "Keras (κέρας) means [horn](https://en.wikipedia.org/wiki/Horn_(anatomy) in greek. It's a reference to a literary image from ancient Greek and Latin literature, first found in the [Odyssey](https://en.wikipedia.org/wiki/Odyssey), where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive with false visions and arrive to Earth through a gate of ivory ([blue pill](https://en.wikipedia.org/wiki/Red_pill_and_blue_pill) in [the Matrix®](https://en.wikipedia.org/wiki/The_Matrix)), and those who announce a future that will come to pass and arrive through a gate of horn ([red pill](https://en.wikipedia.org/wiki/Red_pill_and_blue_pill) in [the Matrix®](https://en.wikipedia.org/wiki/The_Matrix)). Also heavily referenced in [Harry Potter](https://en.wikipedia.org/wiki/Magical_creatures_in_Harry_Potter).\n",
    "\n",
    "*Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them*. - Homer, Odyssey 19 (Shewring translation).\n",
    "\n",
    "`Keras` is a minimalist, modular Neural Networks **API**, written in Python and capable of running on top of either TensorFlow or Theano (so it's really an API over an API). Developed with a focus on going from idea to result with the least possible delay, for fast prototyping (modularity, minimalism, extensibility). It supports both convolutional networks and recurrent networks, as well as combinations. It supports arbitrary connectivity schemes (including multi-input and multi-output training). It runs seamlessly on CPUs and GPUs. Initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System). Written by one person, [François Chollet](https://scholar.google.com/citations?user=VfYhf2wAAAAJ&hl=en).\n",
    "\n",
    "`Tensorflow` is Google's Machine Learning framework, while `Theano` is a framework written by [Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio) and his team at the University of Montreal.\n",
    "\n",
    "There are many quality deep learning frameworks to choose from, but the Keras framework is easy to get started with because of its intuitive high-level API. You can quickly prototype and develop new models.\n",
    "\n",
    "François Chollet, now a deep learning researcher at Google, developed the framework as part of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System). In 2017, Google’s TensorFlow team decided to support Keras in TensorFlow’s core library. Microsoft added a CNTK back end to the framework, which was available as of CNTK v2.0.\n",
    "\n",
    "François Chollet describes Keras in the following way:\n",
    "- Another important decision was to use an object-oriented design. Deep learning models can be understood as chains of functions, thus making a functional approach look potentially interesting. However, these functions are heavily parameterized, mostly by their weight tensors, and manipulating these parameters in a functional way would just be impractical. So in Keras, everything is an object: layers, models, optimizers, etc. All parameters of a model can be accessed as object properties: e.g. `model.layers[3].output` is the output tensor of the 3rd layer in the model, `model.layers[3].weights` is the list of symbolic weight tensors of the layer, and so on.\n",
    "\n",
    "Though developers initially built Keras on top of `Theano`, its abstraction ability made it easy for them to add TensorFlow shortly after Google released the back end. Eventually, the Keras API was implemented as part of Google TensorFlow.\n",
    "Now, the deep learning front end supports a number of back end implementations: TensorFlow, Theano, Microsoft Cognitive Toolkit (CNTK), Eclipse Deeplearning4J, and Apache MXNet.\n",
    "\n",
    "The framework runs on both CPUs and GPUs. It can use single or multiple GPUs to train deep neural networks, or it can run on a GPU using the NVIDIA CUDA Deep Neural Network GPU-accelerated library (cuDNN). This approach is much faster than a typical CPU because developers designed Keras to deal with parallel computation.\n",
    "\n",
    "It has been open sourced since its initial release in March 2015. Its documentation can be found on [keras.io](https://keras.io/) with source code on [GitHub](https://github.com/keras-team/keras).\n",
    "\n",
    "Keras' `Sequential model` is a linear stack of layers. You create a Sequential model by passing a list of layer instances to the constructor:\n",
    "\n",
    "Before training a model, you configure the learning process with 3 parameters:\n",
    "- Optimizer: This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class \n",
    "\n",
    "- Loss function: This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function\n",
    "\n",
    "- List of metrics: For any classification problem you will want to set this to metrics=['accuracy']\n",
    "\n",
    "References:\n",
    "\n",
    "- http://keras.io/ \n",
    "- http://keras.io/documentation/ \n",
    "- http://robotfuture.net \n",
    "- https://github.com/fchollet/keras "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Open an Anaconda prompt and:\n",
    "```python\n",
    "conda install keras\n",
    "```\n",
    "You should get that *the following NEW packages will be INSTALLED:*\n",
    "\n",
    "    _tflow_select:       2.2.0-eigen\n",
    "    absl-py:             0.6.1-py36_0\n",
    "    astor:               0.7.1-py36_0\n",
    "    gast:                0.2.0-py36_0\n",
    "    grpcio:              1.12.1-py36h1a1b453_0\n",
    "    keras:               2.2.4-0\n",
    "    keras-applications:  1.0.6-py36_0\n",
    "    keras-base:          2.2.4-py36_0\n",
    "    keras-preprocessing: 1.0.5-py36_0\n",
    "    libprotobuf:         3.6.0-h1a1b453_0\n",
    "    markdown:            3.0.1-py36_0\n",
    "    protobuf:            3.6.0-py36he025d50_0\n",
    "    tensorboard:         1.11.0-py36he025d50_0\n",
    "    tensorflow:          1.11.0-eigen_py36h346fd36_0\n",
    "    tensorflow-base:     1.11.0-eigen_py36h45df0d8_0\n",
    "    termcolor:           1.1.0-py36_1\n",
    "\n",
    "Proceed ([y]/n)? y\n",
    "\n",
    "Please proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Five neurons (review)\n",
    "\n",
    "We have a neuron that has 3 neurons upstream (in-signal) and another 2 neurons downstream (out-signal).\n",
    "\n",
    "Indexes for the incoming nodes (`fromNodes`), and the result nodes (`toNodes`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toNodes = range(4, 6)\n",
    "fromNodes = range(0, 3)\n",
    "allNodes = range(0, 6)\n",
    "print ('in-signal from neurons', [i for i in fromNodes])\n",
    "print ('out-signal towards neurons', [i for i in toNodes])\n",
    "print ('I am neuron #3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias       = [0.2, -0.1, 0.5, 0.1, 0.4, 0.9] # one per neuron\n",
    "activation = [0.8, -0.3, -0.8, 0.1, 0.5, 0.9] # the amount of signal it takes to wake up each\n",
    "netInput   = [0, 0, 0, 0, 0, 0] # initial input into each neuron\n",
    "weights = [[ 0., 0., 0., 0.1, 0., 0.],  # weights matrix only w03, w13, w23, w34, and w35 are non-zero\n",
    "          [ 0., 0., 0., -0.3, 0., 0.], \n",
    "          [ 0., 0., 0., 0.2, 0., 0.], \n",
    "          [ 0., 0., 0., 0., 0.1, -0.8], \n",
    "          [ 0., 0., 0., 0., 0., 0.], \n",
    "          [ 0., 0., 0., 0., 0., 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`toNodes` receive signals, `fromNodes` send them. Let's see who receives based on what is sent: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in toNodes:\n",
    "    netInput[i] = bias[i]\n",
    "    for j in fromNodes:\n",
    "        netInput[i] += (weights[i][j] * activation[j]) \n",
    "netInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the same if we look at the entire matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in allnodes:\n",
    "    netInput[i] = bias[i]\n",
    "    for j in allNodes:\n",
    "        netInput[i] += (weights[i][j] * activation[j]) \n",
    "netInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which you can do with a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netinput_nonzero = [bias[i] + sum([weights[i][j] * activation[j] for j in fromNodes]) for i in toNodes]\n",
    "netinput_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..or also with a simple linear algebra formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "net_state = np.array(bias) + np.array(weights) @ np.array(activation)\n",
    "net_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and that is why we learned **linear algebra**, isn't the formula above much simpler than using indexes, loops, or list comprehensions?\n",
    "\n",
    "Now, this is the non-linear activation function happening at each neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def activationFunction(netInput):\n",
    "    return 1.0 / (1.0 + math.exp(-netInput))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the output from each neuron is not really just what it receives from the upstream neurons, but what it receives *transformed by the activation function*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in allNodes:\n",
    "    activation[i] = activationFunction(net_state[i])\n",
    "activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " net_state = activationFunction ( np.array(bias) + np.array(weights) @ np.array(activation) )\n",
    "net_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..oops! Why? We need to **vectorize** the activation function so it can work on vector inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one(v):\n",
    "    return v + 1\n",
    "\n",
    "np.vectorize(add_one)([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activationFunction_v = np.vectorize(activationFunction)\n",
    "new_net_state =  activationFunction_v( np.array(bias) + np.array(weights) @ np.array(activation) )\n",
    "new_net_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! One operation and we have the next state of our neural network :-)\n",
    "\n",
    "If the neuron received *no new inputs*, you could now theoretically compute the long-term state of the neural network, just by figuring out the dominant eigenvector!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. One Neuron model with Keras\n",
    "\n",
    "This is a single neuron in your brain:\n",
    "<center>\n",
    "<img src =ipynb.images/neuron1.png width = 400 />\n",
    "</center>\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/neuron.png width = 400 />\n",
    "</center>\n",
    "\n",
    "This is the artificial equivalent, with many inputs going into that neuron, and multiple outputs:\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/neuron-many.png width = 400 />\n",
    "</center>\n",
    "\n",
    "I am confident you can write this in python because we *learned linear algebra* in class, and the equation above is nothing more than a matrix (neuron weights per synapse) mutlipled by a (input) vector, leading to another (output) vector. If i asked you in your final exam to compute the dominant eigenvector *of your brain*, you could do that, right?\n",
    "\n",
    "Now, with a **single input** and **a single output**, thus a **single synapse** for the neuron, it simplifies to:\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/neuron-one.png width = 400 />\n",
    "</center>\n",
    "\n",
    "$w_0$ is the *bias* of the neuron: It fires *irrespective* of the input, because your neuron has been preconditioned by long-term past input. It's as if your boyfriend says *I don't want you to see Lin Kai again*, and you snap and you say *stop being jealous!*. Would you say that if that's the first time he asked you not to see someone? **No**, but what if it's the thousand-th time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some ridiculously simple data to model with an artificial neural network (ANN) composed of a single neuron with a single synapse. Not possible to do simpler!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the signal we're going to auto-encode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "n_points = 200\n",
    "x = np.linspace(0, 2, n_points)\n",
    "y = np.array([0] * int(n_points / 2) + list(x[:int(n_points / 2)])) * 2\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(x, y, linewidth=2)\n",
    "plt.title('ridiculously simple data')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a network with a single neuron with `Keras`, with the simplest nonlinear activation function ([ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))), which looks supsiciously similar to the input data (and so professor is cheating because he knows we should be pretty successful modeling a function that looks like the activation function of our neurons), to auto-encode our input data (in other words, the output should be the same as the input). The error metric in training is the means squared error between the data and the model output, and the training optimizer algorithm is stochastic gradient descent (we'll reexamine training algorithms in later lectures). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=1, input_dim=1, init=\"normal\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our training history by defining, and then calling, the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class TrainingHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.predictions = []\n",
    "        self.i = 0\n",
    "        self.save_every = 50\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.i += 1        \n",
    "        if self.i % self.save_every == 0:        \n",
    "            pred = model.predict(X_train)\n",
    "            self.predictions.append(pred)\n",
    "            \n",
    "history = TrainingHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our neuron with 2000 traning timesteps, using no additional hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          nb_epoch=2000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print our neuron's *weights*. That is your *model* in the same way that an analytic pdf is the model of your data. \n",
    "\n",
    "In the case of ANNs, the model consists of the weights of your ANNs neurons. In the case of *you*, your total identity as a person different from the person sitting next to you is contained in the weights of the neurons in your brain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print trained weights\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "print('neural net weigths after training w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh.. one of the weight is *zero*. Bad news.. Did we successfully model our ridiculously simple data? Let's see.\n",
    "\n",
    "Create folders `ml_images` and `ml_videos` in your `C:\\Users\\<username>` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the animation\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y,  label='data')\n",
    "line, = plt.plot(x, history.predictions[0],  label='prediction')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, that didn't work too well, did it?\n",
    "\n",
    "Let's build animations to debug the process. This is pretty much how you debug machines: You inspect them after every training state. Training *is* the program. \n",
    "\n",
    "You're going to need to download ffmpeg *and* install it at `C:\\ffmpeg`.\n",
    "\n",
    "- For Windows: https://ffmpeg.zeranoe.com/builds/\n",
    "- Directions: http://www.wikihow.com/Install-FFmpeg-on-Windows \n",
    "- For OSX: http://www.renevolution.com/ffmpeg/2013/03/16/how-to-install-ffmpeg-on-mac-os-x.html \n",
    "\n",
    "To reset your PATH environment variable without killing and restarting your command console:\n",
    "SET PATH=%PATH%;C:\\ffmpeg\\bin\n",
    "\n",
    "To do it all in the same notebook, run the cell below.\n",
    "\n",
    "Now, let's use `ffmpeg` to create a video of the training process in the neuron. Imagine if we could create videos of the neurons in your brain learning.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# add to your PATH environment variable:\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/ffmpeg/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you may need an extra `/` at the end of `C:/ffmpeg/bin`. Note that on the Mac, it installs in `/usr/local/bin` automatically, so that is the path you will need. You may also have to include it in your PYTHONPATH environment variable. Afterwards, you should be able to run:\n",
    "```python\n",
    "!ffmpeg -version\n",
    "```\n",
    "in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to have to create folders referenced in the cells below. **Read and understand the code** before you execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_line(num):\n",
    "    plt.title('iteration: {0}'.format((history.save_every * (num + 1))))\n",
    "    line.set_xdata(x)\n",
    "    line.set_ydata(history.predictions[num])\n",
    "    return []\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update_line, len(history.predictions),\n",
    "                                   interval=50, blit=True)\n",
    "ani.save('ml_videos2/neuron.mp4', fps=30, extra_args=['-vcodec', 'libx264', '-pix_fmt','yuv420p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the file `ml_videos2/neuron.mp4`. What happens?\n",
    "\n",
    "This is the learning starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y, label='data')\n",
    "plt.plot(x, history.predictions[0], label='prediction')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('iteration: 0')\n",
    "plt.savefig('ml_images2/neuron_start.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the learning ending point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y, label='data')\n",
    "plt.plot(x, history.predictions[30], label='prediction')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('iteration: 30')\n",
    "plt.savefig('ml_images2/neuron_finish.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(history.losses)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('training error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the training error improve (get reduced)? Is your end state different from your start state? Did your artificial neural network learn anything?\n",
    "\n",
    "Hmm... Does not look like we are learning very much. Let's modify with learning hyperparameter `batch_size`, so we're actually running a mini-batch neural network.\n",
    "\n",
    "`batch_size` defines the number of samples that will be propagated through the network.\n",
    "\n",
    "For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next it takes the second 100 samples (from 101st to 200th) and trains the network again. We can keep doing this procedure until we have propagated through all samples of the network. A problem usually happens with the last set of samples. In our example we've used 1050 which is not divisible by 100 without remainder. The simplest solution is just to get the final 50 samples and train the network.\n",
    "\n",
    "Advantages of using a batch size < number of all samples:\n",
    "- It requires less memory. Since you train the network using fewer samples, the overall training procedure requires less memory. That's especially important if you are not able to fit the whole dataset in your machine's memory.\n",
    "\n",
    "Typically networks train faster with mini-batches. That's because we update the weights after each propagation. \n",
    "\n",
    "Disadvantages of using a batch size < number of all samples:\n",
    "- The smaller the batch the less accurate the estimate of the gradient will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: larger batch size\n",
    "\n",
    "history = TrainingHistory()\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=1, input_dim=1, init=\"normal\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=200,\n",
    "          nb_epoch=2000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at our training error now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(history.losses)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('training error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! It looks like we're learning something!\n",
    "\n",
    "Ok, just to make sure this is not by chance, let's shuffle our random number generator, repeat, compare our neuron's weights before and after training, plot training error, and visualize the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "history = TrainingHistory()\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=1, input_dim=1, init=\"normal\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "print('neural net initialized with weigths w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=200,\n",
    "          nb_epoch=2000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])\n",
    "\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "print('neural net weigths after training w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals()))\n",
    "print('dying ReLU problem!')\n",
    "print('http://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about the dying ReLU problem [here](http://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks).\n",
    "\n",
    "before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y,  label='data')\n",
    "line, = plt.plot(x, history.predictions[0],  label='prediction')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y,  label='data')\n",
    "line, = plt.plot(x, history.predictions[30],  label='prediction')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'training error')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(history.losses)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('training error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save training animation in file `ml_videos2/neuron2.mp4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = animation.FuncAnimation(fig, update_line, len(history.predictions),\n",
    "                                   interval=50, blit=True)\n",
    "ani.save('ml_videos2/neuron2.mp4', fps=30, extra_args=['-vcodec', 'libx264', '-pix_fmt','yuv420p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run your video `ml_videos2/neuron2.mp4` abd see what your network learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 60 Neurons model with Keras\n",
    "\n",
    "Ok, now we're ready to model more complicated data, using more neurons and more complicated network architectures. \n",
    "\n",
    "We're going to use a single hidden layers of 60 neurons first, and after that we're going to modify to a double hidden layer of equal complexity (which mean same *number* of neural weights). How many neurons per layer for the two-layer ANN? **Think about this**, figure out the answer, and **tell professor** before proceeding to the next cell.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/neurons-60.png width = 400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to model a noisy sine curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.linspace(0, 2 * math.pi, 200)\n",
    "sine = np.sin(x)\n",
    "err = np.random.normal(0, 0.2, len(sine))\n",
    "y = sine + err\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(x, y)\n",
    "plt.xlim([0, 2 * math.pi])\n",
    "plt.title('A noise sine')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class TrainingHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.predictions = []\n",
    "        self.i = 0\n",
    "        self.save_every = 50\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.i += 1        \n",
    "        if self.i % self.save_every == 0:        \n",
    "            pred = model.predict(X_train)\n",
    "            self.predictions.append(pred)\n",
    "            \n",
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = TrainingHistory()\n",
    "res = model.fit(X_train,\n",
    "          Y_train,\n",
    "          nb_epoch=5000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "def visualize_training(history, name):\n",
    "    fig = plt.figure(figsize=(5, 2.5))\n",
    "    plt.plot(x, y, label='data')\n",
    "    line, = plt.plot(x, history.predictions[0],  label='prediction')\n",
    "    plt.legend()\n",
    "\n",
    "    def update_line(num):\n",
    "        plt.title('iteration: {0}'.format((history.save_every * (num + 1))))\n",
    "        line.set_xdata(x)\n",
    "        line.set_ydata(history.predictions[num])\n",
    "        return []\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update_line, len(history.predictions),\n",
    "                                       interval=50, blit=True)\n",
    "    ani.save('ml_videos2/{0}.mp4'.format(name), dpi=100, extra_args=['-vcodec', 'libx264', '-pix_fmt','yuv420p'])\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5, 2.5))\n",
    "    plt.plot(x, y, label='data')\n",
    "    plt.plot(x, history.predictions[0], label='prediction')\n",
    "    plt.legend()\n",
    "    plt.title('iteration: 0')\n",
    "    plt.savefig('ml_images2/{0}.png'.format(name))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(history.losses)\n",
    "    plt.ylabel('error')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylim([0, 0.5])\n",
    "    plt.title('training error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training(history, 'noisy-sine-one-layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the new `noisy-sine-one-layer.mp4` file in your `ml_videos2` folder (but *wait for training to finish!*). Are we learning? \n",
    "\n",
    "Hmmm.. Looks like we're not learning. One layer is not enough! That's what led researchers in the 1960s to say that neural networks don't work! But then researchers in Canada in the 1980s thought about adding hidden layers..\n",
    "\n",
    "So let's increase our architecture to two layers, with 10 neurons in each layer, while keeping the same network complexity (i.e. the same number of weights in the connections between neurons). As you guessed previously, that leads to 10 (weights corresponding to connections between input node and first hidden layer) + 100 (weights corresponding to connections between the two hidden layers) + 10 (weights corresponding to connections between second hidden layer and output node) weigths overall, which is similar to the number of weights of our single layers with 60 neurons (60 + 60). So, equal complexity, but more hidden layers.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/neurons-10-10.png width = 200 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conn = 10\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=n_conn, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=n_conn))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=1))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T\n",
    "history = TrainingHistory()\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          nb_epoch=5000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])\n",
    "\n",
    "visualize_training(history, 'tiny-sine-two-layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the new `tiny-sine-two-layer.mp4` file in your `ml_videos2` folder.\n",
    "\n",
    "Much better! \n",
    "\n",
    "Now, for a different kind of impreovement on the single one layer: Let's use an optimizer (ADAM), so you can see that a single layer can get it right, too!\n",
    "\n",
    "The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.\n",
    "\n",
    "Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n",
    "\n",
    "Adam was presented by Diederik Kingma from OpenAI and Jimmy Ba from the University of Toronto in their 2015 ICLR paper (poster) [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980). \n",
    "\n",
    "Gentle Introduction to the Adam Optimization Algorithm for Deep Learning\n",
    "by Jason Brownlee on July 3, 2017 in Better Deep Learning\n",
    "Tweet   Share \n",
    "The choice of optimization algorithm for your deep learning model can mean the difference between good results in minutes, hours, and days.\n",
    "\n",
    "The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. It is used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n",
    "\n",
    "Adam was presented by Diederik Kingma from OpenAI and Jimmy Ba from the University of Toronto in their 2015 ICLR paper (poster) titled “Adam: A Method for Stochastic Optimization“. I will quote liberally from their paper in this post, unless stated otherwise.\n",
    "\n",
    "The algorithm is called Adam. It is not an acronym and is not written as “ADAM”. The name Adam is derived from **adaptive moment estimation**.\n",
    "\n",
    "When introducing the algorithm, the authors list the attractive benefits of using Adam on non-convex optimization problems, as follows:\n",
    "\n",
    "- Straightforward to implement.\n",
    "- Computationally efficient.\n",
    "- Little memory requirements.\n",
    "- Invariant to diagonal rescale of the gradients.\n",
    "- Well suited for problems that are large in terms of data and/or parameters.\n",
    "- Appropriate for non-stationary objectives.\n",
    "- Appropriate for problems with very noisy/or sparse gradients.\n",
    "- Hyper-parameters have intuitive interpretation and typically require little tuning.\n",
    "\n",
    "Stochastic gradient descent (we learned about this in our previous lecture) maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training. A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds. The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n",
    "\n",
    "The authors describe Adam as combining the advantages of two other extensions of stochastic gradient descent. Specifically:\n",
    "\n",
    "- Adaptive Gradient Algorithm (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
    "\n",
    "- Root Mean Square Propagation (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n",
    "\n",
    "Adam is a very popular algorithm in the field of deep learning because it achieves good results *fast*. Let's apply it to our *single layer: network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "n_conn = 60\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=n_conn, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=1))\n",
    "adam = Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T\n",
    "history = TrainingHistory()\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          nb_epoch=5000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])\n",
    "\n",
    "visualize_training(history, 'tiny-sine-one-layer-adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine `tiny-sine-one-layer-adam.mp4`. Wow, *now* we're learning!\n",
    "\n",
    "But that does not necessarily mean you can get similar improvements with the same optimizer on the two-layer version! That's the dark art of **hyperparameter turning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "n_conn = 10\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=n_conn, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=n_conn))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=1))\n",
    "adam = Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T\n",
    "history = TrainingHistory()\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          nb_epoch=5000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])\n",
    "\n",
    "visualize_training(history, 'tiny-sine-two-layer-adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine `tiny-sine-two-layer-adam.mp4`. Looked promising at the beginning, we reduced the training error by half the amount at the same timestep with the non-adam version, but the training error stopped improving after a certain point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Image Classification with CIFar10\n",
    "\n",
    "Ok, by now you should know how artificial neural networks work, at least as autoencoders. Now let's do some real Machine Learning and classify images, instead of autoencoding using the CIFAR10 dataset. This means we don;t have to learn the entire signal, we just have to learn how to discriminate between 10 different *kinds* of images.\n",
    "\n",
    "It's as if you don't have to learn *everything* about *all* potential girlfriends, but only to distinguish between the high-maintenance ones, and the low-maintenance ones.\n",
    "\n",
    "Noe, please switch to notebook `keras-cifar10-load_model.ipynb`, which loads pre-trained network weights from my laptop, and run the rest of this notebook at home, because it might take your laptop all night to train over the CIFar10 dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load CIFAR10 straight from Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split into training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a 3-layers convolutional ANN, also called a [CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network). It's a specific kind of neural architecure, similar to the neural connections of your vision circuits. Typically, you use CNNs for vision, and [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network) for language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model. That will take some time on your laptop. You won't be finished in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a long time to compute: Make sure you're plugged in, and ready to **do something else** while your laptop sweats it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "    \n",
    "        # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now save the neural weights for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3.5.1\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, input_dim=20, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "d:\\Anaconda3.5.1\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "d:\\Anaconda3.5.1\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#extra cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a comprehensive routine to show images from cifar10, given indexes into the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing CIFAR 10, takes indicides and shows in a grid\n",
    "def cifar_grid(X,Y,inds,n_col, predictions=None):\n",
    "  import matplotlib.pyplot as plt\n",
    "  import math\n",
    "  if predictions is not None:\n",
    "    if Y.shape != predictions.shape:\n",
    "      print(\"Predictions must equal Y in length!\\n\")\n",
    "      return(None)\n",
    "  N = len(inds)\n",
    "  n_row = int(math.ceil(1.0*N/n_col))\n",
    "  fig, axes = plt.subplots(n_row,n_col,figsize=(10,10))\n",
    "  \n",
    "  clabels = labels[\"label_names\"]\n",
    "  for j in range(n_row):\n",
    "    for k in range(n_col):\n",
    "      i_inds = j*n_col+k\n",
    "      i_data = inds[i_inds]\n",
    "      \n",
    "      axes[j][k].set_axis_off()\n",
    "      if i_inds < N:\n",
    "        axes[j][k].imshow(X[i_data,...], interpolation=\"nearest\")\n",
    "        label = clabels[np.argmax(Y[i_data,...])]\n",
    "        axes[j][k].set_title(label)\n",
    "        if predictions is not None:\n",
    "          pred = clabels[np.argmax(predictions[i_data,...])]\n",
    "          if label != pred:\n",
    "            label += \" n\"\n",
    "            axes[j][k].set_title(pred, color=\"red\")\n",
    "  \n",
    "  fig.set_tight_layout(True)\n",
    "  return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a comprehensive routine to load cifar10 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_load_cifar(verbose=False):\n",
    "  import os,shutil\n",
    "  from keras.datasets import cifar10\n",
    "  from keras.utils import to_categorical\n",
    "  \n",
    "  #datadir = os.path.expanduser(\"~\") + \"/.keras/datasets/\"\n",
    "  datadir = \"/data\"\n",
    "  datafile = datadir+\"cifar-10-batches-py.tar.gz\" # the name keras looks for\n",
    "  \n",
    "  #if not os.path.isfile(datafile):\n",
    "  #  os.makedirs(datadir)\n",
    "  #  shutil.copyfile($$ref{{[\"~:output\",\"119210b3-a610-428e-93f2-ad5d987f442b\",\"cifar-10-python.tar.gz\"]}}, datafile)\n",
    "  \n",
    "  # The data, shuffled and split between train and test sets:\n",
    "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "  if verbose:\n",
    "    print(\"x_train shape: {}, {} train samples, {} test samples.\\n\".format(\n",
    "      x_train.shape, x_train.shape[0], x_test.shape[0]))\n",
    "  \n",
    "  # Convert class vectors to binary class matrices.\n",
    "  y_train = to_categorical(y_train, num_classes)\n",
    "  y_test = to_categorical(y_test, num_classes)\n",
    "  \n",
    "  x_train = x_train.astype(\"float32\")\n",
    "  x_test = x_test.astype(\"float32\")\n",
    "  x_train /= 255.0\n",
    "  x_test /= 255.0\n",
    "  \n",
    "  # Load label names to use in prediction results\n",
    "  label_list_path = \"datasets/cifar-10-batches-py/batches.meta\"\n",
    "  \n",
    "  keras_dir = os.path.expanduser(os.path.join(\"~\", \".keras\"))\n",
    "  datadir_base = os.path.expanduser(keras_dir)\n",
    "  if not os.access(datadir_base, os.W_OK):\n",
    "    datadir_base = os.path.join(\"/tmp\", \".keras\")\n",
    "  label_list_path = os.path.join(datadir_base, label_list_path)\n",
    "  \n",
    "  with open(label_list_path, mode=\"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "  \n",
    "  return x_train, y_train, x_test, y_test, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dill` extends python’s pickle module for serializing and de-serializing python objects to the majority of the built-in python types. Serialization is the process of converting an object to a byte stream, and the inverse of which is converting a byte stream back to on python object hierarchy.\n",
    "\n",
    "`dill` provides the user the same interface as the pickle module, and also includes some additional features. In addition to pickling python objects, dill provides the ability to save the state of an interpreter session in a single command. Hence, it would be feasable to save a interpreter session, close the interpreter, ship the pickled file to another computer, open a new interpreter, unpickle the session and thus continue from the ‘saved’ state of the original interpreter session.\n",
    "\n",
    "Please open an anaconda prompt (terminal on the Mac) and:\n",
    "```python\n",
    "conda install dill\n",
    "```\n",
    "\n",
    "Then extract the labesl using our comprehensive routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install dill\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "from math import *\n",
    "_,_,_,_,labels = setup_load_cifar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some random images and their label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "indices = [np.random.choice(range(len(x_train))) for i in range(36)]\n",
    "cifar_grid(x_train,y_train,indices,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download an image of a cat, or dog, or horse, or frog, or deer, or bird, or ship, or airplane, or automobile, or truck, and test it against the model. Keep a score :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras.backend.tensorflow_backend as K\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tensorflow.read_file('data/Eirplane.jpg')\n",
    "img = tensorflow.image.decode_jpeg(img, channels=3)\n",
    "img.set_shape([None, None, 3])\n",
    "img = tensorflow.image.resize_images(img, (32, 32))\n",
    "img = img.eval(session=sess) # convert to numpy array\n",
    "img = np.expand_dims(img, 0) # make 'batch' of 1\n",
    "\n",
    "pred = model.predict(img)\n",
    "pred = labels[\"label_names\"][np.argmax(pred)]\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this concludes our introduction to Machine Learning :-) Next week, we will look at **regression trees**, and then we'll build models with a Bayesian approach, because most neural networks use a frequentist approach: essentially autoencoding by maximizing log-likelihood (maximum likelihood estimation - MLE). **Bayesian Neural Nets** (BNNs) are heavily researched today.\n",
    "\n",
    "# Homework\n",
    "\n",
    "Pick 10 images of a cat, or dog, or horse, or frog, or deer, or bird, or ship, or airplane, or automobile, or truck from the Web, and classify them with Keras. if you cannot train Keras because it takes too long or any other issues arise, use my pretrained weights which I uploaded to blackboard. Submit all your images and your Keras classification thereof as one jpg file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
