{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 9</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, ! April 2019, with material from Joe McCarthy and Chris Roach</div>\n",
    "\n",
    "Last semsester a student emailed me this video and told me that's what my lectures felt like:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/6105stat.gif width = 250 />\n",
    "</center>\n",
    "\n",
    "So I tried to slow down my lectures. But we have a lot of material to cover today because we're actually going to encode a decision tree algorithm. On Wednesday, we'll have a lab with `scikit-learn` implementations.\n",
    "\n",
    "We spent the first part of the semester learning about how to think of data and fit the data to a model. Now let's make it more complicated and think about how to design machines to do the same thing :-) \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/irobot.jpg width = 300 />\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision Trees and regression trees\n",
    "\n",
    "The idea is to split a dataset based on **homogeneity of data**. A **decision tree** is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar **values** (homogenous). \n",
    "\n",
    "On the other hand, in a **regression tree**, since the target (dependent) variable is a real valued number, we fit a regression model to the target variable using each of the independent variables. \n",
    "\n",
    "Then for each independent variable, the data is split at several split points. We calculate Sum of Squared Error(SSE) at each split point between the predicted value and the actual values. The variable resulting in minimum SSE is selected for the node. Then this process is recursively continued till the entire data is covered. Each split point may belong to a different independent variable.\n",
    "\n",
    "All of us actually use decision trees in our daily life! To illustrate the concept, let's use an everyday example: predicting tomorrow’s maximum temperature for Boston. Wasn't today so much colder than yesterday?!\n",
    "\n",
    "In order to answer the single max temperature question, we need to work through an entire series of queries. We start by forming an initial reasonable range given our domain knowledge, which for Boston is very little.. Let's say that it's 30–60 degrees Fahrenheit. Gradually, through a set of questions and answers we will reduce this range until we are confident enough to make a single prediction.\n",
    "\n",
    "What makes a good question to start with? What kind of idnependent variable should we split the data by? Well, if we want to limit the range **as much as possible** initially, let's think of the most relevant question to ask. Since temperature is highly dependent on time of year, a decent place to start would be: what season are we in? Winter *close to spring*, right? So we can limit the prediction range to 30–50 degrees because we have an idea of what the general max temperatures are in Boston winter-close-to-spring-time (yes, yesterday was 60 but that was an outlier). This first question already cuts our range by a lot. We use that independent variable as our first node variable. But, this question isn’t quite enough to narrow down our estimate so we need to find out more information for our second node.\n",
    "\n",
    "A good follow-up question is: what is the historical average max temperature on this day? For Boston, the answer is 36 degrees. This allows us to further restrict our range of consideration to, let's say, 30–40 degrees. \n",
    "\n",
    "Two questions (two nodes)  are still not quite enough to make a prediction because this year might be warmer or colder than average. Therefore, we also would want to look at the max temperature today to get an idea if the year has been unusually warm or cold. Our question is simple: what is the maximum temperature today? If the max temperature today was 40 degrees, it might be colder this year and our estimate for tomorrow should be a little lower than the historical average. At this point, we can feel pretty confident in making a prediction of 35 degrees for the max temperature tomorrow. \n",
    "\n",
    "So, to arrive at an estimate, we used a series of questions, with each question narrowing our possible values until we were confident enough to make a single prediction. So, following one path (the most probable one) down the tree, we used 3 nodes to make a decision. \n",
    "\n",
    "We also need to complete all paths and add nodes to all split points so we have a decision for each leaf of the tree (we did not do this in our questioning above).\n",
    "\n",
    "**Regression Forests** are different than a single tree: They are an **ensemble** of different regression trees. These models work on the principle of the **wisdom of the crowd** . In short, it is better to consider the opinions of 1000 different people with not much knowledge than to consider the opinion of only one expert (provided the 1000 people have accuracy better than random guessing, i.e more than 50%). There is actually a mathematical proff about this.\n",
    "\n",
    "Ok, is our intuition about the algorithm in place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Computer Science *Tree* \n",
    "\n",
    "In computer science, a **tree** is a widely used abstract data type (ADT) that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes. A tree ends in leaf nodes and is represented upside node with the root at the top.\n",
    "\n",
    "A **decision tree** is a decision support tool that uses a tree-like model of decisions and their possible outcomes. Much like a [graph](https://en.wikipedia.org/wiki/Graph_theory) is a way to display transitions of state machines, a [tree](https://en.wikipedia.org/wiki/Tree_(graph_theory) is a specialization of a graph that displays conditional control statements.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/election.png width = 500 />\n",
    "</center>\n",
    "\n",
    "Decision trees are commonly used in operations research and [Machine Learning](https://en.wikipedia.org/wiki/Decision_tree_learning).\n",
    "\n",
    "Some techniques, often called **ensemble methods**, construct *more than one* decision tree, and thus talk about **decision forests** rather than trees. For example:\n",
    "\n",
    "- **Boosted trees** incrementally build an ensemble by training each new instance to emphasize the training instances previously mis-modeled. A typical example is [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost#LogitBoost)\n",
    "- **Bootstrap aggregated** (or **bagged**) decision trees, an early ensemble method, build multiple decision trees by repeatedly resampling training data *with replacement*, and voting the trees for a consensus prediction\n",
    "- A **random forest classifier** is a specific type of bootstrap aggregating rotation forest in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features\n",
    "\n",
    "Decision Trees are simple to understand, maybe the simplest, albeit powerful, ML method there is, since trees can also be displayed graphically in a way that is easy for non-experts to understand. Trees are able to handle both numerical and categorical data. For example, relation rules can be used with nominal variables while neural networks can be used with numerical variables or categoricals converted to 0-1 values.\n",
    "\n",
    "Decision trees use a **white box model**: If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a **black box model**, the explanation for the results is typically difficult to understand, as for example with artificial neural networks (unless they're Bayesian).\n",
    "\n",
    "Decision Trees perform well with large datasets. Large amounts of data can be analysed using standard computing resources in reasonable time and mirror human decision making pretty closely.\n",
    "\n",
    "Finally, Decision trees can be sampled using **MCMC**: By constructing a Markov chain that has the desired distribution as its equilibrium distribution, we can obtain a sample of the desired distribution by observing the chain after a number of steps. For example, [here](https://www2.stat.duke.edu/courses/Fall05/sta395/casper1.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Algorithm type: Supervised Classification\n",
    "\n",
    "[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning), or **Classification**, is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. In unsupervised learning, you are essentially missing the output.\n",
    "\n",
    "<img src=\"http://www.nltk.org/images/supervised-classification.png\" title=\"Supervised Classification, from NLTK book, Chapter 6\" alt=\"nltk_ch06_supervised-classification.png\" style=\"width: 500px\" /></a>\n",
    "\n",
    "> (a) During *training*, a **feature extractor** is used to convert each **input value** to a **feature set**. These feature sets, which capture the basic information about each input that should be used to classify it, are discussed in the next section. Pairs of feature sets and **labels** are fed into the **machine learning algorithm** to generate a **model**. (b) During *prediction*, the same feature extractor is used to convert **unseen inputs** to feature sets. These feature sets are then fed into the model, which generates **predicted labels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The data: UCI Mushrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Center for Machine Learning and Intelligent Systems](http://cml.ics.uci.edu/) at the University of California, Irvine (UCI), hosts a  [Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html) containing over 200 publicly available data sets. It is yours truly most loved (and used) dataset archive.\n",
    "\n",
    "<img src=\"ipynb.images/mushrooms.jpg\" alt=\"mushroom\" width = 400/>\n",
    "\n",
    "<a href=\"https://archive.ics.uci.edu/ml/datasets/Mushroom\"><img src=\"https://archive.ics.uci.edu/ml/assets/MLimages/Large73.jpg\"  style=\"margin: 0px 0px 5px 20px; width: 125px; float: right;\" title=\"Mushrooms from Agaricus and Lepiota Family\" alt=\"mushroom\"/></a>\n",
    "For our decision tree data, will use the [mushroom](https://archive.ics.uci.edu/ml/datasets/Mushroom) data set, used in Chapter 3 of Provost & Fawcett [data science book](https://www.amazon.com/Data-Science-Business-Data-Analytic-Thinking/dp/1449361323).\n",
    "\n",
    "The following description of the dataset is provided at the UCI repository:\n",
    "\n",
    ">This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family (pp. 500-525 [The Audubon Society Field Guide to North American Mushrooms, 1981]). Each species is identified as:\n",
    "- definitely edible\n",
    "- definitely poisonous\n",
    "- of unknown edibility and not recommended. \n",
    "\n",
    ">This latter class is usually combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom..\n",
    "> \n",
    "> **Number of Instances**: 8124\n",
    "> \n",
    "> **Number of Attributes**: 22 (all nominally valued)\n",
    "> \n",
    "> **Attribute Information**: (*classes*: edible=e, poisonous=p)\n",
    "> \n",
    "> 1. *cap-shape*: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s\n",
    "> 2. *cap-surface*: fibrous=f, grooves=g, scaly=y, smooth=s\n",
    "> 3. *cap-color*: brown=n ,buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y\n",
    "> 4. *bruises?*: bruises=t, no=f\n",
    "> 5. *odor*: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s\n",
    "> 6. *gill-attachment*: attached=a, descending=d, free=f, notched=n\n",
    "> 7. *gill-spacing*: close=c, crowded=w, distant=d\n",
    "> 8. *gill-size*: broad=b, narrow=n\n",
    "> 9. *gill-color*: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y\n",
    "> 10. *stalk-shape*: enlarging=e, tapering=t\n",
    "> 11. *stalk-root*: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?\n",
    "> 12. *stalk-surface-above-ring*: fibrous=f, scaly=y, silky=k, smooth=s\n",
    "> 13. *stalk-surface-below-ring*: fibrous=f, scaly=y, silky=k, smooth=s\n",
    "> 14. *stalk-color-above-ring*: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\n",
    "> 15. *stalk-color-below-ring*: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\n",
    "> 16. *veil-type*: partial=p, universal=u\n",
    "> 17. *veil-color*: brown=n, orange=o, white=w, yellow=y\n",
    "> 18. *ring-number*: none=n, one=o, two=t\n",
    "> 19. *ring-type*: cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z\n",
    "> 20. *spore-print-color*: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y\n",
    "> 21. *population*: abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y\n",
    "> 22. *habitat*: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d\n",
    "> \n",
    "> **Missing Attribute Values**: 2480 of them (denoted by \"?\"), all for attribute #11 (starting from 0 indeex).\n",
    "> \n",
    "> **Class Distribution**: -- edible: 4208 (51.8%) -- poisonous: 3916 (48.2%) -- total: 8124 instances\n",
    "\n",
    "The [data file](https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data) associated with this dataset, up on blackboard, has one instance of a hypothetical mushroom per line, with abbreviations for the values of the class and each of the other 22 attributes separated by commas.\n",
    "\n",
    "Here is a sample line from the data file:\n",
    "\n",
    "p,k,f,n,f,n,f,c,n,w,e,?,k,y,w,n,p,w,o,e,w,v,d\n",
    "\n",
    "This instance represents a mushroom with the following attribute values (highlighted in **bold**):\n",
    "\n",
    "*class*: edible=e, **poisonous=p**\n",
    "\n",
    "1. *cap-shape*: bell=b, conical=c, convex=x, flat=f, **knobbed=k**, sunken=s\n",
    "2. *cap-surface*: **fibrous=f**, grooves=g, scaly=y, smooth=s\n",
    "3. *cap-color*: **brown=n** ,buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y\n",
    "4. *bruises?*: bruises=t, **no=f**\n",
    "5. *odor*: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, **none=n**, pungent=p, spicy=s\n",
    "6. *gill-attachment*: attached=a, descending=d, **free=f**, notched=n\n",
    "7. *gill-spacing*: **close=c**, crowded=w, distant=d\n",
    "8. *gill-size*: broad=b, **narrow=n**\n",
    "9. *gill-color*: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, **white=w**, yellow=y\n",
    "10. *stalk-shape*: **enlarging=e**, tapering=t\n",
    "11. *stalk-root*: bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, **missing=?**\n",
    "12. *stalk-surface-above-ring*: fibrous=f, scaly=y, **silky=k**, smooth=s\n",
    "13. *stalk-surface-below-ring*: fibrous=f, **scaly=y**, silky=k, smooth=s\n",
    "14. *stalk-color-above-ring*: brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, **white=w**, yellow=y\n",
    "15. *stalk-color-below-ring*: **brown=n**, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y\n",
    "16. *veil-type*: **partial=p**, universal=u\n",
    "17. *veil-color*: brown=n, orange=o, **white=w**, yellow=y\n",
    "18. *ring-number*: none=n, **one=o**, two=t\n",
    "19. *ring-type*: cobwebby=c, **evanescent=e**, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z\n",
    "20. *spore-print-color*: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, **white=w**, yellow=y\n",
    "21. *population*: abundant=a, clustered=c, numerous=n, scattered=s, **several=v**, solitary=y\n",
    "22. *habitat*: grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, **woods=d**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises?', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']\n"
     ]
    }
   ],
   "source": [
    "attribute_names = ['class', \n",
    "                   'cap-shape', 'cap-surface', 'cap-color', \n",
    "                   'bruises?', \n",
    "                   'odor', \n",
    "                   'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', \n",
    "                   'stalk-shape', 'stalk-root', \n",
    "                   'stalk-surface-above-ring', 'stalk-surface-below-ring', \n",
    "                   'stalk-color-above-ring', 'stalk-color-below-ring',\n",
    "                   'veil-type', 'veil-color', \n",
    "                   'ring-number', 'ring-type', \n",
    "                   'spore-print-color', \n",
    "                   'population', \n",
    "                   'habitat']\n",
    "print(attribute_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in our data file (up on blackboard): agaricus-lepiota.data. [Lepiota](https://en.wikipedia.org/wiki/Lepiota) is a genus of gilled mushrooms in the family [Agaricaceae](https://en.wikipedia.org/wiki/Agaricaceae). All Lepiota species are ground-dwelling [saprotrophs](https://en.wikipedia.org/wiki/Saprotrophic_nutrition) with a preference for rich, calcareous soils. We build a list of instances, where each instance is a list of attribute values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a list of instances, where each instance is a list of attribute values (like `instance_1_str` above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8124 instances from data/agaricus-lepiota.data\n",
      "First instance: ['p', 'x', 's', 'n', 't', 'p', 'f', 'c', 'n', 'k', 'e', 'e', 's', 's', 'w', 'w', 'p', 'w', 'o', 'p', 'k', 's', 'u']\n"
     ]
    }
   ],
   "source": [
    "all_instances = []  # initialize instances to an empty list\n",
    "data_filename = 'data/agaricus-lepiota.data'\n",
    "\n",
    "with open(data_filename, 'r') as f:\n",
    "    for line in f:  # 'line' will be bound to the next line in f in each for loop iteration\n",
    "        all_instances.append(line.strip().split(','))\n",
    "        \n",
    "print('Read', len(all_instances), 'instances from', data_filename)\n",
    "# we don't want to print all the instances, so we'll just print the first one to verify\n",
    "print('First instance:', all_instances[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise*: Using a Python list comprehension, convert comma-separated strings to '|' separated strings.\n",
    "```python\n",
    "DELIMITER = '|'\n",
    "delimited_string = ''\n",
    "token_list = ['a', 'b', 'c']\n",
    "\n",
    "delimited_string = DELIMITER.join([...])\n",
    "delimited_string\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a|b|c'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DELIMITER = '|'\n",
    "delimited_string = ''\n",
    "token_list = ['a', 'b', 'c']\n",
    "\n",
    "delimited_string = DELIMITER.join([token for token in token_list])\n",
    "delimited_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values & \"clean\" instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the initial description of the UCI mushroom set above, 2480 of the 8124 instances have missing attribute values (denoted by `'?'`). We will simply ignore any such instances and restrict our focus to only the *clean* instances (with no missing values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5644 clean instances\n"
     ]
    }
   ],
   "source": [
    "UNKNOWN_VALUE = '?'\n",
    "clean_instances = [instance\n",
    "                   for instance in all_instances\n",
    "                   if UNKNOWN_VALUE not in instance]\n",
    "\n",
    "print(len(clean_instances), 'clean instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise*: Rewrite the code above using list comprehensions\n",
    "```python\n",
    "[instance for ... if ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = bell\n",
      "c = conical\n",
      "x = convex\n",
      "f = flat\n",
      "k = knobbed\n",
      "s = sunken\n"
     ]
    }
   ],
   "source": [
    "attribute_values_cap_type = {'b': 'bell', \n",
    "                             'c': 'conical', \n",
    "                             'x': 'convex', \n",
    "                             'f': 'flat', \n",
    "                             'k': 'knobbed', \n",
    "                             's': 'sunken'}\n",
    "\n",
    "for attribute_value_abbrev in attribute_values_cap_type:\n",
    "    print(attribute_value_abbrev, '=', attribute_values_cap_type[attribute_value_abbrev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to count the numbers of **edible** and **poisonous** mushrooms in the *clean_instances* list we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3488 edible mushrooms among the 5644 clean instances\n"
     ]
    }
   ],
   "source": [
    "edible_count = 0\n",
    "for instance in clean_instances:\n",
    "    if instance[0] == 'e':\n",
    "        edible_count += 1  # this is shorthand for edible_count = edible_count + 1\n",
    "\n",
    "print('There are', edible_count, 'edible mushrooms among the', \n",
    "      len(clean_instances), 'clean instances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to count the number of occurrences (**frequencies**) of each possible value for an attribute, we can create a dictionary where each dictionary key is an attribute value and each dictionary value is the count of instances with that attribute value.\n",
    "\n",
    "Using an ordinary dictionary, we must be careful to create a new dictionary entry the first time we see a new attribute value (that is not already contained in the dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each value of cap-state:\n",
      "x : 2840\n",
      "b : 300\n",
      "s : 32\n",
      "f : 2432\n",
      "k : 36\n",
      "c : 4\n"
     ]
    }
   ],
   "source": [
    "cap_state_value_counts = {}\n",
    "for instance in clean_instances:\n",
    "    cap_state_value = instance[1]  # cap-state is the 2nd attribute\n",
    "    if cap_state_value not in cap_state_value_counts:\n",
    "        # first occurrence, must explicitly initialize counter for this cap_state_value\n",
    "        cap_state_value_counts[cap_state_value] = 0\n",
    "    cap_state_value_counts[cap_state_value] += 1\n",
    "\n",
    "print('Counts for each value of cap-state:')\n",
    "for value in cap_state_value_counts:\n",
    "    print(value, ':', cap_state_value_counts[value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise*: Create the same dictionary using list comprehensions\n",
    "```python\n",
    "attribute_values_cap_type_2 = [[x[0], x ]\n",
    "                               for x in ['bell', 'conical', 'convex', 'flat', 'knobbed', 'sunken']]\n",
    "print(attribute_values_cap_type_2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['b', 'bell'], ['c', 'conical'], ['c', 'convex'], ['f', 'flat'], ['k', 'knobbed'], ['s', 'sunken']]\n"
     ]
    }
   ],
   "source": [
    "attribute_values_cap_type_2 = [[x[0], x ]\n",
    "                               for x in ['bell', 'conical', 'convex', 'flat', 'knobbed', 'sunken']]\n",
    "print(attribute_values_cap_type_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also had used a `Counter` object, which when instantiated with a list of items, returns a dictionary-like container in which the *keys* are the unique items in the list, and the *values* are the counts of each unique item in that list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 3, 'b': 2, 'c': 1})\n",
      "[('a', 3), ('b', 2), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cap_state_value_counts = Counter()\n",
    "\n",
    "counts = Counter(['a', 'b', 'c', 'a', 'b', 'a'])\n",
    "print(counts)\n",
    "print(counts.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each value of cap-state:\n",
      "x : 2840\n",
      "b : 300\n",
      "s : 32\n",
      "f : 2432\n",
      "k : 36\n",
      "c : 4\n"
     ]
    }
   ],
   "source": [
    "# Rebuild `cap_state_value_counts` using a Counter object.\n",
    "# ```python\n",
    "from collections import Counter\n",
    "\n",
    "cap_state_value_counts = Counter()\n",
    "for instance in clean_instances:\n",
    "    cap_state_value = instance[1]\n",
    "    # no need to explicitly initialize counters for cap_state_value; all start at zero\n",
    "    cap_state_value_counts[cap_state_value] += 1\n",
    "\n",
    "print('Counts for each value of cap-state:')\n",
    "for value in cap_state_value_counts:\n",
    "    print(value, ':', cap_state_value_counts[value])\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Do it better using a list comprehension constructor in Counter(...):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild `cap_state_value_counts` using a Counter object.\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "cap_state_value_counts = Counter(...)\n",
    "\n",
    "print('Counts for each value of cap-state:')\n",
    "for value in cap_state_value_counts:\n",
    "    print(value, ':', cap_state_value_counts[value])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The Classifier: A Decision Tree\n",
    "\n",
    "The image below depicts a decision tree created from the UCI mushroom dataset. It is taken from [here](http://gieseanw.wordpress.com/2012/03/03/decision-tree-learning/).\n",
    "\n",
    "* a white box represents an *internal node* (and the label represents the *attribute* being tested)\n",
    "* a blue box represents an attribute value (an *outcome* of the *test* of that attribute)\n",
    "* a green box represents a *leaf node* with a *class label* of *edible*\n",
    "* a red box represents a *leaf node* with a *class label* of *poisonous*\n",
    "\n",
    "<img src=\"http://gieseanw.files.wordpress.com/2012/03/mushroomdecisiontree.png\" style=\"width: 800px;\" />\n",
    "\n",
    "The UCI mushroom dataset consists entirely of [categorical variables](https://en.wikipedia.org/wiki/Categorical_variable), i.e., every variable (or *attribute*) has an **enumerated set of possible values**. \n",
    "\n",
    "Our decision tree will only accommodate *categorical variables*. It is based on [this](https://github.com/chrisspen/dtree) codebase, which itself is based on an article (since deleted) by Chris Roach.\n",
    "\n",
    "This algorithm will be our introduction to the 3 basic steps of Machine Learning:\n",
    "* ***Create*** a decision tree using a set of *training* instances\n",
    "* ***Classify*** (predict class labels) for a set of *test* instances (!= *training* instances) using a simple decision tree\n",
    "* ***Evaluate*** the performance of a simple decision tree on classifying a set of test instances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-parametric supervised learning\n",
    "\n",
    "Decision Trees (DTs) are a **non-parametric** *supervised* learning method used for classification and regression: There are no hyper-parameters to play with. That is why ML neophytes *love* decision trees.\n",
    "\n",
    "A decision node has two or more branches. A **leaf node** represents a classification or ***decision***. An **inner** node represents an intermediate (non-terminal) classification, or ***splitting***. The topmost decision node in a tree, which corresponds to the *best* overall predictor, is called the **root node**. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/dectree.png width = 400 />\n",
    "</center>\n",
    "\n",
    "**Splitting** is the process of partitioning the data set into subsets. Splits are formed on a particular variable.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/splitting.png width = 600 />\n",
    "</center>\n",
    "\n",
    "A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogeneous). The `ID3` algorithm uses **entropy** to calculate the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided one, it has entropy of one.\n",
    "\n",
    "The **information gain** is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding an attribute that returns the highest information gain (i.e., the most homogeneous branches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "When building a supervised classification model, the frequency distribution of attribute values is potentially a factor in determining the relative importance of each attribute in the model building process. That distribution is exactly the dictionary we built up in our Probability counting framework.\n",
    "\n",
    "This pdf can be used to compute [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory), a measure of **disorder in a dataset**. We compute entropy by multiplying the proportion of instances of each class label by the $log$ of that proportion, and then take the negative sum of those terms:\n",
    "\n",
    "For a 2-class (binary) classification task:\n",
    "\n",
    "$\\text{entropy}(S) = - p_1 log_2 (p_1) - p_2 log_2 (p_2)$\n",
    "\n",
    "where $p_i$ is proportion (relative frequency) of class *i* within the set *S*.\n",
    "\n",
    "Why entropy is a good metric for splitting involves a bit of math, which we skip. If you want to see the math, look [here](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/decision-tree-binary.md), then [here](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf).\n",
    "\n",
    "We know that the proportion of `clean_instances` that are labeled `'e'` (class `edible`) in the UCI dataset is $3488 \\div 5644 = 0.618$, and the proportion labeled `'p'` (class `poisonous`) is $2156 \\div 5644 = 0.382$. Thus the entropy of our mushroom dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9594413373534086\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "entropy = \\\n",
    "    - (3488 / 5644) * math.log(3488 / 5644, 2) \\\n",
    "    - (2156 / 5644) * math.log(2156 / 5644, 2)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download `simple_ml.py`, a python helper module for decision trees from blackboard, create a folder called `pythoncode` in your `C:/Users/<Username>` folder, and copy the file to that folder. Therein, a function `entropy(instances)` computes the entropy of `instances`. You may assume the class label is in position 0.\n",
    "\n",
    "*Note: We'll see in subsequent lectures that the class label in most datasets is the **last** rather than the **first** item on each row.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9594413373534086\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('pythoncode/')\n",
    "\n",
    "import simple_ml\n",
    "\n",
    "# delete 'simple_ml.' in the function call below to test your function\n",
    "print(simple_ml.entropy(clean_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally, a decision tree is constructed from a set of instances using a recursive algorithm that: \n",
    "\n",
    "* Selects the *best* attribute \n",
    "* Splits the set into subsets based on the values of that attribute (each subset is composed of instances from the original set that have the same value for that attribute)\n",
    "* Repeats the process on each of these subsets until a stopping condition is met (e.g., a subset has no instances or has instances which all have the same class label)\n",
    "\n",
    "Entropy is a metric that can be used in selecting the best attribute for each split: the best attribute is the one resulting in the *largest decrease in entropy* for a set of instances.\n",
    "\n",
    "*Information gain* measures the decrease in entropy that results from splitting a set of instances based on an attribute.\n",
    "\n",
    "$IG(S, a) = \\text{entropy}(S) - [p(s_1) * \\text{entropy}(s_1) + p(s_2) * \\text{entropy}(s_2) ... + p(s_n) * \\text{entropy}(s_n)]$\n",
    "\n",
    "where \n",
    "* $n$ is the number of distinct values of attribute $a$\n",
    "* $s_i$ is the subset of $S$ where all instances have the $i$th value of $a$\n",
    "* $p(s_i)$ is the proportion of instances in $S$ that have the $i$th value of $a$\n",
    "\n",
    "We'll use `information_gain()` in `simple_ml` to print the information gain for each of the attributes in the mushroom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain for the different mushroom attributes:\n",
      "\n",
      "0.017   1 cap-shape\n",
      "0.005   2 cap-surface\n",
      "0.195   3 cap-color\n",
      "0.140   4 bruises?\n",
      "0.860   5 odor\n",
      "0.004   6 gill-attachment\n",
      "0.058   7 gill-spacing\n",
      "0.032   8 gill-size\n",
      "0.213   9 gill-color\n",
      "0.275  10 stalk-shape\n",
      "0.097  11 stalk-root\n",
      "0.425  12 stalk-surface-above-ring\n",
      "0.409  13 stalk-surface-below-ring\n",
      "0.306  14 stalk-color-above-ring\n",
      "0.279  15 stalk-color-below-ring\n",
      "0.000  16 veil-type\n",
      "0.002  17 veil-color\n",
      "0.012  18 ring-number\n",
      "0.463  19 ring-type\n",
      "0.583  20 spore-print-color\n",
      "0.110  21 population\n",
      "0.101  22 habitat\n"
     ]
    }
   ],
   "source": [
    "print('Information gain for the different mushroom attributes:', end='\\n\\n')\n",
    "for i in range(1, len(attribute_names)):\n",
    "    print('{:5.3f}  {:2} {}'.format(\n",
    "        simple_ml.information_gain(clean_instances, i), i, attribute_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the attributes based in decreasing order of information gain, which indicates that `odor` is the best attribute for the first split in a decision tree that models the instances in this dataset.\n",
    "\n",
    "#### *Exercise#: Sort the attributes based in decreasing order of information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a modified version of the [ID3](https://en.wikipedia.org/wiki/ID3_algorithm) algorithm for building a  decision tree.\n",
    "\n",
    "    ID3 (Examples, Target_Attribute, Candidate_Attributes)\n",
    "        Create a Root node for the tree\n",
    "        If all examples have the same value of the Target_Attribute, \n",
    "            Return the single-node tree Root with label = that value \n",
    "        If the list of Candidate_Attributes is empty,\n",
    "            Return the single node tree Root,\n",
    "                with label = most common value of Target_Attribute in the examples.\n",
    "        Otherwise Begin\n",
    "            A ← The Attribute that best classifies examples (most information gain)\n",
    "            Decision Tree attribute for Root = A.\n",
    "            For each possible value, v_i, of A,\n",
    "                Add a new tree branch below Root, corresponding to the test A = v_i.\n",
    "                Let Examples(v_i) be the subset of examples that have the value v_i for A\n",
    "                If Examples(v_i) is empty,\n",
    "                    Below this new branch add a leaf node \n",
    "                        with label = most common target value in the examples\n",
    "                Else \n",
    "                    Below this new branch add the subtree \n",
    "                        ID3 (Examples(v_i), Target_Attribute, Attributes – {A})\n",
    "        End\n",
    "        Return Root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In building a decision tree, we will need to split the instances based on the index of the *best* attribute, i.e., the attribute that offers the *highest information gain*. We will use separate utility functions to handle these subtasks. To simplify, we rely exclusively on attribute *indexes* rather than attribute *names*.\n",
    "\n",
    "First, define a function, **`split_instances(instances, attribute_index)`**, to split a set of instances based on any attribute. This function will return a dictionary where each *key* is a distinct value of the specified `attribute_index`, and the *value* of each key is a list representing the subset of `instances` that have that `attribute_index` value.\n",
    "\n",
    "Use a [**`defaultdict`**](http://docs.python.org/2/library/collections.html#defaultdict-objects), a specialized dictionary class in the [**`collections`**](http://docs.python.org/2/library/collections.html) module that automatically creates an appropriate default value for a new key. For example, a `defaultdict(int)` automatically initializes a new dictionary entry to 0 (zero). A `defaultdict(list)` automatically initializes a new dictionary entry to the empty list (`[]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def split_instances(instances, attribute_index):\n",
    "    '''Returns a list of dictionaries, splitting a list of instances \n",
    "        according to their values of a specified attribute index\n",
    "    \n",
    "    The key of each dictionary is a distinct value of attribute_index,\n",
    "    and the value of each dictionary is a list representing \n",
    "       the subset of instances that have that value for the attribute\n",
    "    '''\n",
    "    partitions = defaultdict(list)\n",
    "    for instance in instances:\n",
    "        partitions[instance[attribute_index]].append(instance)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the function, we will partition `clean_instances` based on the `odor` attribute (index position 5) and print out the size (number of instances) in each partition rather than the lists of instances in each partition. These are all the possible odor categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('p', 256), ('a', 400), ('l', 400), ('n', 2776), ('f', 1584), ('c', 192), ('m', 36)]\n"
     ]
    }
   ],
   "source": [
    "partitions = split_instances(clean_instances, 5)\n",
    "print([(partition, len(partitions[partition])) for partition in partitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowe we have our first split and we know we can split instances based on a particular attribute. We want to be able to choose the *best* attribute with which to split the instances at every node, where *best* is defined as the attribute that provides the *greatest information gain* if instances were split based on that attribute. \n",
    "\n",
    "We restrict candidate attributes so that we don't try to split on an attribute that was used higher up in the decision tree (or use the target attribute as a candidate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`choose_best_attribute_index(instances, candidate_attribute_indexes)` returns the index in the list of `candidate_attribute_indexes` that provides the highest information gain if `instances` are split based on that attribute index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best attribute index: 5\n"
     ]
    }
   ],
   "source": [
    "print('Best attribute index:', \n",
    "      simple_ml.choose_best_attribute_index(clean_instances, range(1, len(attribute_names))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A leaf node in a decision tree represents the most frequently occurring - or majority - class value for that path through the tree (it's still a probabilistic algorithm). We will need a function that determines the majority value for the class index among a set of instances. One way to do this is to use the [`Counter`](https://docs.python.org/2/library/collections.html#counter-objects) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_counts: Counter({'e': 3488, 'p': 2156})\n",
      "  most_common(1): [('e', 3488)]\n",
      "  most_common(1)[0][0]: e\n"
     ]
    }
   ],
   "source": [
    "class_counts = Counter()  # create an empty counter\n",
    "for instance in clean_instances:\n",
    "    class_counts[instance[0]] += 1\n",
    "    \n",
    "print ('class_counts: {}\\n  most_common(1): {}\\n  most_common(1)[0][0]: {}'.format(\n",
    "    class_counts,\n",
    "    class_counts.most_common(1), # returns a list in which the 1st element is a tuple with the most common value and its count\n",
    "    class_counts.most_common(1)[0][0]))  # the most common value (1st element in that tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Exercise*: Use a list comprehension to improve on the cell above\n",
    "```python\n",
    "class_counts = Counter([instance[0] for ...])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to compute the number of unique values and/or the total number of values in a `Counter`.\n",
    "\n",
    "The number of unique values is simply the number of dictionary entries.\n",
    "\n",
    "The total number of values can be computed by taking the [**`sum()`**](https://docs.python.org/2/library/functions.html#sum) of all the counts (the *value* of each *key: value* pair ... or *key, value* tuple, if we use `Counter().most_common()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values: 2\n",
      "Total number of values:  5644\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique values: {}'.format(len(class_counts)))\n",
    "print('Total number of values:  {}'.format(sum([v \n",
    "                                                for k, v in class_counts.most_common()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Aside*\n",
    "\n",
    "Note that Python has a flexible mechanism for the testing truth values: In an **if** condition, any null object, zero-valued numerical expression or empty container (string, list, dictionary or tuple) is interpreted as *False* (i.e., *not True*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"False\" is False\n",
      "\"None\" is False\n",
      "\"0\" is False\n",
      "\"0.0\" is False\n",
      "\"\" is False\n",
      "\"[]\" is False\n",
      "\"{}\" is False\n",
      "\"()\" is False\n"
     ]
    }
   ],
   "source": [
    "for x in [False, None, 0, 0.0, \"\", [], {}, ()]:\n",
    "    print('\"{}\" is'.format(x), end=' ')\n",
    "    if x:\n",
    "        print(True)\n",
    "    else:\n",
    "        print(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, particularly with function parameters, it is helpful to differentiate `None` from empty lists and other data structures with a `False` truth value (one common use case is illustrated in `create_decision_tree()` below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"False is None\" is False\n",
      "\"None is None\" is True\n",
      "\"0 is None\" is False\n",
      "\"0.0 is None\" is False\n",
      "\" is None\" is False\n",
      "\"[] is None\" is False\n",
      "\"{} is None\" is False\n",
      "\"() is None\" is False\n"
     ]
    }
   ],
   "source": [
    "for x in [False, None, 0, 0.0, \"\", [], {}, ()]:\n",
    "    print('\"{} is None\" is'.format(x), end=' ')\n",
    "    if x is None:\n",
    "        print(True)\n",
    "    else:\n",
    "        print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"False\" is False\n",
      "\"None\" is False\n",
      "\"0\" is False\n",
      "\"0.0\" is False\n",
      "\"\" is False\n",
      "\"[]\" is False\n",
      "\"{}\" is False\n",
      "\"()\" is False\n"
     ]
    }
   ],
   "source": [
    "for x in [False, None, 0, 0.0, \"\", [], {}, ()]:\n",
    "    print('\"{}\" is {}'.format(x, True if x else False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`majority_value(instances, class_index)` returns the most frequently occurring value of `class_index` in `instances`. The `class_index` parameter should be optional, and have a default value of `0` (zero).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority value of index 0: e\n",
      "Majority value of index 1: x\n",
      "Majority value of index 2: y\n"
     ]
    }
   ],
   "source": [
    "print('Majority value of index {}: {}'.format(\n",
    "    0, simple_ml.majority_value(clean_instances))) \n",
    "\n",
    "# although there is only one class_index for the dataset, \n",
    "# we'll test the function by specifying other indexes using optional / keyword arguments\n",
    "print('Majority value of index {}: {}'.format(\n",
    "    1, simple_ml.majority_value(clean_instances, 1)))  # using argument order\n",
    "print('Majority value of index {}: {}'.format(\n",
    "    2, simple_ml.majority_value(clean_instances, class_index=2)))  # using keyword argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Simple Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recursive `create_decision_tree()` function below uses an optional parameter, `class_index`, which defaults to `0`. This is to accommodate other datasets in which the class label is the ***last*** element on each line (which would be most easily specified by using a `-1` value). Most data files in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html) have class labels as either the first element or the last element.\n",
    "\n",
    "To show how the decision tree is being built, an optional `trace` parameter, when non-zero, will generate trace information as the tree is constructed. The indentation level is incremented with each recursive call via the use of the conditional expression (ternary operator), `trace + 1 if trace else 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Creating tree node for attribute index 5\n",
      "> Creating subtree for value p (256, 21, 0, e)\n",
      "< < All 256 instances have label p\n",
      "> Creating subtree for value a (400, 21, 0, e)\n",
      "< < All 400 instances have label e\n",
      "> Creating subtree for value l (400, 21, 0, e)\n",
      "< < All 400 instances have label e\n",
      "> Creating subtree for value n (2764, 21, 0, e)\n",
      "> > Creating tree node for attribute index 20\n",
      "> > Creating subtree for value n (1296, 20, 0, e)\n",
      "< < < All 1296 instances have label e\n",
      "> > Creating subtree for value k (1296, 20, 0, e)\n",
      "< < < All 1296 instances have label e\n",
      "> > Creating subtree for value r (72, 20, 0, e)\n",
      "< < < All 72 instances have label p\n",
      "> > Creating subtree for value w (100, 20, 0, e)\n",
      "> > > Creating tree node for attribute index 21\n",
      "> > > Creating subtree for value v (60, 19, 0, e)\n",
      "< < < < All 60 instances have label e\n",
      "> > > Creating subtree for value c (16, 19, 0, e)\n",
      "< < < < All 16 instances have label p\n",
      "> > > Creating subtree for value y (24, 19, 0, e)\n",
      "< < < < All 24 instances have label e\n",
      "> Creating subtree for value f (1584, 21, 0, e)\n",
      "< < All 1584 instances have label p\n",
      "> Creating subtree for value c (192, 21, 0, e)\n",
      "< < All 192 instances have label p\n",
      "> Creating subtree for value m (28, 21, 0, e)\n",
      "< < All 28 instances have label p\n",
      "{5: {'p': 'p', 'a': 'e', 'l': 'e', 'n': {20: {'n': 'e', 'k': 'e', 'r': 'p', 'w': {21: {'v': 'e', 'c': 'p', 'y': 'e'}}}}, 'f': 'p', 'c': 'p', 'm': 'p'}}\n"
     ]
    }
   ],
   "source": [
    "def create_decision_tree(instances, \n",
    "                         candidate_attribute_indexes=None, \n",
    "                         class_index=0, \n",
    "                         default_class=None, \n",
    "                         trace=0):\n",
    "    '''Returns a new decision tree trained on a list of instances.\n",
    "    \n",
    "    The tree is constructed by recursively selecting and splitting instances based on \n",
    "    the highest information_gain of the candidate_attribute_indexes.\n",
    "    \n",
    "    The class label is found in position class_index.\n",
    "    \n",
    "    The default_class is the majority value for the current node's parent in the tree.\n",
    "    A positive (int) trace value will generate trace information \n",
    "        with increasing levels of indentation.\n",
    "    \n",
    "    Derived from the simplified ID3 algorithm presented in Building Decision Trees in Python \n",
    "        by Christopher Roach,\n",
    "    http://www.onlamp.com/pub/a/python/2006/02/09/ai_decision_trees.html?page=3\n",
    "    '''\n",
    "    \n",
    "    # if no candidate_attribute_indexes are provided, \n",
    "    # assume that we will use all but the target_attribute_index\n",
    "    # Note that None != [], \n",
    "    # as an empty candidate_attribute_indexes list is a recursion stopping condition\n",
    "    if candidate_attribute_indexes is None:\n",
    "        candidate_attribute_indexes = [i \n",
    "                                       for i in range(len(instances[0])) \n",
    "                                       if i != class_index]\n",
    "        # Note: do not use candidate_attribute_indexes.remove(class_index)\n",
    "        # as this would destructively modify the argument,\n",
    "        # causing problems during recursive calls\n",
    "        \n",
    "    class_labels_and_counts = Counter([instance[class_index] for instance in instances])\n",
    "\n",
    "    # If the dataset is empty or the candidate attributes list is empty, \n",
    "    # return the default value\n",
    "    if not instances or not candidate_attribute_indexes:\n",
    "        if trace:\n",
    "            print('{}Using default class {}'.format('< ' * trace, default_class))\n",
    "        return default_class\n",
    "    \n",
    "    # If all the instances have the same class label, return that class label\n",
    "    elif len(class_labels_and_counts) == 1:\n",
    "        class_label = class_labels_and_counts.most_common(1)[0][0]\n",
    "        if trace:\n",
    "            print('{}All {} instances have label {}'.format(\n",
    "                '< ' * trace, len(instances), class_label))\n",
    "        return class_label\n",
    "    else:\n",
    "        default_class = simple_ml.majority_value(instances, class_index)\n",
    "\n",
    "        # Choose the next best attribute index to best classify the instances\n",
    "        best_index = simple_ml.choose_best_attribute_index(\n",
    "            instances, candidate_attribute_indexes, class_index)        \n",
    "        if trace:\n",
    "            print('{}Creating tree node for attribute index {}'.format(\n",
    "                    '> ' * trace, best_index))\n",
    "\n",
    "        # Create a new decision tree node with the best attribute index \n",
    "        # and an empty dictionary object (for now)\n",
    "        tree = {best_index:{}}\n",
    "\n",
    "        # Create a new decision tree sub-node (branch) for each of the values \n",
    "        # in the best attribute field\n",
    "        partitions = simple_ml.split_instances(instances, best_index)\n",
    "\n",
    "        # Remove that attribute from the set of candidates for further splits\n",
    "        remaining_candidate_attribute_indexes = [i \n",
    "                                                 for i in candidate_attribute_indexes \n",
    "                                                 if i != best_index]\n",
    "        for attribute_value in partitions:\n",
    "            if trace:\n",
    "                print('{}Creating subtree for value {} ({}, {}, {}, {})'.format(\n",
    "                    '> ' * trace,\n",
    "                    attribute_value, \n",
    "                    len(partitions[attribute_value]), \n",
    "                    len(remaining_candidate_attribute_indexes), \n",
    "                    class_index, \n",
    "                    default_class))\n",
    "                \n",
    "            # Create a subtree for each value of the the best attribute\n",
    "            subtree = create_decision_tree(\n",
    "                partitions[attribute_value],\n",
    "                remaining_candidate_attribute_indexes,\n",
    "                class_index,\n",
    "                default_class,\n",
    "                trace + 1 if trace else 0)\n",
    "\n",
    "            # Add the new subtree to the empty dictionary object \n",
    "            # in the new tree/node we just created\n",
    "            tree[best_index][attribute_value] = subtree\n",
    "\n",
    "    return tree\n",
    "\n",
    "# split instances into separate training and testing sets\n",
    "training_instances = clean_instances[:-20]\n",
    "test_instances = clean_instances[-20:]\n",
    "tree = create_decision_tree(training_instances, trace=1)  # remove trace=1 to turn off tracing\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the tree shown above is difficult to discern from the normal printed representation of a dictionary.\n",
    "\n",
    "Python's [**`pprint`**](http://docs.python.org/2/library/pprint.html) module has a number of useful methods for pretty-printing or formatting objects in a more human readable way.\n",
    "\n",
    "The [**`pprint.pprint(object, stream=None, indent=1, width=80, depth=None)`**](http://docs.python.org/2/library/pprint.html#pprint.pprint) method will print `object` to a `stream` (a default value of `None` will dictate the use of [sys.stdout](http://docs.python.org/2/library/sys.html#sys.stdout), the same destination as `print` function output), using `indent` spaces to differentiate nesting levels, using up to a maximum `width` columns and up to to a maximum nesting level `depth` (`None` indicating no maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: {'a': 'e',\n",
      "     'c': 'p',\n",
      "     'f': 'p',\n",
      "     'l': 'e',\n",
      "     'm': 'p',\n",
      "     'n': {20: {'k': 'e',\n",
      "                'n': 'e',\n",
      "                'r': 'p',\n",
      "                'w': {21: {'c': 'p', 'v': 'e', 'y': 'e'}}}},\n",
      "     'p': 'p'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Instances with a Simple Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, when we construct a decision tree based on a set of *training* instances, we do so with the intent of using that tree to classify a set of one or more *test* instances.\n",
    "\n",
    "We define a function, **`classify(tree, instance, default_class=None)`**, to use a decision tree to classify a single `instance`, where an optional `default_class` can be specified as the return value if the instance represents a set of attribute values that don't have a representation in the decision tree.\n",
    "\n",
    "We will use a design pattern in which we will use a series of `if` statements, each of which returns a value if the condition is true, rather than a nested series of `if`, `elif` and/or `else` clauses, as it helps constrain the levels of indentation in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: p; actual: p\n",
      "predicted: p; actual: p\n",
      "predicted: p; actual: p\n",
      "predicted: e; actual: e\n",
      "predicted: e; actual: e\n",
      "predicted: p; actual: p\n",
      "predicted: e; actual: e\n",
      "predicted: e; actual: e\n",
      "predicted: e; actual: e\n",
      "predicted: p; actual: p\n",
      "predicted: e; actual: e\n",
      "predicted: e; actual: e\n",
      "predicted: e; actual: e\n",
      "predicted: p; actual: p\n",
      "predicted: e; actual: e\n",
      "predicted: e; actual: e\n",
      "predicted: e; actual: e\n",
      "predicted: e; actual: e\n",
      "predicted: p; actual: p\n",
      "predicted: p; actual: p\n"
     ]
    }
   ],
   "source": [
    "def classify(tree, instance, default_class=None):\n",
    "    '''Returns a classification label for instance, given a decision tree'''\n",
    "    if not tree:  # if the node is empty, return the default class\n",
    "        return default_class\n",
    "    if not isinstance(tree, dict):  # if the node is a leaf, return its class label\n",
    "        return tree\n",
    "    attribute_index = list(tree.keys())[0]  # using list(dict.keys()) for Python 3 compatibility\n",
    "    attribute_values = list(tree.values())[0]\n",
    "    instance_attribute_value = instance[attribute_index]\n",
    "    if instance_attribute_value not in attribute_values:  # this value was not in training data\n",
    "        return default_class\n",
    "    # recursively traverse the subtree (branch) associated with instance_attribute_value\n",
    "    return classify(attribute_values[instance_attribute_value], instance, default_class)\n",
    "\n",
    "for instance in test_instances:\n",
    "    predicted_label = classify(tree, instance)\n",
    "    actual_label = instance[0]\n",
    "    print('predicted: {}; actual: {}'.format(predicted_label, actual_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Accuracy of a Simple Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often helpful to evaluate the performance of a model using a dataset not used in the training of that model. In the simple example shown above, we used all but the last 20 instances to train a simple decision tree, then classified those last 20 instances using the tree.\n",
    "\n",
    "The advantage of this training/test split is that visual inspection of the classifications (sometimes called *predictions*) is relatively straightforward, revealing that all 20 instances were correctly classified.\n",
    "\n",
    "There are a variety of metrics that can be used to evaluate the performance of a model. Scikit Learn's [Model Evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html) library provides an overview and implementation of several possible metrics. For now, we simply measure the *accuracy* of a model, i.e., the percentage of test instances that are correctly classified (*true positives* and *true negatives*).\n",
    "\n",
    "The accuracy of the model above, given the set of 20 test instances, is 100% (20/20).\n",
    "\n",
    "The function below calculates the classification accuracy of a `tree` over a set of `test_instances` (with an optional `class_index` parameter indicating the position of the class label in each instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def classification_accuracy(tree, test_instances, class_index=0, default_class=None):\n",
    "    '''Returns the accuracy of classifying test_instances with tree, \n",
    "    where the class label is in position class_index'''\n",
    "    num_correct = 0\n",
    "    for i in range(len(test_instances)):\n",
    "        prediction = classify(tree, test_instances[i], default_class)\n",
    "        actual_value = test_instances[i][class_index]\n",
    "        if prediction == actual_value:\n",
    "            num_correct += 1\n",
    "    return num_correct / len(test_instances)\n",
    "\n",
    "print(classification_accuracy(tree, test_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to showing the percentage of correctly classified instances, it may be helpful to return the actual counts of correctly and incorrectly classified instances, e.g., if we want to compile a total count of correctly and incorrectly classified instances over a collection of test instances.\n",
    "\n",
    "We use the [**`zip([iterable, ...])`**](http://docs.python.org/2.7/library/functions.html#zip) function, which combines 2 or more sequences or iterables; the function returns a list of tuples, where the *i*th tuple contains the *i*th element from each of the argument sequences or iterables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'a'), (1, 'b'), (2, 'c')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([0, 1, 2], ['a', 'b', 'c']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use list comprehensions, the `Counter` class and the `zip()` function to modify `classification_accuracy()` so that it returns a packed tuple with: \n",
    "\n",
    "* the percentage of instances correctly classified\n",
    "* the number of correctly classified instances\n",
    "* the number of incorrectly classified instances\n",
    "\n",
    "We also modify the function to use `instances` rather than `test_instances`, as we sometimes want to be able to evaluate the accuracy of a model when tested on the training instances used to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 20, 0)\n"
     ]
    }
   ],
   "source": [
    "def classification_accuracy(tree, instances, class_index=0, default_class=None):\n",
    "    '''Returns the accuracy of classifying test_instances with tree, \n",
    "    where the class label is in position class_index'''\n",
    "    predicted_labels = [classify(tree, instance, default_class) \n",
    "                        for instance in instances]\n",
    "    actual_labels = [x[class_index] \n",
    "                     for x in instances]\n",
    "    counts = Counter([x == y \n",
    "                      for x, y in zip(predicted_labels, actual_labels)])\n",
    "    return counts[True] / len(instances), counts[True], counts[False]\n",
    "\n",
    "print(classification_accuracy(tree, test_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sometimes want to partition instances into subsets of equal size to measure performance. One metric this partitioning allows us to compute is a [learning curve](https://en.wikipedia.org/wiki/Learning_curve), i.e., assess how well the model performs based on the size of its training set. Another use of these partitions (aka *folds*) would be to conduct an [*n-fold cross validation*](https://en.wikipedia.org/wiki/Cross-validation_(statistics) evaluation.\n",
    "\n",
    "The following function, **`partition_instances(instances, num_partitions)`**, partitions a set of `instances` into `num_partitions` relatively equal-sized subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_instances(instances, num_partitions):\n",
    "    '''Returns a list of relatively equally sized disjoint sublists (partitions) \n",
    "    of the list of instances'''\n",
    "    return [[instances[j] \n",
    "             for j in range(i, len(instances), num_partitions)]\n",
    "            for i in range(num_partitions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing this function on the 5644 `clean_instances` from the UCI mushroom dataset, we create a small number of simplified instances to verify that the function has the desired behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances: [[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]]\n",
      "Partitions: [[[0, 1, 2], [2, 3, 4], [4, 5, 6]], [[1, 2, 3], [3, 4, 5]]]\n"
     ]
    }
   ],
   "source": [
    "instance_length = 3\n",
    "num_instances = 5\n",
    "\n",
    "simplified_instances = [[j \n",
    "                         for j in range(i, instance_length + i)] \n",
    "                        for i in range(num_instances)]\n",
    "\n",
    "print('Instances:', simplified_instances)\n",
    "partitions = partition_instances(simplified_instances, 2)\n",
    "print('Partitions:', partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`enumerate(sequence, start=0)`**](http://docs.python.org/2.7/library/functions.html#enumerate) function creates an iterator that successively returns the index and value of each element in a `sequence`, beginning at the `start` index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 b\n",
      "2 c\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(['a', 'b', 'c']):\n",
    "    print(i, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `enumerate()` to facilitate slightly more rigorous testing of our `partition_instances` function on our `simplified_instances`.\n",
    "\n",
    "Note that since we are printing values rather than accumulating values, we will not use nested list comprehensions for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# partitions: 0\n",
      "\n",
      "# partitions: 1\n",
      "partition 0: [[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]]\n",
      "\n",
      "# partitions: 2\n",
      "partition 0: [[0, 1, 2], [2, 3, 4], [4, 5, 6]]\n",
      "partition 1: [[1, 2, 3], [3, 4, 5]]\n",
      "\n",
      "# partitions: 3\n",
      "partition 0: [[0, 1, 2], [3, 4, 5]]\n",
      "partition 1: [[1, 2, 3], [4, 5, 6]]\n",
      "partition 2: [[2, 3, 4]]\n",
      "\n",
      "# partitions: 4\n",
      "partition 0: [[0, 1, 2], [4, 5, 6]]\n",
      "partition 1: [[1, 2, 3]]\n",
      "partition 2: [[2, 3, 4]]\n",
      "partition 3: [[3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_instances):\n",
    "    print('\\n# partitions:', i)\n",
    "    for j, partition in enumerate(partition_instances(simplified_instances, i)):\n",
    "        print('partition {}: {}'.format(j, partition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning our attention to the UCI mushroom dataset, the following will partition our `clean_instances` into 10 relatively equally sized disjoint subsets. We will use a list comprehension to print out the length of each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[565, 565, 565, 565, 564, 564, 564, 564, 564, 564]\n"
     ]
    }
   ],
   "source": [
    "partitions = partition_instances(clean_instances, 10)\n",
    "print([len(partition) for partition in partitions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the different trees that are constructed based on partition 0 (first 10th) of `clean_instances`, partitions 0 and 1 (first 2/10ths) of `clean_instances` and all `clean_instances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree trained with 565 instances:\n",
      "{5: {'a': 'e',\n",
      "     'c': 'p',\n",
      "     'f': 'p',\n",
      "     'l': 'e',\n",
      "     'm': 'p',\n",
      "     'n': {20: {'k': 'e', 'n': 'e', 'r': 'p', 'w': 'e'}},\n",
      "     'p': 'p'}}\n",
      "\n",
      "Tree trained with 1130 instances:\n",
      "{5: {'a': 'e',\n",
      "     'c': 'p',\n",
      "     'f': 'p',\n",
      "     'l': 'e',\n",
      "     'm': 'p',\n",
      "     'n': {20: {'k': 'e',\n",
      "                'n': 'e',\n",
      "                'r': 'p',\n",
      "                'w': {21: {'c': 'p', 'v': 'e', 'y': 'e'}}}},\n",
      "     'p': 'p'}}\n",
      "\n",
      "Tree trained with 5644 instances:\n",
      "{5: {'a': 'e',\n",
      "     'c': 'p',\n",
      "     'f': 'p',\n",
      "     'l': 'e',\n",
      "     'm': 'p',\n",
      "     'n': {20: {'k': 'e',\n",
      "                'n': 'e',\n",
      "                'r': 'p',\n",
      "                'w': {21: {'c': 'p', 'v': 'e', 'y': 'e'}}}},\n",
      "     'p': 'p'}}\n"
     ]
    }
   ],
   "source": [
    "tree0 = create_decision_tree(partitions[0])\n",
    "print('Tree trained with {} instances:'.format(len(partitions[0])))\n",
    "pprint(tree0)\n",
    "print()\n",
    "\n",
    "tree1 = create_decision_tree(partitions[0] + partitions[1])\n",
    "print('Tree trained with {} instances:'.format(len(partitions[0] + partitions[1])))\n",
    "pprint(tree1)\n",
    "print()\n",
    "\n",
    "tree = create_decision_tree(clean_instances)\n",
    "print('Tree trained with {} instances:'.format(len(clean_instances)))\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between the first two trees - *tree0* and *tree1* - is that in the first tree, instances with no `odor` (attribute index `5` is `'n'`) and a `spore-print-color` of white (attribute `20` = `'w'`) are classified as `edible` (`'e'`). With additional training data in the 2nd partition, an additional distinction is made such that instances with no `odor`, a white `spore-print-color` and a clustered `population` (attribute `21` = `'c'`) are classified as `poisonous` (`'p'`), while all other instances with no `odor` and a white `spore-print-color` (and any other value for the `population` attribute) are classified as `edible` (`'e'`).\n",
    "\n",
    "Note that there is no difference between `tree1` and `tree` (the tree trained with all instances). This early convergence on an optimal model is uncommon on most datasets (outside the UCI repository)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can partition our instances into subsets, we can use these subsets to construct different-sized training sets in the process of computing a learning curve.\n",
    "\n",
    "We will start off with an initial training set consisting only of the first partition, and then progressively extend that training set by adding a new partition during each iteration of computing the learning curve.\n",
    "\n",
    "[**`list.extend(L)`**](http://docs.python.org/2/tutorial/datastructures.html#more-on-lists) enables us to extend `list` by appending all the items in another list, `L`, to the end of `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "x.extend([4, 5])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the function, **`compute_learning_curve(instances, num_partitions=10)`**, which will take a list of `instances`, partition it into `num_partitions` relatively equally sized disjoint partitions, and then iteratively evaluate the accuracy of models trained with an incrementally increasing combination of instances in the first `num_partitions - 1` partitions then tested with instances in the last partition, a variant of . That is, a model trained with the first partition will be constructed (and tested), then a model trained with the first 2 partitions will be constructed (and tested), and so on. \n",
    "\n",
    "The function will return a list of `num_partitions - 1` tuples representing the size of the training set and the accuracy of a tree trained with that set (and tested on the `num_partitions - 1` set). This will provide some indication of the relative impact of the size of the training set on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(565, (0.9964539007092199, 562, 2)), (1130, (1.0, 564, 0)), (1695, (1.0, 564, 0)), (2260, (1.0, 564, 0)), (2824, (1.0, 564, 0)), (3388, (1.0, 564, 0)), (3952, (1.0, 564, 0)), (4516, (1.0, 564, 0)), (5080, (1.0, 564, 0))]\n"
     ]
    }
   ],
   "source": [
    "def compute_learning_curve(instances, num_partitions=10):\n",
    "    '''Returns a list of training sizes and scores for incrementally increasing partitions.\n",
    "\n",
    "    The list contains 2-element tuples, each representing a training size and score.\n",
    "    The i-th training size is the number of instances in partitions 0 through num_partitions - 2.\n",
    "    The i-th score is the accuracy of a tree trained with instances \n",
    "    from partitions 0 through num_partitions - 2\n",
    "    and tested on instances from num_partitions - 1 (the last partition).'''\n",
    "    \n",
    "    partitions = partition_instances(instances, num_partitions)\n",
    "    test_instances = partitions[-1][:]\n",
    "    training_instances = []\n",
    "    accuracy_list = []\n",
    "    for i in range(0, num_partitions - 1):\n",
    "        # for each iteration, the training set is composed of partitions 0 through i - 1\n",
    "        training_instances.extend(partitions[i][:])\n",
    "        tree = create_decision_tree(training_instances)\n",
    "        partition_accuracy = classification_accuracy(tree, test_instances)\n",
    "        accuracy_list.append((len(training_instances), partition_accuracy))\n",
    "    return accuracy_list\n",
    "\n",
    "accuracy_list = compute_learning_curve(clean_instances)\n",
    "print(accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI mushroom dataset is a particularly clean and simple data set, enabling quick convergence on an optimal decision tree for classifying new instances using relatively few training instances. \n",
    "\n",
    "We can use a larger number of smaller partitions to see a little more variation in accuracy performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(57, (0.9821428571428571, 55, 1)), (114, (1.0, 56, 0)), (171, (0.9821428571428571, 55, 1)), (228, (1.0, 56, 0)), (285, (1.0, 56, 0)), (342, (1.0, 56, 0)), (399, (1.0, 56, 0)), (456, (1.0, 56, 0)), (513, (1.0, 56, 0)), (570, (1.0, 56, 0))]\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = compute_learning_curve(clean_instances, 100)\n",
    "print(accuracy_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Class to Encapsulate a Simple Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple decision tree defined above uses a Python dictionary for its representation. One can imagine using other data structures, and/or extending the decision tree to support confidence estimates, numeric features and other capabilities that are often included in more fully functional implementations. To support future extensibility, and hide the details of the representation from the user, it would be helpful to have a user-defined class for simple decision trees.\n",
    "\n",
    "Note that other machine learning libraries may use different terminology for some of the functions defined above. For example, in the [`sklearn.tree.DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) class (and in most `sklearn` classifier classes), the method for constructing a classifier is named [`fit()`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.fit) - since it \"fits\" the data to a model - and the method for classifying instances is named [`predict()`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict) - since it is predicting the class label for an instance.\n",
    "\n",
    "In keeping with this common terminology, the code below defines a class, **`SimpleDecisionTree`**, with a single pseudo-protected member variable `_tree`, three public methods - `fit()`, `predict()` and `pprint()` - and two auxilary methods - `_create_tree()` and `_predict()` - to augment the `fit()` and `predict()` methods, respectively. \n",
    "\n",
    "The `fit()` method is identical to the `create_decision_tree()` function above, with the inclusion of the `self` parameter (as it is now a class method rather than a function). The `predict()` method is a similarly modified version of the `classify()` function, with the added capability to predict the label of either a single instance or a list of instances. The `classification_accuracy()` method is similar to the function of the same name (with the addition of the `self` parameter). The `pprint()` method prints the tree in a human-readable format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecisionTree:\n",
    "\n",
    "    _tree = {}  # this instance variable becomes accessible to class methods via self._tree\n",
    "\n",
    "    def __init__(self):\n",
    "        # this is where we would initialize any parameters to the SimpleDecisionTree\n",
    "        pass\n",
    "            \n",
    "    def fit(self, \n",
    "            instances, \n",
    "            candidate_attribute_indexes=None,\n",
    "            target_attribute_index=0,\n",
    "            default_class=None):\n",
    "        if not candidate_attribute_indexes:\n",
    "            candidate_attribute_indexes = [i \n",
    "                                           for i in range(len(instances[0]))\n",
    "                                           if i != target_attribute_index]\n",
    "        self._tree = self._create_tree(instances,\n",
    "                                       candidate_attribute_indexes,\n",
    "                                       target_attribute_index,\n",
    "                                       default_class)\n",
    "        \n",
    "    def _create_tree(self,\n",
    "                     instances,\n",
    "                     candidate_attribute_indexes,\n",
    "                     target_attribute_index=0,\n",
    "                     default_class=None):\n",
    "        class_labels_and_counts = Counter([instance[target_attribute_index] \n",
    "                                           for instance in instances])\n",
    "        if not instances or not candidate_attribute_indexes:\n",
    "            return default_class\n",
    "        elif len(class_labels_and_counts) == 1:\n",
    "            class_label = class_labels_and_counts.most_common(1)[0][0]\n",
    "            return class_label\n",
    "        else:\n",
    "            default_class = simple_ml.majority_value(instances, target_attribute_index)\n",
    "            best_index = simple_ml.choose_best_attribute_index(instances, \n",
    "                                                               candidate_attribute_indexes, \n",
    "                                                               target_attribute_index)\n",
    "            tree = {best_index:{}}\n",
    "            partitions = simple_ml.split_instances(instances, best_index)\n",
    "            remaining_candidate_attribute_indexes = [i \n",
    "                                                     for i in candidate_attribute_indexes \n",
    "                                                     if i != best_index]\n",
    "            for attribute_value in partitions:\n",
    "                subtree = self._create_tree(\n",
    "                    partitions[attribute_value],\n",
    "                    remaining_candidate_attribute_indexes,\n",
    "                    target_attribute_index,\n",
    "                    default_class)\n",
    "                tree[best_index][attribute_value] = subtree\n",
    "            return tree\n",
    "    \n",
    "    def predict(self, instances, default_class=None):\n",
    "        if not isinstance(instances, list):\n",
    "            return self._predict(self._tree, instance, default_class)\n",
    "        else:\n",
    "            return [self._predict(self._tree, instance, default_class) \n",
    "                    for instance in instances]\n",
    "    \n",
    "    def _predict(self, tree, instance, default_class=None):\n",
    "        if not tree:\n",
    "            return default_class\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        attribute_index = list(tree.keys())[0]  # using list(dict.keys()) for Py3 compatibiity\n",
    "        attribute_values = list(tree.values())[0]\n",
    "        instance_attribute_value = instance[attribute_index]\n",
    "        if instance_attribute_value not in attribute_values:\n",
    "            return default_class\n",
    "        return self._predict(attribute_values[instance_attribute_value],\n",
    "                             instance,\n",
    "                             default_class)\n",
    "    \n",
    "    def classification_accuracy(self, instances, default_class=None):\n",
    "        predicted_labels = self.predict(instances, default_class)\n",
    "        actual_labels = [x[0] for x in instances]\n",
    "        counts = Counter([x == y for x, y in zip(predicted_labels, actual_labels)])\n",
    "        return counts[True] / len(instances), counts[True], counts[False]\n",
    "    \n",
    "    def pprint(self):\n",
    "        pprint(self._tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following statements instantiate a `SimpleDecisionTree`, using all but the last 20 `clean_instances`, prints out the tree using its `pprint()` method, and then uses the `classify()` method to print the classification of the last 20 `clean_instances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: {'a': 'e',\n",
      "     'c': 'p',\n",
      "     'f': 'p',\n",
      "     'l': 'e',\n",
      "     'm': 'p',\n",
      "     'n': {20: {'k': 'e',\n",
      "                'n': 'e',\n",
      "                'r': 'p',\n",
      "                'w': {21: {'c': 'p', 'v': 'e', 'y': 'e'}}}},\n",
      "     'p': 'p'}}\n",
      "\n",
      "Model: p; truth: p\n",
      "Model: p; truth: p\n",
      "Model: p; truth: p\n",
      "Model: e; truth: e\n",
      "Model: e; truth: e\n",
      "Model: p; truth: p\n",
      "Model: e; truth: e\n",
      "Model: e; truth: e\n",
      "Model: e; truth: e\n",
      "Model: p; truth: p\n",
      "Model: e; truth: e\n",
      "Model: e; truth: e\n",
      "Model: e; truth: e\n",
      "Model: p; truth: p\n",
      "Model: e; truth: e\n",
      "Model: e; truth: e\n",
      "Model: e; truth: e\n",
      "Model: e; truth: e\n",
      "Model: p; truth: p\n",
      "Model: p; truth: p\n",
      "\n",
      "Classification accuracy: (1.0, 20, 0)\n"
     ]
    }
   ],
   "source": [
    "simple_decision_tree = SimpleDecisionTree()\n",
    "simple_decision_tree.fit(training_instances)\n",
    "simple_decision_tree.pprint()\n",
    "print()\n",
    "\n",
    "predicted_labels = simple_decision_tree.predict(test_instances)\n",
    "actual_labels = [instance[0] for instance in test_instances]\n",
    "for predicted_label, actual_label in zip(predicted_labels, actual_labels):\n",
    "    print('Model: {}; truth: {}'.format(predicted_label, actual_label))\n",
    "print()\n",
    "print('Classification accuracy:', simple_decision_tree.classification_accuracy(test_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scikit-Learn](http://scikit-learn.org/) has more full-featured decision tree building functions, and other types of machine learning algorithms. We'll use scikit-learn's decision forest building algoirthm in our lab.\n",
    "\n",
    "#### The ID3 and C4.5 Algorithms\n",
    "[Ross Quinlan](https://en.wikipedia.org/wiki/Ross_Quinlan) invented the ID3 algorithm that uses entropy and information gain to recursively create decision trees. This algorithm had some weaknesses, such as the inability to handle numerical attributes or attributes with missing values. His extension to ID3, C4.5, addressed those weaknesses.\n",
    "\n",
    "#### Overfit\n",
    "The idea of overfitting means that your prediction model is too biased towards your training data. Think about the hypothetical case where a node in our mushroom decision tree has 1,000 examples that are poisonous and only 1 that is edible. Should we really split the tree again here, or would it be a better idea to just assume that all mushrooms at this node are poisonous? Perhaps that 1 edible mushroom was simply mislabeled by our scientists. Or perhaps there is some attribute about the mushrooms that the scientists did not take into account that might better split the examples earlier on? It’s a good idea to assume that there is noise in our examples. If we don’t we may build a tree too affected by this noise, and thus have it overfit to our training examples. New examples, then, could have lower classification accuracy.\n",
    "\n",
    "To alleviate the problem of overfit, it is custom to [prune](https://en.wikipedia.org/wiki/Pruning_(decision_trees) decision trees, or remove sections of the tree that provide little power to classify instances.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/pruned.jpeg width = 600 />\n",
    "</center>\n",
    "\n",
    "#### MLB\n",
    "\n",
    "How could you apply this ML algorithm to problems we tackled in class? Well, for one, you could just extensively compile *hundreds* of ML statistics, put them all in a spreadsheet, and let *your laptop tell you which statistics are the most important* by having it build a decision tree (most important factors are closer to the root of the tree). Then you can model these parameters, or simply have the decision tree predict winners and losers based on new data.\n",
    "\n",
    "Don't you hate me, that I tell you just now? :-)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
